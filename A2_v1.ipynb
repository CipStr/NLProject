{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Credits**: Federico Ruggeri, Eleonora Mancini, Paolo Torroni\n",
    "\n",
    "**Keywords**: Human Value Detection, Multi-label classification, Transformers, BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ------------------------------------ 123.5/123.5 kB 258.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (21.3)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/24/97/9b59b43431f98d01806b288532da38099cc6f2fea0f3d712e21e269c0279/PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/42/db/0061fb8004ce9173b9249a0c323c799be51f2c8e6d4ff3cc38b549c3f8b6/tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/1a/ad/a78510af50f569b25dbcbed6e7b94e1d36612ef599eed94ad7e14fb9826c/safetensors-0.4.1-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp310-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 7.9/7.9 MB 287.0 kB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   -------------------------------------- 311.7/311.7 kB 283.8 kB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp310-cp310-win_amd64.whl (145 kB)\n",
      "   -------------------------------------- 145.3/145.3 kB 375.3 kB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp310-none-win_amd64.whl (277 kB)\n",
      "   -------------------------------------- 277.3/277.3 kB 534.1 kB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 2.2/2.2 MB 282.5 kB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, pyyaml, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 pyyaml-6.0.1 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\alepa\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: C:\\Users\\alepa\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#%pip install nltk\n",
    "#%pip install scikit-learn\n",
    "%pip install transformers\n",
    "#%pip install matplotlib\n",
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Corpus:\n",
    "\n",
    "We address a multi-label classification problem. We consider only level 3 categories which are the following:\n",
    "- Openness to change\n",
    "- Self-enhancement\n",
    "- Conservation\n",
    "- Self-transcendence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna merge annotations of level 2 categories belonging to the same level 3 category. For example, we merge the annotations of the level 2 categories \"Stimulation\" and \"Hedonism\" into the level 3 category \"Openness to change\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding to pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Argument ID                   Conclusion       Stance  \\\n",
      "0      A01002  We should ban human cloning  in favor of   \n",
      "\n",
      "                                             Premise  \n",
      "0  we should ban human cloning as it will only ca...  \n",
      "Shape of the training data: (5393, 4)\n",
      "Shape of the validation data: (1896, 4)\n",
      "Shape of the test data: (1576, 4)\n"
     ]
    }
   ],
   "source": [
    "arg_train = pd.read_csv('arguments/arguments-training.tsv', sep='\\t')\n",
    "print(arg_train.head(1))\n",
    "print(f\"Shape of the training data: {arg_train.shape}\")\n",
    "arg_val = pd.read_csv('arguments/arguments-validation.tsv', sep='\\t')\n",
    "print(f\"Shape of the validation data: {arg_val.shape}\")\n",
    "arg_test = pd.read_csv('arguments/arguments-test.tsv', sep='\\t')\n",
    "print(f\"Shape of the test data: {arg_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Argument ID  Self-direction: thought  Self-direction: action  Stimulation  \\\n",
      "0      A01002                        0                       0            0   \n",
      "\n",
      "   Hedonism  Achievement  Power: dominance  Power: resources  Face  \\\n",
      "0         0            0                 0                 0     0   \n",
      "\n",
      "   Security: personal  ...  Tradition  Conformity: rules  \\\n",
      "0                   0  ...          0                  0   \n",
      "\n",
      "   Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
      "0                          0         0                    0   \n",
      "\n",
      "   Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
      "0                           0                      0                     0   \n",
      "\n",
      "   Universalism: tolerance  Universalism: objectivity  \n",
      "0                        0                          0  \n",
      "\n",
      "[1 rows x 21 columns]\n",
      "Shape of the training data: (5393, 21)\n",
      "Shape of the validation data: (1896, 21)\n",
      "Shape of the test data: (1576, 21)\n"
     ]
    }
   ],
   "source": [
    "lab_train = pd.read_csv('arguments/labels-training.tsv', sep='\\t')\n",
    "print(lab_train.head(1))\n",
    "print(f\"Shape of the training data: {lab_train.shape}\")\n",
    "lab_val = pd.read_csv('arguments/labels-validation.tsv', sep='\\t')\n",
    "print(f\"Shape of the validation data: {lab_val.shape}\")\n",
    "lab_test = pd.read_csv('arguments/labels-test.tsv', sep='\\t')\n",
    "print(f\"Shape of the test data: {lab_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each split we merge arguments and labels into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data: (5393, 24)\n",
      "Shape of the validation data: (1896, 24)\n",
      "Shape of the test data: (1576, 24)\n",
      "  Argument ID                   Conclusion       Stance  \\\n",
      "0      A01002  We should ban human cloning  in favor of   \n",
      "\n",
      "                                             Premise  Self-direction: thought  \\\n",
      "0  we should ban human cloning as it will only ca...                        0   \n",
      "\n",
      "   Self-direction: action  Stimulation  Hedonism  Achievement  \\\n",
      "0                       0            0         0            0   \n",
      "\n",
      "   Power: dominance  ...  Tradition  Conformity: rules  \\\n",
      "0                 0  ...          0                  0   \n",
      "\n",
      "   Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
      "0                          0         0                    0   \n",
      "\n",
      "   Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
      "0                           0                      0                     0   \n",
      "\n",
      "   Universalism: tolerance  Universalism: objectivity  \n",
      "0                        0                          0  \n",
      "\n",
      "[1 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "df_train = arg_train.merge(lab_train, on='Argument ID')\n",
    "df_val = arg_val.merge(lab_val, on='Argument ID')\n",
    "df_test = arg_test.merge(lab_test, on='Argument ID')\n",
    "print(f\"Shape of the training data: {df_train.shape}\")\n",
    "print(f\"Shape of the validation data: {df_val.shape}\")\n",
    "print(f\"Shape of the test data: {df_test.shape}\")\n",
    "print(df_train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge level 2 categories into level 3 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Argument ID                   Conclusion       Stance  \\\n",
      "0      A01002  We should ban human cloning  in favor of   \n",
      "\n",
      "                                             Premise  Openness to change  \\\n",
      "0  we should ban human cloning as it will only ca...                   0   \n",
      "\n",
      "   Conservation  Self-enhancement  Self-transcendence  \n",
      "0             0                 1                   0  \n",
      "Shape of the training data: (5393, 8)\n",
      "Shape of the validation data: (1896, 8)\n",
      "Shape of the test data: (1576, 8)\n"
     ]
    }
   ],
   "source": [
    "# Merge the level 2 categories to level 3\n",
    "# They start from column 4 so we can just add 4 to the level 2 category\n",
    "# Openness to change: 4 columns\n",
    "# Conservation: columns 4 columns\n",
    "# Self-enhancement: 6 columns\n",
    "# Self-transcendence: 6 columns\n",
    "\n",
    "def merge_categories(df):\n",
    "    # Logical or \n",
    "    df['Openness to change'] = df[df.columns[4:8]].any(axis=1).astype(int)\n",
    "    df['Conservation'] = df[df.columns[7:12]].any(axis=1).astype(int)\n",
    "    df['Self-enhancement'] = df[df.columns[11:18]].any(axis=1).astype(int)\n",
    "    df['Self-transcendence'] = df[df.columns[17:24]].any(axis=1).astype(int)\n",
    "\n",
    "    #df['Openness to change'] = df[df.columns[4:8]].sum(axis=1)\n",
    "    #df['Conservation'] = df[df.columns[8:12]].sum(axis=1)\n",
    "    #df['Self-enhancement'] = df[df.columns[12:18]].sum(axis=1)\n",
    "    #df['Self-transcendence'] = df[df.columns[18:24]].sum(axis=1)\n",
    "    df = df.drop(df.columns[4:24], axis=1)\n",
    "    return df\n",
    "\n",
    "# get column names\n",
    "df_train = merge_categories(df_train)\n",
    "df_val = merge_categories(df_val)\n",
    "df_test = merge_categories(df_test)\n",
    "print(df_train.head(1))\n",
    "print(f\"Shape of the training data: {df_train.shape}\")\n",
    "print(f\"Shape of the validation data: {df_val.shape}\")\n",
    "print(f\"Shape of the test data: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Argument ID                                                      A26068\n",
       "Conclusion                        We should subsidize stay-at-home dads\n",
       "Stance                                                      in favor of\n",
       "Premise               we should subsidize stay-at-home dads because ...\n",
       "Openness to change                                                    1\n",
       "Conservation                                                          0\n",
       "Self-enhancement                                                      1\n",
       "Self-transcendence                                                    1\n",
       "Name: 8, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.loc[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "100%|██████████| 5393/5393 [00:00<00:00, 8413.42it/s] \n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "train_text = df_train['Premise'].tolist()\n",
    "train_lengths = [len(word_tokenize(x)) for x in tqdm(train_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distribution of the number of words per argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_lengths, color='blue', edgecolor='black')\n",
    "plt.xlabel('Length of text')\n",
    "plt.ylabel('Number of texts')\n",
    "plt.title('Distribution of text length in training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preproccesing with nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alepa\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "punct = string.punctuation\n",
    "bad_symbols = re.compile('[^a-z ]')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = bad_symbols.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in punct)\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ban human cloning cause huge issue bunch human running around acting\n"
     ]
    }
   ],
   "source": [
    "train = df_train.copy()\n",
    "train['Preprocessed'] = train['Premise'].apply(preprocess)\n",
    "print(train['Preprocessed'][0])\n",
    "train = train.drop(columns=train.columns[1:4])\n",
    "train.head(1)\n",
    "val = df_val.copy()\n",
    "val['Preprocessed'] = val['Premise'].apply(preprocess)\n",
    "val = val.drop(columns=val.columns[1:4])\n",
    "test = df_test.copy()\n",
    "test['Preprocessed'] = test['Premise'].apply(preprocess)\n",
    "test = test.drop(columns=test.columns[1:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Openness to change</th>\n",
       "      <th>Conservation</th>\n",
       "      <th>Self-enhancement</th>\n",
       "      <th>Self-transcendence</th>\n",
       "      <th>Preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ban human cloning cause huge issue bunch human...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID  Openness to change  Conservation  Self-enhancement  \\\n",
       "0      A01002                   0             0                 1   \n",
       "\n",
       "   Self-transcendence                                       Preprocessed  \n",
       "0                   0  ban human cloning cause huge issue bunch human...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training data: (5393, 6836)\n",
      "Size of the vocabulary: 6836\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train['Preprocessed'])\n",
    "print(f\"Shape of the training data: {X_train.shape}\")\n",
    "print(f\"Size of the vocabulary: {len(vectorizer.vocabulary_)}\")\n",
    "X_val = vectorizer.transform(val['Preprocessed'])\n",
    "X_test = vectorizer.transform(test['Preprocessed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the validation data: (1896, 6836)\n",
      "Shape of the test data: (1576, 6836)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of the validation data: {X_val.shape}\")\n",
    "print(f\"Shape of the test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Model definition\n",
    "\n",
    "You are tasked to define several neural models for multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a random uniform classifier (an individual classifier per category).\n",
    "* **Baseline**: implement a majority classifier (an individual classifier per category).\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **BERT w/ C**: define a BERT-based classifier that receives an argument **conclusion** as input.\n",
    "* **BERT w/ CP**: add argument **premise** as an additional input.\n",
    "* **BERT w/ CPS**: add argument premise-to-conclusion **stance** as an additional input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation report for baseline models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y_test, Y_pred):\n",
    "    report = classification_report(Y_test, Y_pred)\n",
    "    print(report)\n",
    "\n",
    "Y_test = test.iloc[:, 1:5].values # get as array the labels of the test set [oppenness, conservation, self-enhancement, self-transcendence]\n",
    "Y_val = val.iloc[:, 1:5].values # get as array the labels of the validation set [oppenness, conservation, self-enhancement, self-transcendence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a random uniform classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "uniform_classifier_openness = DummyClassifier(strategy='uniform')\n",
    "uniform_classifier_conserv = DummyClassifier(strategy='uniform')\n",
    "uniform_classifier_self_enh = DummyClassifier(strategy='uniform')\n",
    "uniform_classifier_self_trans = DummyClassifier(strategy='uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a majority classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_classifier_openness = DummyClassifier(strategy='most_frequent')\n",
    "majority_classifier_conserv = DummyClassifier(strategy='most_frequent')\n",
    "majority_classifier_self_enh = DummyClassifier(strategy='most_frequent')\n",
    "majority_classifier_self_trans = DummyClassifier(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alepa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup, RobertaConfig\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 481/481 [00:00<?, ?B/s] \n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.20MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.53MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.67MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'roberta-base'\n",
    "max_len = 100\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "learning_rate = 2e-5\n",
    "output_channels = 768\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token generation for RoBERTa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training labels: (5393, 4)\n",
      "Shape of the validation labels: (1896, 4)\n",
      "Shape of the test labels: (1576, 4)\n"
     ]
    }
   ],
   "source": [
    "class RobertaDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.targets = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data[index])\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create the dataset for the neural network\n",
    "train_targets = df_train.iloc[:, 4:8].values\n",
    "val_targets = df_val.iloc[:, 4:8].values\n",
    "test_targets = df_test.iloc[:, 4:8].values\n",
    "\n",
    "print(f\"Shape of the training labels: {train_targets.shape}\")\n",
    "print(f\"Shape of the validation labels: {val_targets.shape}\")\n",
    "print(f\"Shape of the test labels: {test_targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa model definition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roBERTa(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(roBERTa, self).__init__()\n",
    "        self.roberta = AutoModel.from_pretrained(model_name, return_dict=False)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        self.classifier = torch.nn.Linear(output_channels, 4)\n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output = self.roberta(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output = self.dropout(output)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa w/C data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training dataset: 5393\n",
      "Shape of the validation dataset: 1896\n",
      "Shape of the test dataset: 1576\n"
     ]
    }
   ],
   "source": [
    "train_datasetc = RobertaDataset(df_train.iloc[:, 1], train_targets, tokenizer, max_len)\n",
    "val_datasetc = RobertaDataset(df_val.iloc[:, 1], val_targets, tokenizer, max_len)\n",
    "test_datasetc = RobertaDataset(df_test.iloc[:, 1], test_targets, tokenizer, max_len)\n",
    "\n",
    "print(f\"Shape of the training dataset: {len(train_datasetc)}\")\n",
    "print(f\"Shape of the validation dataset: {len(val_datasetc)}\")\n",
    "print(f\"Shape of the test dataset: {len(test_datasetc)}\")\n",
    "\n",
    "train_dataloaderc = DataLoader(train_datasetc, batch_size=batch_size)\n",
    "val_dataloaderc = DataLoader(val_datasetc, batch_size=batch_size)\n",
    "test_dataloaderc = DataLoader(test_datasetc, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gpu if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 499M/499M [00:09<00:00, 54.7MB/s] \n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "roBERTa(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_c = roBERTa(model_name)\n",
    "model_c.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training RoBERTa w/C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model_c.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloaderc)*epochs)\n",
    "\n",
    "def trainBert(model):\n",
    "    size = len(train_dataloaderc.dataset)\n",
    "    model.train()\n",
    "    for batch, data in enumerate(train_dataloaderc, 0):\n",
    "        ids = data['ids'].to(device, dtype=torch.long)\n",
    "        mask = data['mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data['targets'].to(device, dtype=torch.float)\n",
    "\n",
    "        outputs = model_c(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(ids)\n",
    "            print(f\"Train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation RoBERTa w/C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, val_loss_min_input, model):\n",
    "    size = len(test_dataloaderc.dataset)\n",
    "    num_batches = len(test_dataloaderc)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_dataloaderc, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model_c(ids, mask, token_type_ids)\n",
    "            val_loss += loss_fn(outputs, targets).item()\n",
    "        \n",
    "        val_loss /= num_batches\n",
    "        #outputs, targets = fin_outputs, fin_targets\n",
    "        print(f\"\\nValidation loss: {val_loss:>8f}.\")\n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if val_loss <= val_loss_min_input:\n",
    "            #create checkpoint variable and add important data\n",
    "            if epoch > 0: \n",
    "                print('Validation loss decreased ({:.8f} --> {:.8f}).  Saving model ...'.format(val_loss_min_input, val_loss))\n",
    "            else: print('Saving model ...')   \n",
    "            # save best moel\n",
    "            torch.save(model.state_dict(), \"model_c.pth\")\n",
    "            print(\"Saved PyTorch Model State to model.pth\\n\")\n",
    "            val_loss_min_input = val_loss\n",
    "    \n",
    "    return val_loss_min_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoBERTa w/CP data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training dataset: 5393\n",
      "Shape of the validation dataset: 1896\n",
      "Shape of the test dataset: 1576\n"
     ]
    }
   ],
   "source": [
    "train_datasetp = RobertaDataset(df_train.iloc[:, 3], train_targets, tokenizer, max_len)\n",
    "val_datasetp = RobertaDataset(df_val.iloc[:, 3], val_targets, tokenizer, max_len)\n",
    "test_datasetp = RobertaDataset(df_test.iloc[:, 3], test_targets, tokenizer, max_len)\n",
    "\n",
    "print(f\"Shape of the training dataset: {len(train_datasetp)}\")\n",
    "print(f\"Shape of the validation dataset: {len(val_datasetp)}\")\n",
    "print(f\"Shape of the test dataset: {len(test_datasetp)}\")\n",
    "\n",
    "train_dataloaderp = DataLoader(train_datasetp, batch_size=batch_size)\n",
    "val_dataloaderp = DataLoader(val_datasetp, batch_size=batch_size)\n",
    "test_dataloaderp = DataLoader(test_datasetp, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1828,  0.2094, -0.0038, -0.0573],\n",
       "        [ 0.0874,  0.0400, -0.0721, -0.0200],\n",
       "        [ 0.0093,  0.1461, -0.0029,  0.0195],\n",
       "        [-0.1033,  0.0264,  0.0550,  0.0222],\n",
       "        [-0.1014,  0.1246, -0.0914, -0.0866],\n",
       "        [-0.0512,  0.1199,  0.1506,  0.0140],\n",
       "        [ 0.0833,  0.2248,  0.1292,  0.0194],\n",
       "        [ 0.0859,  0.1430,  0.0661,  0.0882],\n",
       "        [ 0.0087,  0.0444,  0.1451,  0.0614],\n",
       "        [-0.0438,  0.0545,  0.0341, -0.0201],\n",
       "        [-0.0754,  0.0271,  0.0386, -0.0585],\n",
       "        [ 0.0457,  0.0245,  0.0587,  0.0460],\n",
       "        [-0.0337,  0.0333,  0.1333,  0.1547],\n",
       "        [ 0.0335,  0.0614,  0.0291,  0.0642],\n",
       "        [-0.0490, -0.0449,  0.0747,  0.1677],\n",
       "        [-0.0291, -0.0582,  0.1032,  0.0370]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloaderp))\n",
    "\n",
    "ids = batch[\"ids\"].to(device)\n",
    "mask = batch[\"mask\"].to(device)\n",
    "target = batch[\"targets\"].to(device)\n",
    "token = batch[\"token_type_ids\"].to(device)\n",
    "# boooooooohh raga\n",
    "output = model_c(ids, mask, token)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gpu if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "roBERTa(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cp = roBERTa(model_name)\n",
    "model_cp.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using per-category binary F1-score.\n",
    "* Compute the average binary F1-score over all categories (macro F1-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random uniform classifier metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1scoreRandom(X_val, Y_val):\n",
    "    Y_pred_openness_unif = uniform_classifier_openness.predict(X_val)\n",
    "    Y_pred_conserv_unif = uniform_classifier_conserv.predict(X_val)\n",
    "    Y_pred_self_enh_unif = uniform_classifier_self_enh.predict(X_val)\n",
    "    Y_pred_self_trans_unif = uniform_classifier_self_trans.predict(X_val)\n",
    "    \n",
    "    Y_pred = []\n",
    "    for i in range(len(Y_pred_openness_unif)):\n",
    "        temp = []\n",
    "        temp.append(Y_pred_openness_unif[i])\n",
    "        temp.append(Y_pred_conserv_unif[i])\n",
    "        temp.append(Y_pred_self_enh_unif[i])\n",
    "        temp.append(Y_pred_self_trans_unif[i])\n",
    "        Y_pred.append(temp)\n",
    "        \n",
    "    return evaluate(Y_val, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority classifier metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1scoreMajority(X_val, Y_val):\n",
    "    Y_pred_openness_maj = majority_classifier_openness.predict(X_val)\n",
    "    Y_pred_conserv_maj = majority_classifier_conserv.predict(X_val)\n",
    "    Y_pred_self_enh_maj = majority_classifier_self_enh.predict(X_val)\n",
    "    Y_pred_self_trans_maj = majority_classifier_self_trans.predict(X_val)\n",
    "\n",
    "    Y_pred = []\n",
    "    for i in range(len(Y_pred_openness_maj)):\n",
    "        temp = []\n",
    "        temp.append(Y_pred_openness_maj[i])\n",
    "        temp.append(Y_pred_conserv_maj[i])\n",
    "        temp.append(Y_pred_self_enh_maj[i])\n",
    "        temp.append(Y_pred_self_trans_maj[i])\n",
    "        Y_pred.append(temp)\n",
    "        \n",
    "    return evaluate(Y_val, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate **all** defined models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Compute metrics on the validation set.\n",
    "* Report **per-category** and **macro** F1-score for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed definition\n",
    "#seed = 42\n",
    "#seed = 69\n",
    "seed = 420\n",
    "\n",
    "# Set the seed for numpy and tensorflow\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the random uniform classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyClassifier(strategy=&#x27;uniform&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyClassifier</label><div class=\"sk-toggleable__content\"><pre>DummyClassifier(strategy=&#x27;uniform&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyClassifier(strategy='uniform')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Y_train_openness = train['Openness to change']\n",
    "Y_train_conserv = train['Conservation']\n",
    "Y_train_self_enh = train['Self-enhancement']\n",
    "Y_train_self_trans = train['Self-transcendence']\n",
    "uniform_classifier_openness.fit(X_train, Y_train_openness)\n",
    "uniform_classifier_conserv.fit(X_train, Y_train_conserv)\n",
    "uniform_classifier_self_enh.fit(X_train, Y_train_self_enh)\n",
    "uniform_classifier_self_trans.fit(X_train, Y_train_self_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scoreRandom(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the majority classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyClassifier</label><div class=\"sk-toggleable__content\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_classifier_openness.fit(X_train, Y_train_openness)\n",
    "majority_classifier_conserv.fit(X_train, Y_train_conserv)\n",
    "majority_classifier_self_enh.fit(X_train, Y_train_self_enh)\n",
    "majority_classifier_self_trans.fit(X_train, Y_train_self_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       698\n",
      "           1       0.00      0.00      0.00       885\n",
      "           2       0.75      1.00      0.86      1426\n",
      "           3       0.79      1.00      0.89      1506\n",
      "\n",
      "   micro avg       0.77      0.65      0.71      4515\n",
      "   macro avg       0.39      0.50      0.44      4515\n",
      "weighted avg       0.50      0.65      0.57      4515\n",
      " samples avg       0.77      0.68      0.70      4515\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alepa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "f1scoreMajority(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RoBERTa w/C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min = np.Inf\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    trainBert(model_c)\n",
    "    val_loss_min = validation(epoch, val_loss_min, model_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RoBERTa w/C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_min = np.Inf\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    trainBert(model_cp)\n",
    "    val_loss_min = validation(epoch, val_loss_min, model_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa w/C evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "outputs_c, targets_c = test(model_c, val_dataloader)\n",
    "results_c = {}\n",
    "for tr in np.arange(0.1, 0.9, 0.1):\n",
    "    tr = round(tr, 2)\n",
    "    predictions_c = np.array(outputs_c) >= tr\n",
    "    f1_c = f1_score(targets_c, predictions_c, average='macro', zero_division=1)\n",
    "    results_c[tr] = f1_c\n",
    "\n",
    "for k, v in results_c.items():\n",
    "    print(f\"Threshold: {k}, F1-score: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = max(results_c, key=results_c.get)\n",
    "print(f\"Best threshold: {th}, F1-score: {results_c[th]}\")\n",
    "predictions_c = np.array(outputs_c) >= th\n",
    "labels = ['Openness to change', 'Conservation', 'Self-enhancement', 'Self-transcendence']\n",
    "print(classification_report(targets_c, predictions_c, target_names=labels ,zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Error Analysis\n",
    "\n",
    "You are tasked to discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* **Compare** classification performance of BERT-based models with respect to baselines.\n",
    "* Discuss **difference in prediction** between the best performing BERT-based model and its variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Report\n",
    "\n",
    "Wrap up your experiment in a short report (up to 2 pages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Use the NLP course report template.\n",
    "* Summarize each task in the report following the provided template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
