{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "005CLkdt-mxo"
      },
      "source": [
        "# NLP Standard Project:\n",
        "\n",
        "- Students: **Matteo Belletti**, **Alessandro Pasi**, **Stricescu Razvan Ciprian**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets==2.13.2\n",
        "!pip install accelerate -U\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
            "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
            "     -------------------------------------- 126.8/126.8 kB 3.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: filelock in d:\\nlproject\\.venv\\lib\\site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in d:\\nlproject\\.venv\\lib\\site-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\nlproject\\.venv\\lib\\site-packages (from transformers) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\nlproject\\.venv\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\nlproject\\.venv\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\nlproject\\.venv\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in d:\\nlproject\\.venv\\lib\\site-packages (from transformers) (2.28.1)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
            "  Using cached tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in d:\\nlproject\\.venv\\lib\\site-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\nlproject\\.venv\\lib\\site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\nlproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\nlproject\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
            "Requirement already satisfied: colorama in d:\\nlproject\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\nlproject\\.venv\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\nlproject\\.venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\nlproject\\.venv\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\nlproject\\.venv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
            "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
            "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/8.2 MB 8.9 MB/s eta 0:00:01\n",
            "   -- ------------------------------------- 0.6/8.2 MB 7.4 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 0.9/8.2 MB 7.2 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 1.2/8.2 MB 7.0 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 1.5/8.2 MB 6.8 MB/s eta 0:00:01\n",
            "   -------- ------------------------------- 1.8/8.2 MB 6.8 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 2.1/8.2 MB 6.7 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 2.4/8.2 MB 6.7 MB/s eta 0:00:01\n",
            "   ------------- -------------------------- 2.7/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 3.0/8.2 MB 6.7 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 3.3/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 3.6/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 3.9/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 4.2/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 4.6/8.2 MB 6.5 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 4.9/8.2 MB 6.5 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 4.9/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 4.9/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 5.8/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 6.1/8.2 MB 6.5 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 6.4/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 6.7/8.2 MB 6.5 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 7.0/8.2 MB 6.6 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 7.3/8.2 MB 6.5 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 7.6/8.2 MB 6.5 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 7.9/8.2 MB 6.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  8.2/8.2 MB 6.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  8.2/8.2 MB 6.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.2/8.2 MB 6.1 MB/s eta 0:00:00\n",
            "Using cached tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "Successfully installed tokenizers-0.15.0 transformers-4.36.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ransformers (d:\\nlproject\\.venv\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ransformers (d:\\nlproject\\.venv\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOs950_0-mxp",
        "outputId": "22676575-433c-48c9-e115-df2c26851981"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\NLProject\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
        "from sklearn.dummy import DummyClassifier\n",
        "import pandas as pd\n",
        "import json\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck-GQq5d-mxr"
      },
      "source": [
        "## Data loading and preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvWV-5RY-mxr",
        "outputId": "ab7ccfe6-c3a7-4387-e4bc-0feb0534406a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: 4000\n",
            "Example of a sample: {'episode': 'utterance_0', 'speakers': ['Chandler', 'The Interviewer', 'Chandler', 'The Interviewer', 'Chandler'], 'emotions': ['neutral', 'neutral', 'neutral', 'neutral', 'surprise'], 'utterances': [\"also I was the point person on my company's transition from the KL-5 to GR-6 system.\", \"You must've had your hands full.\", 'That I did. That I did.', \"So let's talk a little bit about your duties.\", 'My duties?  All right.'], 'triggers': [0.0, 0.0, 0.0, 1.0, 0.0]}\n"
          ]
        }
      ],
      "source": [
        "# open json in project_data_MELD folder\n",
        "try:\n",
        "    with open('project_data_MELD/MELD_train_efr.json') as f:\n",
        "        data = json.load(f)\n",
        "except:\n",
        "    with open('MELD_train_efr.json') as f:\n",
        "        data = json.load(f)\n",
        "print(f\"Number of samples: {len(data)}\")\n",
        "print(f\"Example of a sample: {data[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ck5Yz3C-mxr",
        "outputId": "e3724972-d61e-425c-edc6-f607fca32c77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframe shape: (4000, 4)\n",
            "Dataframe columns: Index(['speakers', 'emotions', 'utterances', 'triggers'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Convert data to pandas dataframe\n",
        "df = pd.DataFrame(data)\n",
        "# drop episode and speakers columns\n",
        "df = df.drop(columns=['episode'])\n",
        "print(f\"Dataframe shape: {df.shape}\")\n",
        "print(f\"Dataframe columns: {df.columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Spy3zzO-mxs"
      },
      "source": [
        "Changing nan values to zeros in order to avoid errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu_fACgc-mxs",
        "outputId": "644df322-2ddd-4aea-d5ae-036077ef6100"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"triggers\"] = df[\"triggers\"].apply(lambda x: [0 if elem != 1 and elem != 0 else elem for elem in x])\n",
        "df[\"triggers\"][3359]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UssZaYz3-mxs"
      },
      "source": [
        "Splitting the data into train, validation and test sets with a 80-10-10 ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OFaRTAN7-mxs"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
        "df_val, df_test = train_test_split(temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnChzNkK-mxs",
        "outputId": "67cd45f1-d44a-460f-b6c6-1df3b31dddc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (3200, 4)\n",
            "Val shape: (400, 4)\n",
            "Test shape: (400, 4)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train shape: {df_train.shape}\")\n",
        "print(f\"Val shape: {df_val.shape}\")\n",
        "print(f\"Test shape: {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzsNt8T--mxs"
      },
      "source": [
        "As baselines models we need to implement a random model and a majority class model for emotions and triggers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGSbTu_l-mxt",
        "outputId": "09bed160-d168-4216-ebfa-f4590c363589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neutral': 12066, 'joy': 4986, 'surprise': 3664, 'anger': 3203, 'sadness': 2108, 'fear': 889, 'disgust': 848}\n"
          ]
        }
      ],
      "source": [
        "# first we create a dictionary of all emotions with their corresponding occurences\n",
        "emotions_dict = {}\n",
        "for emotions in df_train[\"emotions\"]:\n",
        "    for emotion in emotions:\n",
        "        if emotion in emotions_dict:\n",
        "            emotions_dict[emotion] += 1\n",
        "        else:\n",
        "            emotions_dict[emotion] = 1\n",
        "\n",
        "# then we sort the dictionary by occurences\n",
        "emotions_dict = {k: v for k, v in sorted(emotions_dict.items(), key=lambda item: item[1], reverse=True)}\n",
        "print(emotions_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aG4tIgr-mxt"
      },
      "source": [
        "Plotting the distribution of emotions could also be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "7ALQKgJ0-mxt",
        "outputId": "7b10b15b-685d-486d-a233-340fadc31f09"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot the emotions distribution as histogram\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.bar(emotions_dict.keys(), emotions_dict.values(), color='blue')\n",
        "plt.title(\"Emotions distribution in the training set\")\n",
        "plt.xlabel(\"Emotions\")\n",
        "plt.ylabel(\"Number of occurences\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQC3STmj-mxt"
      },
      "source": [
        "We can also implement some text processing techniques such as stemming, lemmatization, stop words removal, etc.\n",
        "\n",
        "In this case we will use nltk library for lemmatization and stop words removal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYfAiohQ-mxt",
        "outputId": "7aed0bd6-5c6a-4b49-d94b-b9872896e90d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Ciprian\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Ciprian\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Ciprian\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Ciprian\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "bad_symbols = re.compile('[^a-z ]')\n",
        "punct = string.punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87PMhA2u-mxu"
      },
      "source": [
        "We don't need to use lower case because we are going to use bert uncased models further on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O_PoU-af-mxu"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = bad_symbols.sub('', text)\n",
        "    text = word_tokenize(text)\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    text = ' '.join(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wkjy3t9t-mxu",
        "outputId": "ae2960c5-8b1a-4e73-b500-153d62b19da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of train, val and test sets after preprocessing: \n",
            "Train shape: (3200, 4)\n",
            "Val shape: (400, 4)\n",
            "Test shape: (400, 4)\n"
          ]
        }
      ],
      "source": [
        "baseline_train = df_train.copy()\n",
        "baseline_train[\"utterances\"] = baseline_train[\"utterances\"].apply(lambda x: [preprocess_text(elem) for elem in x])\n",
        "baseline_test = df_test.copy()\n",
        "baseline_test[\"utterances\"] = baseline_test[\"utterances\"].apply(lambda x: [preprocess_text(elem) for elem in x])\n",
        "baseline_val = df_val.copy()\n",
        "baseline_val[\"utterances\"] = baseline_val[\"utterances\"].apply(lambda x: [preprocess_text(elem) for elem in x])\n",
        "print(\"Shape of train, val and test sets after preprocessing: \")\n",
        "print(f\"Train shape: {baseline_train.shape}\")\n",
        "print(f\"Val shape: {baseline_val.shape}\")\n",
        "print(f\"Test shape: {baseline_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ9HYEWk-mxu"
      },
      "source": [
        "In order to use the TF-IDF vectorizer we need to split the utterances into single sentences, likewise the emotions and triggers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0JGlj2Dn-mxu"
      },
      "outputs": [],
      "source": [
        "def splitter(df, y_label):\n",
        "    X = []\n",
        "    y = []\n",
        "    for index, row in df.iterrows():\n",
        "        for i in range(len(row[\"utterances\"])):\n",
        "            X.append(row[\"utterances\"][i])\n",
        "            y.append(row[y_label][i])\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QG7dR_5-mxu",
        "outputId": "f5046e5e-001e-48c8-a3c2-e89f2c9cbbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape after splitting: 27764\n",
            "Val shape after splitting: 3678\n",
            "Test shape after splitting: 3558\n"
          ]
        }
      ],
      "source": [
        "# Emotions baseline\n",
        "x_train_base, y_train_emotions = splitter(baseline_train, \"emotions\")\n",
        "x_val_base, y_val_emotions = splitter(baseline_val, \"emotions\")\n",
        "x_test_base, y_test_emotions = splitter(baseline_test, \"emotions\")\n",
        "\n",
        "# Triggers baseline\n",
        "_ , y_train_triggers = splitter(baseline_train, \"triggers\")\n",
        "_ , y_val_triggers = splitter(baseline_val, \"triggers\")\n",
        "_ , y_test_triggers = splitter(baseline_test, \"triggers\")\n",
        "\n",
        "print(f\"Train shape after splitting: {len(x_train_base)}\")\n",
        "print(f\"Val shape after splitting: {len(x_val_base)}\")\n",
        "print(f\"Test shape after splitting: {len(x_test_base)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF8QlR9Q-mxv"
      },
      "source": [
        "As a tokenizer we use TfidfVectorizer from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6xT-hSj-mxv",
        "outputId": "064b9832-2b56-4b08-ed42-cbaed1313641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape after vectorization: (27764, 5142)\n",
            "Val shape after vectorization: (3678, 5142)\n",
            "Test shape after vectorization: (3558, 5142)\n",
            "Size of the vocabulary: 5142\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "x_train_base = vectorizer.fit_transform(x_train_base)\n",
        "x_val_base = vectorizer.transform(x_val_base)\n",
        "x_test_base = vectorizer.transform(x_test_base)\n",
        "\n",
        "print(f\"Train shape after vectorization: {x_train_base.shape}\")\n",
        "print(f\"Val shape after vectorization: {x_val_base.shape}\")\n",
        "print(f\"Test shape after vectorization: {x_test_base.shape}\")\n",
        "\n",
        "print(f\"Size of the vocabulary: {len(vectorizer.vocabulary_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJsm3qId-mxv"
      },
      "source": [
        "Evaluation function that returns the classification report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "835ywCQu-mxv"
      },
      "outputs": [],
      "source": [
        "def evaluate(Y_test, Y_pred):\n",
        "    report = classification_report(Y_test, Y_pred, zero_division=0)\n",
        "    return report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ishbbG6-mxv"
      },
      "source": [
        "Defining the dummy classifier for emotions and triggers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r_GP91pL-mxv"
      },
      "outputs": [],
      "source": [
        "dummy_clf_majority_emotions = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf_random_emotions = DummyClassifier(strategy=\"uniform\")\n",
        "\n",
        "dummy_clf_majority_triggers = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf_random_triggers = DummyClassifier(strategy=\"uniform\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvYhjUQE-mxv"
      },
      "source": [
        "Training and evaluation of the classifiers fitted on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24CrukE2-mxv",
        "outputId": "b5aa1272-5a20-46e9-8c11-f3cbbd1be9a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Majority classifier for emotions: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.00      0.00      0.00       369\n",
            "     disgust       0.00      0.00      0.00       101\n",
            "        fear       0.00      0.00      0.00       109\n",
            "         joy       0.00      0.00      0.00       663\n",
            "     neutral       0.44      1.00      0.61      1572\n",
            "     sadness       0.00      0.00      0.00       258\n",
            "    surprise       0.00      0.00      0.00       486\n",
            "\n",
            "    accuracy                           0.44      3558\n",
            "   macro avg       0.06      0.14      0.09      3558\n",
            "weighted avg       0.20      0.44      0.27      3558\n",
            "\n",
            "-------------------------------------------------------\n",
            "Random classifier for emotions: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.10      0.13      0.12       369\n",
            "     disgust       0.04      0.18      0.06       101\n",
            "        fear       0.03      0.14      0.05       109\n",
            "         joy       0.19      0.14      0.16       663\n",
            "     neutral       0.43      0.15      0.22      1572\n",
            "     sadness       0.05      0.11      0.07       258\n",
            "    surprise       0.13      0.14      0.13       486\n",
            "\n",
            "    accuracy                           0.14      3558\n",
            "   macro avg       0.14      0.14      0.12      3558\n",
            "weighted avg       0.26      0.14      0.16      3558\n",
            "\n",
            "-------------------------------------------------------\n",
            "Majority classifier for triggers: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      1.00      0.92      3027\n",
            "         1.0       0.00      0.00      0.00       531\n",
            "\n",
            "    accuracy                           0.85      3558\n",
            "   macro avg       0.43      0.50      0.46      3558\n",
            "weighted avg       0.72      0.85      0.78      3558\n",
            "\n",
            "-------------------------------------------------------\n",
            "Random classifier for triggers: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.51      0.64      3027\n",
            "         1.0       0.14      0.47      0.22       531\n",
            "\n",
            "    accuracy                           0.51      3558\n",
            "   macro avg       0.49      0.49      0.43      3558\n",
            "weighted avg       0.74      0.51      0.58      3558\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Emotions baseline\n",
        "dummy_clf_majority_emotions.fit(x_train_base, y_train_emotions)\n",
        "dummy_clf_random_emotions.fit(x_train_base, y_train_emotions)\n",
        "\n",
        "y_pred_majority_emotions = dummy_clf_majority_emotions.predict(x_test_base)\n",
        "y_pred_random_emotions = dummy_clf_random_emotions.predict(x_test_base)\n",
        "print(\"Majority classifier for emotions: \\n\")\n",
        "print(evaluate(y_test_emotions, y_pred_majority_emotions))\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Random classifier for emotions: \\n\")\n",
        "print(evaluate(y_test_emotions, y_pred_random_emotions))\n",
        "print(\"-------------------------------------------------------\")\n",
        "\n",
        "# Triggers baseline\n",
        "dummy_clf_majority_triggers.fit(x_train_base, y_train_triggers)\n",
        "dummy_clf_random_triggers.fit(x_train_base, y_train_triggers)\n",
        "\n",
        "y_pred_majority_triggers = dummy_clf_majority_triggers.predict(x_test_base)\n",
        "y_pred_random_triggers = dummy_clf_random_triggers.predict(x_test_base)\n",
        "print(\"Majority classifier for triggers: \\n\")\n",
        "print(evaluate(y_test_triggers, y_pred_majority_triggers))\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Random classifier for triggers: \\n\")\n",
        "print(evaluate(y_test_triggers, y_pred_random_triggers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ2aj1El-mxw"
      },
      "source": [
        "Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zsCtXMdG-mxw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\NLProject\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "transformers.set_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gffRztVP-mxw",
        "outputId": "b2b9fb2b-233c-4fcf-c6c3-b0f9a0849b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences distribution: [9, 2, 3, 14, 4, 10, 8, 10, 19, 17, 4, 4, 6, 8, 10, 3, 9, 8, 22, 7, 9, 9, 12, 11, 7, 8, 4, 4, 8, 8, 5, 7, 14, 12, 7, 3, 8, 4, 13, 10, 13, 9, 3, 4, 14, 16, 5, 3, 11, 11, 3, 6, 5, 3, 15, 4, 6, 13, 7, 10, 7, 3, 4, 5, 22, 19, 8, 9, 8, 8, 3, 15, 6, 9, 6, 12, 3, 6, 19, 5, 7, 11, 6, 5, 4, 17, 6, 3, 3, 6, 12, 14, 4, 14, 5, 14, 14, 6, 4, 8, 4, 12, 18, 3, 5, 4, 4, 14, 12, 4, 5, 3, 11, 3, 10, 5, 13, 8, 5, 10, 10, 9, 6, 3, 9, 14, 9, 7, 5, 15, 16, 3, 9, 21, 4, 17, 6, 12, 9, 6, 10, 12, 2, 6, 9, 17, 17, 8, 15, 12, 24, 4, 16, 14, 19, 4, 4, 11, 4, 4, 8, 9, 19, 16, 6, 8, 4, 2, 8, 4, 8, 9, 15, 18, 19, 5, 18, 5, 14, 11, 5, 6, 5, 11, 3, 16, 5, 16, 10, 8, 3, 16, 8, 11, 20, 8, 10, 5, 4, 11, 16, 4, 9, 14, 8, 14, 8, 13, 4, 2, 10, 7, 14, 5, 6, 5, 6, 11, 15, 11, 5, 3, 6, 9, 3, 7, 6, 8, 8, 19, 6, 8, 3, 9, 6, 4, 4, 3, 9, 5, 8, 5, 8, 8, 7, 12, 16, 9, 9, 3, 4, 5, 9, 7, 18, 3, 3, 14, 10, 5, 4, 2, 4, 6, 3, 10, 8, 7, 6, 3, 5, 5, 8, 3, 8, 7, 5, 12, 7, 21, 6, 16, 10, 15, 4, 13, 12, 8, 8, 13, 16, 8, 9, 10, 4, 13, 11, 14, 6, 11, 7, 14, 3, 3, 3, 5, 8, 3, 5, 3, 7, 8, 7, 6, 13, 6, 8, 5, 14, 5, 6, 19, 9, 5, 3, 8, 9, 8, 18, 13, 3, 14, 8, 20, 12, 12, 16, 9, 4, 15, 12, 7, 11, 8, 12, 12, 7, 7, 20, 7, 7, 15, 13, 5, 6, 11, 3, 3, 4, 16, 8, 5, 11, 11, 8, 15, 9, 9, 10, 4, 3, 4, 14, 5, 19, 13, 11, 2, 5, 9, 3, 9, 9, 7, 16, 11, 3, 5, 19, 11, 6, 3, 12, 6, 9, 4, 5, 10, 14, 15, 7, 4, 8, 5, 10, 5, 10, 10, 3, 6, 5, 7, 7, 10, 2, 4, 6, 10, 4, 12, 7, 13, 5, 14, 12, 14, 11, 21, 8, 7, 10, 8, 17, 19, 2, 6, 3, 6, 10, 3, 10, 8, 16, 8, 2, 5, 12, 15, 11, 13, 7, 5, 6, 2, 9, 5, 19, 10, 12, 9, 12, 3, 7, 6, 5, 5, 13, 15, 12, 4, 9, 6, 15, 9, 7, 10, 6, 16, 12, 3, 6, 6, 9, 9, 6, 14, 12, 16, 10, 4, 6, 12, 18, 19, 4, 8, 4, 6, 7, 13, 5, 4, 8, 8, 14, 5, 18, 12, 21, 6, 5, 7, 10, 4, 5, 13, 6, 6, 3, 6, 5, 6, 18, 2, 3, 14, 9, 4, 2, 14, 15, 7, 13, 7, 8, 3, 4, 5, 5, 5, 12, 7, 10, 5, 5, 5, 3, 11, 12, 9, 4, 10, 6, 11, 11, 5, 13, 9, 17, 7, 4, 7, 3, 10, 6, 3, 7, 16, 5, 5, 3, 8, 7, 8, 5, 3, 13, 18, 6, 13, 15, 10, 13, 11, 12, 6, 7, 7, 10, 3, 6, 6, 6, 12, 22, 6, 16, 10, 8, 10, 9, 3, 8, 8, 3, 11, 5, 11, 8, 8, 3, 12, 15, 4, 4, 3, 12, 9, 8, 9, 2, 7, 6, 6, 9, 7, 5, 9, 7, 6, 11, 3, 14, 5, 17, 9, 7, 14, 4, 6, 8, 6, 3, 4, 11, 8, 6, 7, 21, 6, 15, 3, 8, 9, 11, 11, 5, 16, 5, 2, 12, 4, 10, 12, 8, 6, 13, 17, 4, 6, 9, 6, 19, 6, 9, 7, 10, 11, 9, 3, 15, 7, 12, 8, 11, 22, 14, 11, 6, 3, 11, 5, 4, 7, 12, 3, 10, 11, 14, 13, 2, 10, 4, 6, 5, 11, 15, 3, 10, 7, 14, 8, 10, 6, 4, 7, 15, 9, 3, 6, 4, 8, 11, 6, 12, 7, 9, 11, 7, 4, 12, 13, 14, 6, 4, 12, 6, 3, 7, 11, 11, 3, 4, 10, 2, 7, 8, 13, 10, 4, 3, 8, 12, 7, 11, 9, 14, 11, 8, 3, 10, 4, 4, 9, 3, 5, 10, 5, 20, 8, 13, 15, 10, 12, 5, 4, 7, 8, 3, 7, 10, 7, 16, 7, 5, 9, 10, 4, 3, 19, 19, 8, 6, 16, 3, 6, 6, 9, 6, 8, 6, 8, 4, 3, 9, 4, 14, 10, 10, 7, 6, 5, 16, 3, 8, 12, 10, 6, 7, 15, 4, 14, 3, 5, 11, 11, 8, 4, 4, 11, 9, 7, 17, 10, 10, 9, 6, 7, 4, 11, 5, 12, 5, 18, 8, 5, 7, 6, 8, 6, 7, 18, 8, 6, 15, 8, 8, 4, 10, 12, 7, 4, 4, 4, 12, 10, 8, 10, 13, 22, 16, 11, 3, 2, 3, 6, 9, 3, 12, 12, 3, 14, 3, 11, 4, 6, 3, 16, 18, 8, 5, 16, 7, 4, 7, 6, 13, 18, 7, 17, 9, 18, 9, 11, 13, 14, 5, 9, 9, 5, 21, 3, 3, 9, 8, 11, 13, 4, 15, 14, 20, 4, 8, 12, 8, 3, 9, 9, 4, 19, 8, 3, 6, 10, 11, 13, 6, 4, 6, 9, 3, 6, 15, 6, 3, 3, 4, 4, 4, 17, 7, 16, 3, 9, 15, 21, 15, 18, 10, 14, 6, 10, 18, 4, 10, 6, 6, 3, 8, 6, 14, 7, 6, 2, 6, 10, 3, 5, 6, 5, 8, 10, 11, 5, 7, 13, 13, 4, 16, 11, 12, 6, 5, 6, 6, 13, 6, 11, 11, 15, 6, 6, 7, 8, 9, 5, 9, 4, 7, 5, 4, 17, 9, 10, 12, 3, 8, 4, 3, 15, 6, 9, 6, 8, 4, 12, 3, 8, 16, 2, 11, 9, 4, 5, 2, 6, 15, 9, 5, 3, 7, 8, 9, 8, 5, 6, 6, 8, 12, 4, 5, 13, 10, 11, 16, 18, 10, 4, 11, 8, 5, 9, 9, 4, 13, 4, 3, 7, 14, 19, 8, 10, 18, 4, 8, 3, 14, 3, 8, 20, 6, 3, 7, 7, 16, 14, 8, 7, 8, 9, 6, 5, 5, 13, 7, 10, 6, 4, 4, 8, 10, 4, 4, 6, 10, 16, 6, 8, 4, 19, 3, 2, 2, 4, 12, 5, 7, 4, 20, 16, 10, 4, 6, 8, 16, 9, 7, 10, 9, 9, 11, 17, 17, 11, 4, 3, 17, 7, 3, 10, 2, 4, 7, 10, 14, 6, 6, 4, 5, 9, 5, 12, 8, 12, 11, 7, 6, 6, 4, 8, 3, 6, 3, 3, 9, 3, 5, 8, 6, 12, 5, 10, 6, 15, 12, 14, 17, 14, 17, 17, 22, 12, 8, 14, 10, 17, 19, 4, 12, 7, 8, 2, 10, 17, 4, 6, 4, 8, 6, 23, 9, 6, 9, 3, 4, 11, 5, 9, 9, 10, 11, 18, 9, 6, 5, 17, 12, 9, 20, 20, 3, 6, 19, 14, 11, 15, 4, 9, 6, 4, 20, 9, 15, 5, 6, 8, 8, 8, 23, 10, 8, 7, 11, 10, 12, 9, 12, 13, 4, 8, 6, 11, 11, 14, 16, 9, 10, 4, 4, 7, 12, 5, 9, 8, 7, 9, 6, 3, 3, 17, 14, 7, 20, 7, 6, 11, 4, 6, 14, 13, 7, 9, 3, 4, 4, 15, 4, 4, 3, 10, 5, 10, 3, 6, 5, 9, 7, 3, 4, 6, 4, 4, 4, 3, 7, 13, 6, 4, 4, 5, 7, 3, 10, 10, 9, 9, 7, 6, 5, 11, 10, 11, 8, 6, 12, 10, 4, 4, 8, 9, 14, 21, 11, 4, 7, 3, 7, 15, 3, 6, 14, 5, 19, 7, 5, 4, 8, 13, 6, 14, 22, 5, 3, 11, 4, 3, 14, 5, 17, 6, 8, 5, 9, 4, 6, 16, 3, 3, 10, 21, 10, 17, 17, 6, 13, 4, 4, 10, 4, 12, 12, 7, 13, 15, 12, 7, 6, 14, 7, 2, 7, 5, 14, 6, 3, 4, 5, 6, 7, 6, 16, 9, 4, 9, 7, 12, 5, 5, 4, 8, 13, 4, 15, 7, 11, 6, 17, 3, 12, 9, 14, 11, 15, 11, 10, 7, 13, 11, 4, 3, 7, 7, 4, 21, 5, 10, 4, 16, 5, 9, 13, 4, 14, 7, 5, 5, 10, 10, 2, 6, 11, 4, 6, 15, 14, 9, 13, 8, 7, 3, 4, 13, 7, 5, 3, 8, 3, 11, 16, 10, 18, 13, 8, 12, 8, 5, 3, 11, 2, 11, 9, 5, 15, 11, 2, 13, 4, 12, 11, 12, 9, 9, 15, 7, 7, 16, 8, 5, 9, 10, 3, 4, 5, 6, 16, 3, 6, 14, 9, 6, 5, 9, 9, 3, 11, 5, 6, 8, 5, 7, 7, 4, 15, 5, 9, 21, 9, 8, 9, 6, 3, 3, 10, 3, 11, 5, 6, 8, 5, 18, 3, 9, 16, 10, 7, 11, 19, 4, 7, 9, 13, 15, 14, 6, 15, 10, 2, 12, 12, 7, 5, 11, 5, 5, 12, 4, 18, 13, 5, 3, 4, 17, 3, 5, 13, 7, 6, 5, 6, 9, 17, 3, 7, 3, 12, 14, 12, 4, 3, 8, 4, 4, 4, 15, 6, 11, 8, 5, 9, 18, 9, 10, 9, 14, 15, 3, 12, 6, 15, 10, 7, 4, 15, 11, 4, 9, 20, 6, 13, 6, 8, 8, 17, 4, 21, 5, 7, 2, 9, 10, 2, 7, 9, 6, 4, 5, 5, 6, 6, 4, 13, 14, 17, 6, 14, 17, 7, 8, 18, 8, 16, 4, 9, 9, 10, 21, 11, 15, 3, 11, 18, 10, 6, 2, 6, 6, 7, 10, 3, 6, 11, 5, 5, 9, 3, 6, 6, 7, 6, 7, 10, 3, 5, 9, 9, 11, 4, 3, 4, 9, 7, 11, 13, 4, 11, 5, 5, 4, 10, 3, 13, 7, 14, 13, 6, 5, 8, 7, 15, 4, 3, 7, 6, 10, 6, 7, 6, 12, 12, 3, 6, 4, 17, 12, 13, 9, 7, 4, 6, 10, 14, 8, 11, 10, 6, 6, 3, 15, 7, 4, 10, 4, 16, 3, 18, 5, 10, 9, 3, 16, 11, 9, 9, 12, 18, 11, 10, 11, 7, 10, 9, 10, 6, 14, 4, 5, 12, 3, 15, 12, 3, 8, 3, 16, 14, 18, 12, 7, 8, 14, 5, 5, 16, 4, 4, 9, 4, 7, 6, 4, 12, 6, 7, 5, 8, 10, 5, 12, 11, 2, 8, 5, 11, 5, 3, 9, 3, 12, 5, 5, 20, 3, 15, 4, 18, 2, 14, 12, 7, 17, 8, 16, 11, 5, 5, 4, 7, 8, 7, 2, 2, 9, 10, 13, 8, 7, 5, 11, 3, 11, 13, 3, 4, 10, 13, 11, 7, 3, 14, 13, 7, 10, 2, 4, 4, 17, 13, 3, 5, 14, 13, 11, 8, 8, 14, 5, 10, 3, 9, 16, 9, 9, 16, 12, 8, 14, 13, 3, 3, 10, 8, 10, 6, 5, 5, 16, 5, 9, 7, 12, 7, 5, 12, 18, 5, 12, 10, 3, 14, 10, 10, 11, 17, 4, 13, 9, 6, 10, 19, 15, 17, 6, 18, 3, 9, 3, 6, 12, 6, 9, 3, 9, 9, 10, 11, 4, 15, 5, 12, 13, 8, 7, 9, 14, 3, 9, 21, 3, 5, 21, 4, 4, 12, 4, 3, 5, 18, 10, 8, 13, 6, 12, 6, 14, 6, 8, 5, 3, 19, 5, 9, 5, 12, 7, 16, 8, 13, 5, 13, 7, 8, 8, 7, 11, 21, 7, 3, 5, 10, 6, 3, 11, 16, 6, 3, 15, 7, 6, 9, 7, 7, 9, 17, 9, 8, 13, 4, 6, 5, 4, 10, 8, 4, 14, 11, 5, 4, 14, 4, 7, 10, 3, 3, 3, 10, 10, 3, 3, 9, 5, 13, 6, 9, 11, 3, 4, 10, 5, 9, 6, 8, 11, 9, 8, 4, 8, 10, 5, 4, 5, 9, 8, 12, 17, 15, 20, 15, 7, 9, 3, 7, 11, 5, 18, 3, 10, 5, 8, 13, 10, 3, 17, 5, 3, 7, 10, 2, 5, 11, 11, 10, 4, 22, 9, 8, 5, 4, 7, 6, 8, 4, 5, 16, 9, 15, 11, 4, 14, 12, 11, 6, 7, 9, 7, 8, 7, 8, 15, 5, 9, 9, 3, 7, 5, 10, 21, 15, 12, 11, 3, 6, 16, 12, 13, 7, 15, 2, 4, 16, 14, 10, 18, 7, 16, 4, 13, 13, 7, 3, 5, 3, 13, 2, 6, 6, 4, 22, 8, 8, 14, 5, 5, 3, 8, 3, 2, 12, 6, 3, 8, 6, 5, 20, 19, 4, 6, 8, 4, 6, 8, 3, 3, 15, 5, 14, 5, 8, 11, 11, 16, 5, 6, 11, 12, 20, 6, 5, 16, 4, 13, 15, 3, 11, 5, 12, 5, 5, 19, 9, 13, 11, 5, 19, 4, 16, 13, 4, 24, 16, 10, 14, 3, 10, 13, 8, 3, 6, 11, 16, 5, 4, 10, 10, 13, 6, 8, 8, 5, 17, 12, 5, 4, 12, 3, 6, 2, 4, 12, 13, 4, 11, 8, 2, 15, 15, 3, 18, 12, 5, 6, 6, 6, 3, 3, 8, 11, 11, 9, 16, 7, 4, 4, 13, 7, 2, 10, 8, 9, 7, 10, 8, 2, 10, 7, 6, 11, 11, 2, 11, 5, 7, 14, 7, 13, 3, 6, 6, 2, 4, 9, 7, 18, 7, 3, 13, 11, 7, 8, 5, 7, 21, 8, 4, 4, 3, 12, 14, 5, 8, 9, 13, 6, 3, 6, 9, 12, 5, 3, 6, 17, 10, 10, 11, 5, 9, 7, 14, 14, 8, 10, 5, 5, 9, 6, 2, 3, 13, 13, 7, 12, 17, 8, 12, 7, 4, 10, 5, 4, 2, 9, 11, 3, 4, 15, 4, 6, 14, 10, 12, 12, 9, 12, 22, 6, 8, 4, 11, 16, 3, 7, 16, 18, 19, 11, 5, 12, 10, 10, 18, 9, 16, 10, 10, 18, 5, 16, 8, 3, 12, 6, 8, 18, 13, 18, 22, 6, 14, 17, 4, 4, 15, 24, 9, 5, 4, 6, 7, 7, 18, 6, 3, 5, 11, 6, 4, 4, 6, 5, 15, 8, 9, 14, 2, 6, 9, 11, 6, 18, 9, 3, 16, 5, 8, 4, 8, 10, 6, 18, 4, 5, 3, 4, 5, 9, 6, 8, 13, 4, 8, 8, 10, 10, 6, 8, 8, 6, 8, 3, 8, 11, 3, 4, 3, 7, 3, 14, 7, 16, 4, 13, 5, 12, 4, 17, 8, 15, 15, 3, 3, 9, 3, 12, 11, 11, 7, 4, 5, 15, 11, 2, 17, 17, 5, 4, 9, 3, 3, 7, 17, 20, 6, 18, 18, 6, 15, 19, 17, 4, 19, 12, 7, 18, 11, 3, 8, 3, 9, 5, 4, 6, 7, 5, 13, 6, 6, 4, 8, 7, 7, 3, 5, 12, 9, 6, 4, 8, 8, 8, 3, 17, 3, 11, 5, 8, 12, 5, 14, 8, 7, 5, 7, 10, 9, 6, 3, 6, 5, 3, 13, 9, 8, 6, 12, 12, 9, 7, 10, 8, 3, 7, 7, 21, 6, 14, 9, 16, 11, 17, 7, 10, 2, 8, 6, 12, 4, 6, 9, 11, 20, 15, 5, 9, 7, 7, 10, 16, 12, 7, 5, 11, 11, 5, 11, 7, 18, 4, 17, 15, 6, 8, 6, 3, 18, 7, 9, 6, 7, 8, 7, 12, 4, 18, 16, 8, 18, 3, 10, 8, 4, 16, 11, 9, 6, 5, 6, 4, 8, 3, 19, 9, 6, 2, 13, 6, 7, 9, 14, 11, 11, 4, 11, 3, 7, 4, 6, 4, 8, 6, 3, 17, 11, 4, 12, 15, 8, 14, 4, 9, 17, 15, 7, 10, 4, 9, 6, 5, 12, 9, 13, 16, 7, 14, 16, 3, 6, 5, 16, 8, 6, 4, 7, 5, 3, 14, 11, 4, 7, 4, 6, 4, 9, 16, 11, 4, 6, 18, 14, 6, 7, 21, 6, 8, 4, 16, 4, 12, 6, 5, 4, 13, 7, 12, 6, 4, 7, 8, 9, 12, 11, 6, 11, 6, 15, 4, 11, 8, 13, 9, 12, 5, 19, 5, 4, 3, 7, 20, 5, 2, 4, 3, 3, 4, 13, 8, 3, 11, 5, 15, 12, 13, 7, 9, 3, 7, 14, 9, 7, 5, 7, 4, 6, 9, 9, 15, 7, 4, 17, 5, 7, 5, 4, 9, 13, 17, 15, 20, 6, 4, 6, 7, 7, 7, 12, 9, 4, 14, 3, 11, 7, 17, 3, 8, 15, 6, 5, 8, 10, 9, 6, 5, 12, 15, 13, 4, 5, 4, 21, 7, 8, 13, 3, 3, 5, 7, 6, 7, 11, 15, 6, 5, 12, 4, 5, 12, 7, 11, 3, 3, 12, 6, 5, 9, 3, 10, 5, 9, 6, 4, 14, 5, 3, 7, 4, 11, 5, 8, 3, 3, 4, 4, 13, 10, 8, 7, 6, 20, 3, 4, 3, 11, 9, 12, 3, 9, 4, 12, 3, 21, 3, 9, 10, 11, 9, 10, 8, 3, 4, 17, 10, 4, 11, 20, 13, 16, 17, 8, 17, 5, 6, 11, 10, 11, 10, 3, 10, 15, 12, 4, 19, 6, 20, 10, 9, 8, 6, 13, 9, 4, 4, 8, 14, 19, 9, 5, 5, 14, 11, 4, 8, 10, 10, 8, 7, 13, 7, 13, 3, 12, 6, 10, 10, 17, 4, 6, 8, 11, 15, 6, 9, 3, 9, 10, 13, 3, 10, 13, 6, 9, 4, 3, 4, 4, 4, 11, 11, 9, 15, 10, 5, 16, 7, 13, 3, 13, 6, 4, 3, 22, 5, 8, 15, 3, 11, 4, 23, 12, 3, 7, 11, 6, 7, 6, 12, 19, 12, 9, 12, 16, 13, 6, 5, 6, 8, 9, 3, 6, 16, 8, 17, 2, 10, 8, 6, 8, 6, 5, 3, 10, 8, 7, 6, 6, 18, 16, 6, 5, 7, 9, 5, 9, 4, 14, 7, 5, 16, 15, 7, 6, 16, 3, 15, 6, 16, 11, 7, 11, 13, 5, 2, 19, 5, 9, 6, 9, 10, 4, 7, 8, 10, 3, 12, 3, 15, 11, 9, 5, 11, 8, 4, 8, 7, 6, 7, 8, 13, 4, 5, 3, 17, 6, 22, 7, 3, 13, 19, 17, 11, 3, 4, 14, 9, 7, 8, 12, 11, 4, 10, 7, 8, 4, 7, 15, 11, 16, 10, 2, 5, 6, 5, 7, 14, 3, 10, 7, 5, 3, 4, 6, 7, 17, 5, 4, 21, 5, 17, 15, 4, 10, 6, 8, 5, 7, 5, 17, 6, 3, 13, 3, 3, 10, 4, 5, 13, 7, 19, 3, 3, 11, 8, 7, 14, 14, 10, 7, 12, 7, 10, 5, 13, 16, 7, 3, 17, 3, 2, 13, 6, 12, 14, 3, 6, 7, 12, 7, 3, 9, 10, 14, 7, 14, 12, 23, 3, 6, 8, 6, 11, 8, 15, 10, 10, 13, 9, 11, 10, 6, 4, 2, 6, 16, 18, 13, 10, 3, 8, 5, 16, 3, 10, 5]\n",
            "Max number of sentences: 24\n"
          ]
        }
      ],
      "source": [
        "# get number of sentencese distribution\n",
        "num_sentences = []\n",
        "for index, row in df_train.iterrows():\n",
        "    num_sentences.append(len(row[\"utterances\"]))\n",
        "print(f\"Number of sentences distribution: {num_sentences}\")\n",
        "print(f\"Max number of sentences: {max(num_sentences)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "e3kggk1g-mxw"
      },
      "outputs": [],
      "source": [
        "bert_train = df_train.copy()\n",
        "bert_val = df_val.copy()\n",
        "bert_test = df_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XDrUUvu1-mxw"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndef chain_utterances(df):\\n    # add column to df\\n    df_copy = df.copy()\\n\\n    for i in range(len(df[\"utterances\"])):\\n        speakers = []\\n        sentence = \"\"\\n        for j in range(len(df.iloc[i][\"utterances\"])):\\n            df_copy.iloc[i][\"emotions\"][j] = df.iloc[i][\"emotions\"][j] + str(int(df.iloc[i][\"triggers\"][j]))\\n    return df_copy\\n'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chain utterances in one sentence while also adding the speaker + plus change emotions to numeric \n",
        "'''\n",
        "def chain_utterances(df):\n",
        "    # add column to df\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    for i in range(len(df[\"utterances\"])):\n",
        "        speakers = []\n",
        "        sentence = \"\"\n",
        "        for j in range(len(df.iloc[i][\"utterances\"])):\n",
        "            df_copy.iloc[i][\"emotions\"][j] = df.iloc[i][\"emotions\"][j] + str(int(df.iloc[i][\"triggers\"][j]))\n",
        "    return df_copy\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ncolumns = [\"speakers\", \"emotions\", \"utterances\", \"triggers\"]\\n\\ndef splitter_bert(df):\\n    temp = pd.DataFrame(columns=columns)\\n    for i in range(df.shape[0]):\\n        for j, _ in enumerate(df.iloc[i][\\'speakers\\']):\\n            new_row = pd.DataFrame({\\'speakers\\': [df.iloc[i][\\'speakers\\']],\\n                        \\'emotions\\': [df.iloc[i][\\'emotions\\'][:j+1]],\\n                        \\'utterances\\': [df.iloc[i][\\'utterances\\']],\\n                        \\'triggers\\': [df.iloc[i][\\'triggers\\'][:j+1]]})\\n            temp = pd.concat([temp, new_row], ignore_index=True)\\n    return temp\\n'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "columns = [\"speakers\", \"emotions\", \"utterances\", \"triggers\"]\n",
        "\n",
        "def splitter_bert(df):\n",
        "    temp = pd.DataFrame(columns=columns)\n",
        "    for i in range(df.shape[0]):\n",
        "        for j, _ in enumerate(df.iloc[i]['speakers']):\n",
        "            new_row = pd.DataFrame({'speakers': [df.iloc[i]['speakers']],\n",
        "                        'emotions': [df.iloc[i]['emotions'][:j+1]],\n",
        "                        'utterances': [df.iloc[i]['utterances']],\n",
        "                        'triggers': [df.iloc[i]['triggers'][:j+1]]})\n",
        "            temp = pd.concat([temp, new_row], ignore_index=True)\n",
        "    return temp\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def splitter_bert(df):\n",
        "    columns = [\"label\", \"sentence\", \"trigger\"]\n",
        "     # split the utteracnes into sentences with their corresponding labels\n",
        "    new_df = pd.DataFrame(columns=columns)\n",
        "    for index, row in df.iterrows():\n",
        "        for i in range(len(row[\"utterances\"])):\n",
        "            new_row = pd.DataFrame({\"label\": [row[\"emotions\"][i]], \"sentence\": [row[\"utterances\"][i]], \"trigger\": [row[\"triggers\"][i]]})\n",
        "            new_df = pd.concat([new_df, new_row], ignore_index=True)\n",
        "    return new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ciprian\\AppData\\Local\\Temp\\ipykernel_16716\\2463025762.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  new_df = pd.concat([new_df, new_row], ignore_index=True)\n",
            "C:\\Users\\Ciprian\\AppData\\Local\\Temp\\ipykernel_16716\\2463025762.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  new_df = pd.concat([new_df, new_row], ignore_index=True)\n",
            "C:\\Users\\Ciprian\\AppData\\Local\\Temp\\ipykernel_16716\\2463025762.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  new_df = pd.concat([new_df, new_row], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of train, val and test sets after splitting: \n",
            "Train shape: (27764, 3)\n",
            "Val shape: (3678, 3)\n",
            "Test shape: (3558, 3)\n",
            "-------------------------------------------------------\n",
            "Example of a sample: \n",
            "label       neutral\n",
            "sentence       Hey.\n",
            "trigger         0.0\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# split train, val and test sets for BERT\n",
        "bert_train = splitter_bert(bert_train)\n",
        "bert_val = splitter_bert(bert_val)\n",
        "bert_test = splitter_bert(bert_test)\n",
        "print(\"Shape of train, val and test sets after splitting: \")\n",
        "print(f\"Train shape: {bert_train.shape}\")\n",
        "print(f\"Val shape: {bert_val.shape}\")\n",
        "print(f\"Test shape: {bert_test.shape}\")\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Example of a sample: \")\n",
        "print(bert_train.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "      <th>trigger</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Hey.</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>joy</td>\n",
              "      <td>Hey!</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>So how was Joan?</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>neutral</td>\n",
              "      <td>I broke up with her.</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>surprise</td>\n",
              "      <td>Don't tell me, because of the big nostril thing?</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                          sentence  trigger\n",
              "0   neutral                                              Hey.      0.0\n",
              "1       joy                                              Hey!      0.0\n",
              "2   neutral                                  So how was Joan?      0.0\n",
              "3   neutral                              I broke up with her.      0.0\n",
              "4  surprise  Don't tell me, because of the big nostril thing?      0.0"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# one hot encode the emotions\n",
        "emotions_one_hot_dict = {\n",
        "    \"neutral\": [1, 0, 0, 0, 0, 0, 0],\n",
        "    \"joy\": [0, 1, 0, 0, 0, 0, 0],\n",
        "    \"surprise\": [0, 0, 1, 0, 0, 0, 0],\n",
        "    \"sadness\": [0, 0, 0, 1, 0, 0, 0],\n",
        "    \"anger\": [0, 0, 0, 0, 1, 0, 0],\n",
        "    \"disgust\": [0, 0, 0, 0, 0, 1, 0],\n",
        "    \"fear\": [0, 0, 0, 0, 0, 0, 1]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "# one hot encode the triggers\n",
        "triggers_one_hot_dict = {\n",
        "    0: [1, 0],\n",
        "    1: [0, 1]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "emotion_to_index = {\n",
        "    \"neutral\": 0,\n",
        "    \"joy\": 1,\n",
        "    \"surprise\": 2,\n",
        "    \"sadness\": 3,\n",
        "    \"anger\": 4,\n",
        "    \"disgust\": 5,\n",
        "    \"fear\": 6\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of sentences: 327\n"
          ]
        }
      ],
      "source": [
        "# get max length of sentences\n",
        "max_len = 0\n",
        "for sentence in bert_train[\"sentence\"]:\n",
        "    if len(sentence) > max_len:\n",
        "        max_len = len(sentence)\n",
        "print(f\"Max length of sentences: {max_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = 'bert-base-uncased'\n",
        "#MODEL_NAME = \"prajjwal1/bert-tiny\"\n",
        "\n",
        "MAX_LEN = 360\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-05\n",
        "OUT_CHANNELS = 768 if \"base\" in  MODEL_NAME else 128 #128 if \"tiny\" 1024 if \"base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, tokenizer, max_len, test_flag = False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.text = data.sentence\n",
        "        self.emotions = data.label\n",
        "        self.max_len = max_len\n",
        "        self.test_flag = test_flag\n",
        "        self.triggers = data.trigger\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        #speakers = row[\"speakers\"]\n",
        "        text = row[\"sentence\"]\n",
        "        emotions = row[\"label\"]\n",
        "        trigger = row[\"trigger\"]\n",
        "        target_emotion = emotions_one_hot_dict[emotions] #+ [triggers[-1]] # trigger is float while emotions are one hot encoded but as integers IN CASE OF ERROR CHECK THIS\n",
        "        target_trigger = triggers_one_hot_dict[trigger]\n",
        "\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'label': torch.tensor(target_emotion, dtype=torch.float),\n",
        "            'trigger': torch.tensor(target_trigger, dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(bert_train, tokenizer, MAX_LEN)\n",
        "val_dataset = CustomDataset(bert_val, tokenizer, MAX_LEN, test_flag=True)\n",
        "test_dataset = CustomDataset(bert_test, tokenizer, MAX_LEN, test_flag=True)\n",
        "\n",
        "# Definiition of the Dataloader that will feed the data in batches to the neural network for suitable training and processing.\n",
        "training_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([ 101, 4931, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'label': tensor([1., 0., 0., 0., 0., 0., 0.]),\n",
              " 'trigger': tensor([1., 0.])}"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self,model):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.AutoModel.from_pretrained(model, return_dict=False)\n",
        "        # Emotions\n",
        "        self.l2 = torch.nn.Dropout(p=0.3)\n",
        "        self.l3 = torch.nn.Linear(OUT_CHANNELS, 7)\n",
        "        # Triggers\n",
        "        self.l4 = torch.nn.Dropout(p=0.3)\n",
        "        self.l5 = torch.nn.Linear(OUT_CHANNELS, 2)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_emotions = self.l2(output_1)\n",
        "        output_emotions = self.l3(output_emotions)\n",
        "        output_trigger = self.l4(output_1)\n",
        "        output_trigger = self.l5(output_trigger)\n",
        "        return output_emotions, output_trigger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=768, out_features=7, bias=True)\n",
              "  (l4): Dropout(p=0.3, inplace=False)\n",
              "  (l5): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BERTClass(MODEL_NAME)\n",
        "'''\n",
        "for name, param in model.named_parameters():\n",
        "     if name.startswith(\"l1\"): # choose whatever you like here\n",
        "        param.requires_grad = False\n",
        "'''\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "def compute_metrics(output_info):\n",
        "    predictions, labels = output_info\n",
        "    predictions = np.argmax(predictions, axis=-1)\n",
        "    \n",
        "    f1 = f1_score(y_pred=predictions, y_true=labels, average='macro')\n",
        "    acc = accuracy_score(y_pred=predictions, y_true=labels)\n",
        "    return {'f1': f1, 'acc': acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights: [1.23830338 4.67722372 4.46151374 0.7954845  0.32871587 1.88153971\n",
            " 1.08250156]\n"
          ]
        }
      ],
      "source": [
        "#compute class weights based on distribution of classes in training set\n",
        "from sklearn.utils import class_weight\n",
        "y_train = []\n",
        "for index, row in df_train.iterrows():\n",
        "    for i in range(len(row[\"utterances\"])):\n",
        "        y_train.append(row[\"emotions\"][i])\n",
        "class_weights_emotions = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "print(f\"Class weights: {class_weights_emotions}\")\n",
        "class_weights_emotions = torch.tensor(class_weights_emotions, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class weights: [0.59643394 3.09244821]\n"
          ]
        }
      ],
      "source": [
        "y_train = []\n",
        "for index, row in df_train.iterrows():\n",
        "    for i in range(len(row[\"utterances\"])):\n",
        "        y_train.append(row[\"triggers\"][i])\n",
        "class_weights_triggers = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "print(f\"Class weights: {class_weights_triggers}\")\n",
        "class_weights_triggers = torch.tensor(class_weights_triggers, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss(output_emotion, output_trigger, targets_emotion, targets_trigger):\n",
        "    label_emotion = targets_emotion\n",
        "    label_trigger = targets_trigger\n",
        "    # compute custom loss \n",
        "    loss_fct_emo = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights_emotions).to(device))\n",
        "    loss = loss_fct_emo(output_emotion, label_emotion)\n",
        "    loss_fct_targ = torch.nn.BCEWithLogitsLoss(weight=torch.tensor(class_weights_triggers).to(device))\n",
        "    loss += loss_fct_targ(output_trigger, label_trigger)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=(0.1*(len(training_loader))*EPOCHS), num_training_steps=len(training_loader)*EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train():\n",
        "    size = len(training_loader.dataset)\n",
        "    model.train()\n",
        "    for batch,data in enumerate(training_loader, 0):\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        target_emotion = data['label'].to(device, dtype = torch.float)\n",
        "        target_trigger = data['trigger'].to(device, dtype = torch.float)\n",
        "        \n",
        "        output_emotion, output_trigger = model(ids, mask, token_type_ids)\n",
        "        #loss = loss_fn(outputs, targets)\n",
        "        loss = compute_loss(output_emotion, output_trigger, target_emotion, target_trigger)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if batch%1000==0:\n",
        "            current =  batch * len(data['input_ids'])\n",
        "            print(f\"Train loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation(epoch, val_loss_min_input):\n",
        "    num_batches = len(val_loader)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, data in enumerate(val_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            target_emotion = data['label'].to(device, dtype = torch.float)\n",
        "            target_trigger = data['trigger'].to(device, dtype = torch.float)\n",
        "\n",
        "            output_emotion, output_target = model(ids, mask, token_type_ids)\n",
        "            val_loss += compute_loss(output_emotion, output_target, target_emotion, target_trigger).item()\n",
        "\n",
        "        val_loss /= num_batches\n",
        "        #outputs, targets = fin_outputs, fin_targets\n",
        "        print(f\"\\nValidation loss: {val_loss:>8f}.\")\n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if val_loss <= val_loss_min_input:\n",
        "            #create checkpoint variable and add important data\n",
        "            if epoch > 0:\n",
        "                print('Validation loss decreased ({:.8f} --> {:.8f}).  Saving model ...'.format(val_loss_min_input, val_loss))\n",
        "            else: print('Saving model ...')\n",
        "            # save best moel\n",
        "            torch.save(model.state_dict(), \"model_bert_standard_project.pth\")\n",
        "            print(\"Saved PyTorch Model State to model.pth\\n\")\n",
        "            val_loss_min_input = val_loss\n",
        "\n",
        "    return val_loss_min_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_loss_min_st = np.inf\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train()\n",
        "    val_loss_min_st = validation(epoch, val_loss_min_st)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inference(model, test_load):\n",
        "    model.eval()\n",
        "    fin_targets_emotion = []\n",
        "    fin_outputs_emotion = []\n",
        "    fin_targets_trigger = []\n",
        "    fin_outputs_trigger = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, data in enumerate(test_load, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            target_emotion = data['label'].to(device, dtype = torch.float)\n",
        "            target_trigger = data['trigger'].to(device, dtype = torch.float)\n",
        "\n",
        "            output_emotion, output_trigger = model(ids, mask, token_type_ids)\n",
        "            fin_targets_emotion.extend(torch.argmax(target_emotion, axis=1).cpu().detach().numpy().tolist())\n",
        "            fin_outputs_emotion.extend(torch.argmax(output_emotion, axis=1).cpu().detach().numpy().tolist())\n",
        "            fin_targets_trigger.extend(target_trigger.cpu().detach().numpy().tolist())\n",
        "            fin_outputs_trigger.extend(torch.sigmoid(output_trigger).cpu().detach().numpy().tolist())\n",
        "        \n",
        "    return fin_outputs_emotion, fin_targets_emotion, fin_outputs_trigger, fin_targets_trigger\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs_emotion, targets_emotion, outputs_trigger, targets_trigger = inference(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_threshold(targets, outputs):\n",
        "    results = {}\n",
        "    for tr in np.arange(0.1, 0.9, 0.1):\n",
        "        tr = round(tr, 2)\n",
        "        predictions= np.array(outputs) >= tr\n",
        "        results[tr] = f1_score(targets, predictions, average='macro')\n",
        "    return max(results, key=results.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold: 0.1\n",
            "Report for triggers: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      1.00      0.92      3027\n",
            "           1       0.15      0.99      0.26       531\n",
            "\n",
            "   micro avg       0.50      1.00      0.67      3558\n",
            "   macro avg       0.50      1.00      0.59      3558\n",
            "weighted avg       0.75      1.00      0.82      3558\n",
            " samples avg       0.50      1.00      0.67      3558\n",
            "\n",
            "-------------------------------------------------------\n",
            "Report for emotions: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     neutral       0.65      0.89      0.75      1572\n",
            "         joy       0.58      0.55      0.57       663\n",
            "    surprise       0.46      0.74      0.57       486\n",
            "     sadness       0.00      0.00      0.00       258\n",
            "       anger       0.00      0.00      0.00       369\n",
            "     disgust       0.00      0.00      0.00       101\n",
            "        fear       0.00      0.00      0.00       109\n",
            "\n",
            "    accuracy                           0.60      3558\n",
            "   macro avg       0.24      0.31      0.27      3558\n",
            "weighted avg       0.46      0.60      0.51      3558\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\NLProject\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "d:\\NLProject\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "d:\\NLProject\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "th = get_threshold(targets_trigger, outputs_trigger)\n",
        "print(f\"Threshold: {th}\")\n",
        "predictions_trigger = np.array(outputs_trigger) >= th\n",
        "report_triggers = classification_report(targets_trigger, predictions_trigger)\n",
        "report_emotions = classification_report(targets_emotion, outputs_emotion, target_names=emotion_to_index.keys())\n",
        "print(\"Report for triggers: \\n\")\n",
        "print(report_triggers)\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Report for emotions: \\n\")\n",
        "print(report_emotions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "model = None\n",
        "del model\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
