{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "005CLkdt-mxo"
      },
      "source": [
        "# NLP Standard Project:\n",
        "\n",
        "- Students: **Matteo Belletti**, **Alessandro Pasi**, **Stricescu Razvan Ciprian**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOs950_0-mxp",
        "outputId": "c4d18b00-02b6-4c2d-dafa-918122bf673a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
        "from sklearn.dummy import DummyClassifier\n",
        "import pandas as pd\n",
        "import json\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck-GQq5d-mxr"
      },
      "source": [
        "## Data loading and preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvWV-5RY-mxr",
        "outputId": "5740fa40-b3a9-4663-bebe-7b54ece86213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 4000\n",
            "Example of a sample: {'episode': 'utterance_0', 'speakers': ['Chandler', 'The Interviewer', 'Chandler', 'The Interviewer', 'Chandler'], 'emotions': ['neutral', 'neutral', 'neutral', 'neutral', 'surprise'], 'utterances': [\"also I was the point person on my company's transition from the KL-5 to GR-6 system.\", \"You must've had your hands full.\", 'That I did. That I did.', \"So let's talk a little bit about your duties.\", 'My duties?  All right.'], 'triggers': [0.0, 0.0, 0.0, 1.0, 0.0]}\n"
          ]
        }
      ],
      "source": [
        "# open json in project_data_MELD folder\n",
        "with open('MELD_train_efr.json') as f:\n",
        "    data = json.load(f)\n",
        "print(f\"Number of samples: {len(data)}\")\n",
        "print(f\"Example of a sample: {data[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ck5Yz3C-mxr",
        "outputId": "06a19d10-ba3c-4404-d413-db188d27c9c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataframe shape: (4000, 4)\n",
            "Dataframe columns: Index(['speakers', 'emotions', 'utterances', 'triggers'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Convert data to pandas dataframe\n",
        "df = pd.DataFrame(data)\n",
        "# drop episode and speakers columns\n",
        "df = df.drop(columns=['episode'])\n",
        "print(f\"Dataframe shape: {df.shape}\")\n",
        "print(f\"Dataframe columns: {df.columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Spy3zzO-mxs"
      },
      "source": [
        "Changing nan values to zeros in order to avoid errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fu_fACgc-mxs",
        "outputId": "c47502e7-fb8c-442d-d603-9716ee14c2de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df[\"triggers\"] = df[\"triggers\"].apply(lambda x: [0 if elem != 1 and elem != 0 else elem for elem in x])\n",
        "df[\"triggers\"][3359]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UssZaYz3-mxs"
      },
      "source": [
        "Splitting the data into train, validation and test sets with a 80-10-10 ratio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OFaRTAN7-mxs"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
        "df_val, df_test = train_test_split(temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnChzNkK-mxs",
        "outputId": "bdf26488-c770-4a3a-9358-3ebe4922c3d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (3200, 4)\n",
            "Val shape: (400, 4)\n",
            "Test shape: (400, 4)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train shape: {df_train.shape}\")\n",
        "print(f\"Val shape: {df_val.shape}\")\n",
        "print(f\"Test shape: {df_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzsNt8T--mxs"
      },
      "source": [
        "As baselines models we need to implement a random model and a majority class model for emotions and triggers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGSbTu_l-mxt",
        "outputId": "3ce0f7f6-1138-442b-8acb-d5cae6b264d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neutral': 12066, 'joy': 4986, 'surprise': 3664, 'anger': 3203, 'sadness': 2108, 'fear': 889, 'disgust': 848}\n"
          ]
        }
      ],
      "source": [
        "# first we create a dictionary of all emotions with their corresponding occurences\n",
        "emotions_dict = {}\n",
        "for emotions in df_train[\"emotions\"]:\n",
        "    for emotion in emotions:\n",
        "        if emotion in emotions_dict:\n",
        "            emotions_dict[emotion] += 1\n",
        "        else:\n",
        "            emotions_dict[emotion] = 1\n",
        "\n",
        "# then we sort the dictionary by occurences\n",
        "emotions_dict = {k: v for k, v in sorted(emotions_dict.items(), key=lambda item: item[1], reverse=True)}\n",
        "print(emotions_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aG4tIgr-mxt"
      },
      "source": [
        "Plotting the distribution of emotions could also be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        },
        "id": "7ALQKgJ0-mxt",
        "outputId": "444252a2-2577-4d12-cbc1-ce632d97aceb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot the emotions distribution as histogram\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.bar(emotions_dict.keys(), emotions_dict.values(), color='blue')\n",
        "plt.title(\"Emotions distribution in the training set\")\n",
        "plt.xlabel(\"Emotions\")\n",
        "plt.ylabel(\"Number of occurences\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQC3STmj-mxt"
      },
      "source": [
        "We can also implement some text processing techniques such as stemming, lemmatization, stop words removal, etc.\n",
        "\n",
        "In this case we will use nltk library for lemmatization and stop words removal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYfAiohQ-mxt",
        "outputId": "04704c86-7f64-4236-cbf9-2108bda024ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "bad_symbols = re.compile('[^a-z ]')\n",
        "punct = string.punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87PMhA2u-mxu"
      },
      "source": [
        "We don't need to use lower case because we are going to use bert uncased models further on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O_PoU-af-mxu"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = bad_symbols.sub('', text)\n",
        "    text = word_tokenize(text)\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\n",
        "    text = ' '.join(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wkjy3t9t-mxu",
        "outputId": "8051c9d1-10d0-4338-8acb-dc5d0064df68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train, val and test sets after preprocessing: \n",
            "Train shape: (3200, 4)\n",
            "Val shape: (400, 4)\n",
            "Test shape: (400, 4)\n"
          ]
        }
      ],
      "source": [
        "baseline_train = df_train.copy()\n",
        "baseline_train[\"utterances\"] = baseline_train[\"utterances\"].apply(lambda x: [preprocess_text(elem) for elem in x])\n",
        "baseline_test = df_test.copy()\n",
        "baseline_test[\"utterances\"] = baseline_test[\"utterances\"].apply(lambda x: [preprocess_text(elem) for elem in x])\n",
        "baseline_val = df_val.copy()\n",
        "baseline_val[\"utterances\"] = baseline_val[\"utterances\"].apply(lambda x: [preprocess_text(elem) for elem in x])\n",
        "print(\"Shape of train, val and test sets after preprocessing: \")\n",
        "print(f\"Train shape: {baseline_train.shape}\")\n",
        "print(f\"Val shape: {baseline_val.shape}\")\n",
        "print(f\"Test shape: {baseline_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ9HYEWk-mxu"
      },
      "source": [
        "In order to use the TF-IDF vectorizer we need to split the utterances into single sentences, likewise the emotions and triggers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0JGlj2Dn-mxu"
      },
      "outputs": [],
      "source": [
        "def splitter(df, y_label):\n",
        "    X = []\n",
        "    y = []\n",
        "    for index, row in df.iterrows():\n",
        "        for i in range(len(row[\"utterances\"])):\n",
        "            X.append(row[\"utterances\"][i])\n",
        "            y.append(row[y_label][i])\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QG7dR_5-mxu",
        "outputId": "8ed9aedd-66bf-4b94-ef4e-cf817ffd8335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape after splitting: 27764\n",
            "Val shape after splitting: 3678\n",
            "Test shape after splitting: 3558\n"
          ]
        }
      ],
      "source": [
        "# Emotions baseline\n",
        "x_train_base, y_train_emotions = splitter(baseline_train, \"emotions\")\n",
        "x_val_base, y_val_emotions = splitter(baseline_val, \"emotions\")\n",
        "x_test_base, y_test_emotions = splitter(baseline_test, \"emotions\")\n",
        "\n",
        "# Triggers baseline\n",
        "_ , y_train_triggers = splitter(baseline_train, \"triggers\")\n",
        "_ , y_val_triggers = splitter(baseline_val, \"triggers\")\n",
        "_ , y_test_triggers = splitter(baseline_test, \"triggers\")\n",
        "\n",
        "print(f\"Train shape after splitting: {len(x_train_base)}\")\n",
        "print(f\"Val shape after splitting: {len(x_val_base)}\")\n",
        "print(f\"Test shape after splitting: {len(x_test_base)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF8QlR9Q-mxv"
      },
      "source": [
        "As a tokenizer we use TfidfVectorizer from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6xT-hSj-mxv",
        "outputId": "1038f933-02e7-4f38-8b5c-d14c5de78a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape after vectorization: (27764, 5142)\n",
            "Val shape after vectorization: (3678, 5142)\n",
            "Test shape after vectorization: (3558, 5142)\n",
            "Size of the vocabulary: 5142\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "x_train_base = vectorizer.fit_transform(x_train_base)\n",
        "x_val_base = vectorizer.transform(x_val_base)\n",
        "x_test_base = vectorizer.transform(x_test_base)\n",
        "\n",
        "print(f\"Train shape after vectorization: {x_train_base.shape}\")\n",
        "print(f\"Val shape after vectorization: {x_val_base.shape}\")\n",
        "print(f\"Test shape after vectorization: {x_test_base.shape}\")\n",
        "\n",
        "print(f\"Size of the vocabulary: {len(vectorizer.vocabulary_)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJsm3qId-mxv"
      },
      "source": [
        "Evaluation function that returns the classification report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "835ywCQu-mxv"
      },
      "outputs": [],
      "source": [
        "def evaluate(Y_test, Y_pred):\n",
        "    report = classification_report(Y_test, Y_pred, zero_division=0)\n",
        "    return report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ishbbG6-mxv"
      },
      "source": [
        "Defining the dummy classifier for emotions and triggers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r_GP91pL-mxv"
      },
      "outputs": [],
      "source": [
        "dummy_clf_majority_emotions = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf_random_emotions = DummyClassifier(strategy=\"uniform\")\n",
        "\n",
        "dummy_clf_majority_triggers = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf_random_triggers = DummyClassifier(strategy=\"uniform\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvYhjUQE-mxv"
      },
      "source": [
        "Training and evaluation of the classifiers fitted on the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24CrukE2-mxv",
        "outputId": "6a2b208c-fe82-4153-a0f2-1154550d0690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority classifier for emotions: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.00      0.00      0.00       369\n",
            "     disgust       0.00      0.00      0.00       101\n",
            "        fear       0.00      0.00      0.00       109\n",
            "         joy       0.00      0.00      0.00       663\n",
            "     neutral       0.44      1.00      0.61      1572\n",
            "     sadness       0.00      0.00      0.00       258\n",
            "    surprise       0.00      0.00      0.00       486\n",
            "\n",
            "    accuracy                           0.44      3558\n",
            "   macro avg       0.06      0.14      0.09      3558\n",
            "weighted avg       0.20      0.44      0.27      3558\n",
            "\n",
            "-------------------------------------------------------\n",
            "Random classifier for emotions: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.09      0.13      0.10       369\n",
            "     disgust       0.03      0.17      0.05       101\n",
            "        fear       0.02      0.11      0.04       109\n",
            "         joy       0.18      0.15      0.16       663\n",
            "     neutral       0.44      0.14      0.21      1572\n",
            "     sadness       0.07      0.13      0.09       258\n",
            "    surprise       0.13      0.13      0.13       486\n",
            "\n",
            "    accuracy                           0.14      3558\n",
            "   macro avg       0.14      0.14      0.11      3558\n",
            "weighted avg       0.26      0.14      0.16      3558\n",
            "\n",
            "-------------------------------------------------------\n",
            "Majority classifier for triggers: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      1.00      0.92      3027\n",
            "         1.0       0.00      0.00      0.00       531\n",
            "\n",
            "    accuracy                           0.85      3558\n",
            "   macro avg       0.43      0.50      0.46      3558\n",
            "weighted avg       0.72      0.85      0.78      3558\n",
            "\n",
            "-------------------------------------------------------\n",
            "Random classifier for triggers: \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.85      0.50      0.63      3027\n",
            "         1.0       0.15      0.51      0.23       531\n",
            "\n",
            "    accuracy                           0.50      3558\n",
            "   macro avg       0.50      0.51      0.43      3558\n",
            "weighted avg       0.75      0.50      0.57      3558\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Emotions baseline\n",
        "dummy_clf_majority_emotions.fit(x_train_base, y_train_emotions)\n",
        "dummy_clf_random_emotions.fit(x_train_base, y_train_emotions)\n",
        "\n",
        "y_pred_majority_emotions = dummy_clf_majority_emotions.predict(x_test_base)\n",
        "y_pred_random_emotions = dummy_clf_random_emotions.predict(x_test_base)\n",
        "print(\"Majority classifier for emotions: \\n\")\n",
        "print(evaluate(y_test_emotions, y_pred_majority_emotions))\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Random classifier for emotions: \\n\")\n",
        "print(evaluate(y_test_emotions, y_pred_random_emotions))\n",
        "print(\"-------------------------------------------------------\")\n",
        "\n",
        "# Triggers baseline\n",
        "dummy_clf_majority_triggers.fit(x_train_base, y_train_triggers)\n",
        "dummy_clf_random_triggers.fit(x_train_base, y_train_triggers)\n",
        "\n",
        "y_pred_majority_triggers = dummy_clf_majority_triggers.predict(x_test_base)\n",
        "y_pred_random_triggers = dummy_clf_random_triggers.predict(x_test_base)\n",
        "print(\"Majority classifier for triggers: \\n\")\n",
        "print(evaluate(y_test_triggers, y_pred_majority_triggers))\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Random classifier for triggers: \\n\")\n",
        "print(evaluate(y_test_triggers, y_pred_random_triggers))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ2aj1El-mxw"
      },
      "source": [
        "Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zsCtXMdG-mxw"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "transformers.set_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gffRztVP-mxw",
        "outputId": "08a2dc46-5e71-4181-9b15-f4b0d39654e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences distribution: [9, 2, 3, 14, 4, 10, 8, 10, 19, 17, 4, 4, 6, 8, 10, 3, 9, 8, 22, 7, 9, 9, 12, 11, 7, 8, 4, 4, 8, 8, 5, 7, 14, 12, 7, 3, 8, 4, 13, 10, 13, 9, 3, 4, 14, 16, 5, 3, 11, 11, 3, 6, 5, 3, 15, 4, 6, 13, 7, 10, 7, 3, 4, 5, 22, 19, 8, 9, 8, 8, 3, 15, 6, 9, 6, 12, 3, 6, 19, 5, 7, 11, 6, 5, 4, 17, 6, 3, 3, 6, 12, 14, 4, 14, 5, 14, 14, 6, 4, 8, 4, 12, 18, 3, 5, 4, 4, 14, 12, 4, 5, 3, 11, 3, 10, 5, 13, 8, 5, 10, 10, 9, 6, 3, 9, 14, 9, 7, 5, 15, 16, 3, 9, 21, 4, 17, 6, 12, 9, 6, 10, 12, 2, 6, 9, 17, 17, 8, 15, 12, 24, 4, 16, 14, 19, 4, 4, 11, 4, 4, 8, 9, 19, 16, 6, 8, 4, 2, 8, 4, 8, 9, 15, 18, 19, 5, 18, 5, 14, 11, 5, 6, 5, 11, 3, 16, 5, 16, 10, 8, 3, 16, 8, 11, 20, 8, 10, 5, 4, 11, 16, 4, 9, 14, 8, 14, 8, 13, 4, 2, 10, 7, 14, 5, 6, 5, 6, 11, 15, 11, 5, 3, 6, 9, 3, 7, 6, 8, 8, 19, 6, 8, 3, 9, 6, 4, 4, 3, 9, 5, 8, 5, 8, 8, 7, 12, 16, 9, 9, 3, 4, 5, 9, 7, 18, 3, 3, 14, 10, 5, 4, 2, 4, 6, 3, 10, 8, 7, 6, 3, 5, 5, 8, 3, 8, 7, 5, 12, 7, 21, 6, 16, 10, 15, 4, 13, 12, 8, 8, 13, 16, 8, 9, 10, 4, 13, 11, 14, 6, 11, 7, 14, 3, 3, 3, 5, 8, 3, 5, 3, 7, 8, 7, 6, 13, 6, 8, 5, 14, 5, 6, 19, 9, 5, 3, 8, 9, 8, 18, 13, 3, 14, 8, 20, 12, 12, 16, 9, 4, 15, 12, 7, 11, 8, 12, 12, 7, 7, 20, 7, 7, 15, 13, 5, 6, 11, 3, 3, 4, 16, 8, 5, 11, 11, 8, 15, 9, 9, 10, 4, 3, 4, 14, 5, 19, 13, 11, 2, 5, 9, 3, 9, 9, 7, 16, 11, 3, 5, 19, 11, 6, 3, 12, 6, 9, 4, 5, 10, 14, 15, 7, 4, 8, 5, 10, 5, 10, 10, 3, 6, 5, 7, 7, 10, 2, 4, 6, 10, 4, 12, 7, 13, 5, 14, 12, 14, 11, 21, 8, 7, 10, 8, 17, 19, 2, 6, 3, 6, 10, 3, 10, 8, 16, 8, 2, 5, 12, 15, 11, 13, 7, 5, 6, 2, 9, 5, 19, 10, 12, 9, 12, 3, 7, 6, 5, 5, 13, 15, 12, 4, 9, 6, 15, 9, 7, 10, 6, 16, 12, 3, 6, 6, 9, 9, 6, 14, 12, 16, 10, 4, 6, 12, 18, 19, 4, 8, 4, 6, 7, 13, 5, 4, 8, 8, 14, 5, 18, 12, 21, 6, 5, 7, 10, 4, 5, 13, 6, 6, 3, 6, 5, 6, 18, 2, 3, 14, 9, 4, 2, 14, 15, 7, 13, 7, 8, 3, 4, 5, 5, 5, 12, 7, 10, 5, 5, 5, 3, 11, 12, 9, 4, 10, 6, 11, 11, 5, 13, 9, 17, 7, 4, 7, 3, 10, 6, 3, 7, 16, 5, 5, 3, 8, 7, 8, 5, 3, 13, 18, 6, 13, 15, 10, 13, 11, 12, 6, 7, 7, 10, 3, 6, 6, 6, 12, 22, 6, 16, 10, 8, 10, 9, 3, 8, 8, 3, 11, 5, 11, 8, 8, 3, 12, 15, 4, 4, 3, 12, 9, 8, 9, 2, 7, 6, 6, 9, 7, 5, 9, 7, 6, 11, 3, 14, 5, 17, 9, 7, 14, 4, 6, 8, 6, 3, 4, 11, 8, 6, 7, 21, 6, 15, 3, 8, 9, 11, 11, 5, 16, 5, 2, 12, 4, 10, 12, 8, 6, 13, 17, 4, 6, 9, 6, 19, 6, 9, 7, 10, 11, 9, 3, 15, 7, 12, 8, 11, 22, 14, 11, 6, 3, 11, 5, 4, 7, 12, 3, 10, 11, 14, 13, 2, 10, 4, 6, 5, 11, 15, 3, 10, 7, 14, 8, 10, 6, 4, 7, 15, 9, 3, 6, 4, 8, 11, 6, 12, 7, 9, 11, 7, 4, 12, 13, 14, 6, 4, 12, 6, 3, 7, 11, 11, 3, 4, 10, 2, 7, 8, 13, 10, 4, 3, 8, 12, 7, 11, 9, 14, 11, 8, 3, 10, 4, 4, 9, 3, 5, 10, 5, 20, 8, 13, 15, 10, 12, 5, 4, 7, 8, 3, 7, 10, 7, 16, 7, 5, 9, 10, 4, 3, 19, 19, 8, 6, 16, 3, 6, 6, 9, 6, 8, 6, 8, 4, 3, 9, 4, 14, 10, 10, 7, 6, 5, 16, 3, 8, 12, 10, 6, 7, 15, 4, 14, 3, 5, 11, 11, 8, 4, 4, 11, 9, 7, 17, 10, 10, 9, 6, 7, 4, 11, 5, 12, 5, 18, 8, 5, 7, 6, 8, 6, 7, 18, 8, 6, 15, 8, 8, 4, 10, 12, 7, 4, 4, 4, 12, 10, 8, 10, 13, 22, 16, 11, 3, 2, 3, 6, 9, 3, 12, 12, 3, 14, 3, 11, 4, 6, 3, 16, 18, 8, 5, 16, 7, 4, 7, 6, 13, 18, 7, 17, 9, 18, 9, 11, 13, 14, 5, 9, 9, 5, 21, 3, 3, 9, 8, 11, 13, 4, 15, 14, 20, 4, 8, 12, 8, 3, 9, 9, 4, 19, 8, 3, 6, 10, 11, 13, 6, 4, 6, 9, 3, 6, 15, 6, 3, 3, 4, 4, 4, 17, 7, 16, 3, 9, 15, 21, 15, 18, 10, 14, 6, 10, 18, 4, 10, 6, 6, 3, 8, 6, 14, 7, 6, 2, 6, 10, 3, 5, 6, 5, 8, 10, 11, 5, 7, 13, 13, 4, 16, 11, 12, 6, 5, 6, 6, 13, 6, 11, 11, 15, 6, 6, 7, 8, 9, 5, 9, 4, 7, 5, 4, 17, 9, 10, 12, 3, 8, 4, 3, 15, 6, 9, 6, 8, 4, 12, 3, 8, 16, 2, 11, 9, 4, 5, 2, 6, 15, 9, 5, 3, 7, 8, 9, 8, 5, 6, 6, 8, 12, 4, 5, 13, 10, 11, 16, 18, 10, 4, 11, 8, 5, 9, 9, 4, 13, 4, 3, 7, 14, 19, 8, 10, 18, 4, 8, 3, 14, 3, 8, 20, 6, 3, 7, 7, 16, 14, 8, 7, 8, 9, 6, 5, 5, 13, 7, 10, 6, 4, 4, 8, 10, 4, 4, 6, 10, 16, 6, 8, 4, 19, 3, 2, 2, 4, 12, 5, 7, 4, 20, 16, 10, 4, 6, 8, 16, 9, 7, 10, 9, 9, 11, 17, 17, 11, 4, 3, 17, 7, 3, 10, 2, 4, 7, 10, 14, 6, 6, 4, 5, 9, 5, 12, 8, 12, 11, 7, 6, 6, 4, 8, 3, 6, 3, 3, 9, 3, 5, 8, 6, 12, 5, 10, 6, 15, 12, 14, 17, 14, 17, 17, 22, 12, 8, 14, 10, 17, 19, 4, 12, 7, 8, 2, 10, 17, 4, 6, 4, 8, 6, 23, 9, 6, 9, 3, 4, 11, 5, 9, 9, 10, 11, 18, 9, 6, 5, 17, 12, 9, 20, 20, 3, 6, 19, 14, 11, 15, 4, 9, 6, 4, 20, 9, 15, 5, 6, 8, 8, 8, 23, 10, 8, 7, 11, 10, 12, 9, 12, 13, 4, 8, 6, 11, 11, 14, 16, 9, 10, 4, 4, 7, 12, 5, 9, 8, 7, 9, 6, 3, 3, 17, 14, 7, 20, 7, 6, 11, 4, 6, 14, 13, 7, 9, 3, 4, 4, 15, 4, 4, 3, 10, 5, 10, 3, 6, 5, 9, 7, 3, 4, 6, 4, 4, 4, 3, 7, 13, 6, 4, 4, 5, 7, 3, 10, 10, 9, 9, 7, 6, 5, 11, 10, 11, 8, 6, 12, 10, 4, 4, 8, 9, 14, 21, 11, 4, 7, 3, 7, 15, 3, 6, 14, 5, 19, 7, 5, 4, 8, 13, 6, 14, 22, 5, 3, 11, 4, 3, 14, 5, 17, 6, 8, 5, 9, 4, 6, 16, 3, 3, 10, 21, 10, 17, 17, 6, 13, 4, 4, 10, 4, 12, 12, 7, 13, 15, 12, 7, 6, 14, 7, 2, 7, 5, 14, 6, 3, 4, 5, 6, 7, 6, 16, 9, 4, 9, 7, 12, 5, 5, 4, 8, 13, 4, 15, 7, 11, 6, 17, 3, 12, 9, 14, 11, 15, 11, 10, 7, 13, 11, 4, 3, 7, 7, 4, 21, 5, 10, 4, 16, 5, 9, 13, 4, 14, 7, 5, 5, 10, 10, 2, 6, 11, 4, 6, 15, 14, 9, 13, 8, 7, 3, 4, 13, 7, 5, 3, 8, 3, 11, 16, 10, 18, 13, 8, 12, 8, 5, 3, 11, 2, 11, 9, 5, 15, 11, 2, 13, 4, 12, 11, 12, 9, 9, 15, 7, 7, 16, 8, 5, 9, 10, 3, 4, 5, 6, 16, 3, 6, 14, 9, 6, 5, 9, 9, 3, 11, 5, 6, 8, 5, 7, 7, 4, 15, 5, 9, 21, 9, 8, 9, 6, 3, 3, 10, 3, 11, 5, 6, 8, 5, 18, 3, 9, 16, 10, 7, 11, 19, 4, 7, 9, 13, 15, 14, 6, 15, 10, 2, 12, 12, 7, 5, 11, 5, 5, 12, 4, 18, 13, 5, 3, 4, 17, 3, 5, 13, 7, 6, 5, 6, 9, 17, 3, 7, 3, 12, 14, 12, 4, 3, 8, 4, 4, 4, 15, 6, 11, 8, 5, 9, 18, 9, 10, 9, 14, 15, 3, 12, 6, 15, 10, 7, 4, 15, 11, 4, 9, 20, 6, 13, 6, 8, 8, 17, 4, 21, 5, 7, 2, 9, 10, 2, 7, 9, 6, 4, 5, 5, 6, 6, 4, 13, 14, 17, 6, 14, 17, 7, 8, 18, 8, 16, 4, 9, 9, 10, 21, 11, 15, 3, 11, 18, 10, 6, 2, 6, 6, 7, 10, 3, 6, 11, 5, 5, 9, 3, 6, 6, 7, 6, 7, 10, 3, 5, 9, 9, 11, 4, 3, 4, 9, 7, 11, 13, 4, 11, 5, 5, 4, 10, 3, 13, 7, 14, 13, 6, 5, 8, 7, 15, 4, 3, 7, 6, 10, 6, 7, 6, 12, 12, 3, 6, 4, 17, 12, 13, 9, 7, 4, 6, 10, 14, 8, 11, 10, 6, 6, 3, 15, 7, 4, 10, 4, 16, 3, 18, 5, 10, 9, 3, 16, 11, 9, 9, 12, 18, 11, 10, 11, 7, 10, 9, 10, 6, 14, 4, 5, 12, 3, 15, 12, 3, 8, 3, 16, 14, 18, 12, 7, 8, 14, 5, 5, 16, 4, 4, 9, 4, 7, 6, 4, 12, 6, 7, 5, 8, 10, 5, 12, 11, 2, 8, 5, 11, 5, 3, 9, 3, 12, 5, 5, 20, 3, 15, 4, 18, 2, 14, 12, 7, 17, 8, 16, 11, 5, 5, 4, 7, 8, 7, 2, 2, 9, 10, 13, 8, 7, 5, 11, 3, 11, 13, 3, 4, 10, 13, 11, 7, 3, 14, 13, 7, 10, 2, 4, 4, 17, 13, 3, 5, 14, 13, 11, 8, 8, 14, 5, 10, 3, 9, 16, 9, 9, 16, 12, 8, 14, 13, 3, 3, 10, 8, 10, 6, 5, 5, 16, 5, 9, 7, 12, 7, 5, 12, 18, 5, 12, 10, 3, 14, 10, 10, 11, 17, 4, 13, 9, 6, 10, 19, 15, 17, 6, 18, 3, 9, 3, 6, 12, 6, 9, 3, 9, 9, 10, 11, 4, 15, 5, 12, 13, 8, 7, 9, 14, 3, 9, 21, 3, 5, 21, 4, 4, 12, 4, 3, 5, 18, 10, 8, 13, 6, 12, 6, 14, 6, 8, 5, 3, 19, 5, 9, 5, 12, 7, 16, 8, 13, 5, 13, 7, 8, 8, 7, 11, 21, 7, 3, 5, 10, 6, 3, 11, 16, 6, 3, 15, 7, 6, 9, 7, 7, 9, 17, 9, 8, 13, 4, 6, 5, 4, 10, 8, 4, 14, 11, 5, 4, 14, 4, 7, 10, 3, 3, 3, 10, 10, 3, 3, 9, 5, 13, 6, 9, 11, 3, 4, 10, 5, 9, 6, 8, 11, 9, 8, 4, 8, 10, 5, 4, 5, 9, 8, 12, 17, 15, 20, 15, 7, 9, 3, 7, 11, 5, 18, 3, 10, 5, 8, 13, 10, 3, 17, 5, 3, 7, 10, 2, 5, 11, 11, 10, 4, 22, 9, 8, 5, 4, 7, 6, 8, 4, 5, 16, 9, 15, 11, 4, 14, 12, 11, 6, 7, 9, 7, 8, 7, 8, 15, 5, 9, 9, 3, 7, 5, 10, 21, 15, 12, 11, 3, 6, 16, 12, 13, 7, 15, 2, 4, 16, 14, 10, 18, 7, 16, 4, 13, 13, 7, 3, 5, 3, 13, 2, 6, 6, 4, 22, 8, 8, 14, 5, 5, 3, 8, 3, 2, 12, 6, 3, 8, 6, 5, 20, 19, 4, 6, 8, 4, 6, 8, 3, 3, 15, 5, 14, 5, 8, 11, 11, 16, 5, 6, 11, 12, 20, 6, 5, 16, 4, 13, 15, 3, 11, 5, 12, 5, 5, 19, 9, 13, 11, 5, 19, 4, 16, 13, 4, 24, 16, 10, 14, 3, 10, 13, 8, 3, 6, 11, 16, 5, 4, 10, 10, 13, 6, 8, 8, 5, 17, 12, 5, 4, 12, 3, 6, 2, 4, 12, 13, 4, 11, 8, 2, 15, 15, 3, 18, 12, 5, 6, 6, 6, 3, 3, 8, 11, 11, 9, 16, 7, 4, 4, 13, 7, 2, 10, 8, 9, 7, 10, 8, 2, 10, 7, 6, 11, 11, 2, 11, 5, 7, 14, 7, 13, 3, 6, 6, 2, 4, 9, 7, 18, 7, 3, 13, 11, 7, 8, 5, 7, 21, 8, 4, 4, 3, 12, 14, 5, 8, 9, 13, 6, 3, 6, 9, 12, 5, 3, 6, 17, 10, 10, 11, 5, 9, 7, 14, 14, 8, 10, 5, 5, 9, 6, 2, 3, 13, 13, 7, 12, 17, 8, 12, 7, 4, 10, 5, 4, 2, 9, 11, 3, 4, 15, 4, 6, 14, 10, 12, 12, 9, 12, 22, 6, 8, 4, 11, 16, 3, 7, 16, 18, 19, 11, 5, 12, 10, 10, 18, 9, 16, 10, 10, 18, 5, 16, 8, 3, 12, 6, 8, 18, 13, 18, 22, 6, 14, 17, 4, 4, 15, 24, 9, 5, 4, 6, 7, 7, 18, 6, 3, 5, 11, 6, 4, 4, 6, 5, 15, 8, 9, 14, 2, 6, 9, 11, 6, 18, 9, 3, 16, 5, 8, 4, 8, 10, 6, 18, 4, 5, 3, 4, 5, 9, 6, 8, 13, 4, 8, 8, 10, 10, 6, 8, 8, 6, 8, 3, 8, 11, 3, 4, 3, 7, 3, 14, 7, 16, 4, 13, 5, 12, 4, 17, 8, 15, 15, 3, 3, 9, 3, 12, 11, 11, 7, 4, 5, 15, 11, 2, 17, 17, 5, 4, 9, 3, 3, 7, 17, 20, 6, 18, 18, 6, 15, 19, 17, 4, 19, 12, 7, 18, 11, 3, 8, 3, 9, 5, 4, 6, 7, 5, 13, 6, 6, 4, 8, 7, 7, 3, 5, 12, 9, 6, 4, 8, 8, 8, 3, 17, 3, 11, 5, 8, 12, 5, 14, 8, 7, 5, 7, 10, 9, 6, 3, 6, 5, 3, 13, 9, 8, 6, 12, 12, 9, 7, 10, 8, 3, 7, 7, 21, 6, 14, 9, 16, 11, 17, 7, 10, 2, 8, 6, 12, 4, 6, 9, 11, 20, 15, 5, 9, 7, 7, 10, 16, 12, 7, 5, 11, 11, 5, 11, 7, 18, 4, 17, 15, 6, 8, 6, 3, 18, 7, 9, 6, 7, 8, 7, 12, 4, 18, 16, 8, 18, 3, 10, 8, 4, 16, 11, 9, 6, 5, 6, 4, 8, 3, 19, 9, 6, 2, 13, 6, 7, 9, 14, 11, 11, 4, 11, 3, 7, 4, 6, 4, 8, 6, 3, 17, 11, 4, 12, 15, 8, 14, 4, 9, 17, 15, 7, 10, 4, 9, 6, 5, 12, 9, 13, 16, 7, 14, 16, 3, 6, 5, 16, 8, 6, 4, 7, 5, 3, 14, 11, 4, 7, 4, 6, 4, 9, 16, 11, 4, 6, 18, 14, 6, 7, 21, 6, 8, 4, 16, 4, 12, 6, 5, 4, 13, 7, 12, 6, 4, 7, 8, 9, 12, 11, 6, 11, 6, 15, 4, 11, 8, 13, 9, 12, 5, 19, 5, 4, 3, 7, 20, 5, 2, 4, 3, 3, 4, 13, 8, 3, 11, 5, 15, 12, 13, 7, 9, 3, 7, 14, 9, 7, 5, 7, 4, 6, 9, 9, 15, 7, 4, 17, 5, 7, 5, 4, 9, 13, 17, 15, 20, 6, 4, 6, 7, 7, 7, 12, 9, 4, 14, 3, 11, 7, 17, 3, 8, 15, 6, 5, 8, 10, 9, 6, 5, 12, 15, 13, 4, 5, 4, 21, 7, 8, 13, 3, 3, 5, 7, 6, 7, 11, 15, 6, 5, 12, 4, 5, 12, 7, 11, 3, 3, 12, 6, 5, 9, 3, 10, 5, 9, 6, 4, 14, 5, 3, 7, 4, 11, 5, 8, 3, 3, 4, 4, 13, 10, 8, 7, 6, 20, 3, 4, 3, 11, 9, 12, 3, 9, 4, 12, 3, 21, 3, 9, 10, 11, 9, 10, 8, 3, 4, 17, 10, 4, 11, 20, 13, 16, 17, 8, 17, 5, 6, 11, 10, 11, 10, 3, 10, 15, 12, 4, 19, 6, 20, 10, 9, 8, 6, 13, 9, 4, 4, 8, 14, 19, 9, 5, 5, 14, 11, 4, 8, 10, 10, 8, 7, 13, 7, 13, 3, 12, 6, 10, 10, 17, 4, 6, 8, 11, 15, 6, 9, 3, 9, 10, 13, 3, 10, 13, 6, 9, 4, 3, 4, 4, 4, 11, 11, 9, 15, 10, 5, 16, 7, 13, 3, 13, 6, 4, 3, 22, 5, 8, 15, 3, 11, 4, 23, 12, 3, 7, 11, 6, 7, 6, 12, 19, 12, 9, 12, 16, 13, 6, 5, 6, 8, 9, 3, 6, 16, 8, 17, 2, 10, 8, 6, 8, 6, 5, 3, 10, 8, 7, 6, 6, 18, 16, 6, 5, 7, 9, 5, 9, 4, 14, 7, 5, 16, 15, 7, 6, 16, 3, 15, 6, 16, 11, 7, 11, 13, 5, 2, 19, 5, 9, 6, 9, 10, 4, 7, 8, 10, 3, 12, 3, 15, 11, 9, 5, 11, 8, 4, 8, 7, 6, 7, 8, 13, 4, 5, 3, 17, 6, 22, 7, 3, 13, 19, 17, 11, 3, 4, 14, 9, 7, 8, 12, 11, 4, 10, 7, 8, 4, 7, 15, 11, 16, 10, 2, 5, 6, 5, 7, 14, 3, 10, 7, 5, 3, 4, 6, 7, 17, 5, 4, 21, 5, 17, 15, 4, 10, 6, 8, 5, 7, 5, 17, 6, 3, 13, 3, 3, 10, 4, 5, 13, 7, 19, 3, 3, 11, 8, 7, 14, 14, 10, 7, 12, 7, 10, 5, 13, 16, 7, 3, 17, 3, 2, 13, 6, 12, 14, 3, 6, 7, 12, 7, 3, 9, 10, 14, 7, 14, 12, 23, 3, 6, 8, 6, 11, 8, 15, 10, 10, 13, 9, 11, 10, 6, 4, 2, 6, 16, 18, 13, 10, 3, 8, 5, 16, 3, 10, 5]\n",
            "Max number of sentences: 24\n"
          ]
        }
      ],
      "source": [
        "# get number of sentencese distribution\n",
        "num_sentences = []\n",
        "for index, row in df_train.iterrows():\n",
        "    num_sentences.append(len(row[\"utterances\"]))\n",
        "print(f\"Number of sentences distribution: {num_sentences}\")\n",
        "print(f\"Max number of sentences: {max(num_sentences)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vIpYs0ij-mxw"
      },
      "outputs": [],
      "source": [
        "# one hot encode the emotions\n",
        "emotions_one_hot_dict = {\n",
        "    \"neutral\": [1, 0, 0, 0, 0, 0, 0],\n",
        "    \"joy\": [0, 1, 0, 0, 0, 0, 0],\n",
        "    \"surprise\": [0, 0, 1, 0, 0, 0, 0],\n",
        "    \"sadness\": [0, 0, 0, 1, 0, 0, 0],\n",
        "    \"anger\": [0, 0, 0, 0, 1, 0, 0],\n",
        "    \"disgust\": [0, 0, 0, 0, 0, 1, 0],\n",
        "    \"fear\": [0, 0, 0, 0, 0, 0, 1]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "e3kggk1g-mxw"
      },
      "outputs": [],
      "source": [
        "bert_train = df_train.copy()\n",
        "bert_val = df_val.copy()\n",
        "bert_test = df_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XDrUUvu1-mxw"
      },
      "outputs": [],
      "source": [
        "columns = [\"speakers\", \"emotions\", \"utterances\", \"triggers\"]\n",
        "\n",
        "def splitter_bert(df):\n",
        "    temp = pd.DataFrame(columns=columns)\n",
        "    for i in range(df.shape[0]):\n",
        "        for j, _ in enumerate(df.iloc[i]['speakers']):\n",
        "            new_row = pd.DataFrame({'speakers': [df.iloc[i]['speakers']],\n",
        "                        'emotions': [df.iloc[i]['emotions'][:j+1]],\n",
        "                        'utterances': [df.iloc[i]['utterances']],\n",
        "                        'triggers': [df.iloc[i]['triggers'][:j+1]]})\n",
        "            temp = pd.concat([temp, new_row], ignore_index=True)\n",
        "    return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw27kKGc-mxx",
        "outputId": "344ab9cc-c35f-4124-bc08-e10a449461b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train, val and test sets after splitting: \n",
            "Train shape: (27764, 4)\n",
            "Val shape: (3678, 4)\n",
            "Test shape: (3558, 4)\n",
            "-------------------------------------------------------\n",
            "Example of a sample: \n",
            "speakers      [Chandler, All, Monica, Chandler, Ross, Chandl...\n",
            "emotions                                              [neutral]\n",
            "utterances    [Hey., Hey!, So how was Joan?, I broke up with...\n",
            "triggers                                                  [0.0]\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# split train, val and test sets for BERT\n",
        "bert_train = splitter_bert(bert_train)\n",
        "bert_val = splitter_bert(bert_val)\n",
        "bert_test = splitter_bert(bert_test)\n",
        "print(\"Shape of train, val and test sets after splitting: \")\n",
        "print(f\"Train shape: {bert_train.shape}\")\n",
        "print(f\"Val shape: {bert_val.shape}\")\n",
        "print(f\"Test shape: {bert_test.shape}\")\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Example of a sample: \")\n",
        "print(bert_train.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qAjFEKKy-mxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b9f057-5bcf-4418-c1e2-c7776597fbc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = 'prajjwal1/bert-tiny'\n",
        "\n",
        "MAX_LEN = 512\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 2e-05\n",
        "OUT_CHANNELS = 768 if \"base\" in  MODEL_NAME else 128 #1024\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ylvw_VDJ-mxx"
      },
      "outputs": [],
      "source": [
        "class CustomSTDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.speakers = data.speakers\n",
        "        self.text = data.utterances\n",
        "        self.emotions = data.emotions\n",
        "        self.triggers = data.triggers\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # spatial-temporal bert inputs to wrap intra-context and inter-context information\n",
        "        # intra-context: the preceding utterances of the current utterance from the same speaker\n",
        "        # inter-context: the preceding utterances of the current utterance from different speakers\n",
        "        row  = self.data.iloc[index]\n",
        "        speakers = row[\"speakers\"]\n",
        "        emotions = row[\"emotions\"]\n",
        "        triggers = row[\"triggers\"]\n",
        "        targets = emotions_one_hot_dict[emotions[-1]] + [triggers[-1]]\n",
        "\n",
        "        current_speaker = speakers[-1]\n",
        "        text_intra = \" \"\n",
        "        text_inter = \" \"\n",
        "        for i, d in enumerate(speakers):\n",
        "            if d == current_speaker:\n",
        "                text_intra += f\" {d}: {row['utterances'][i]}\"\n",
        "                if i in range(len(emotions) - 1):\n",
        "                  text_intra += ' emotion:' + f\" {emotions[i]},\" + ' trigger:' + f\" {triggers[i]}\"\n",
        "            else:\n",
        "                text_inter += f\" {d}: {row['utterances'][i]}\"\n",
        "                if i in range(len(emotions) - 1):\n",
        "                  text_inter += ' emotion:' + f\" {emotions[i]},\" + ' trigger:' + f\" {triggers[i]}\"\n",
        "        #text_intra = text_intra + ' emotions:'\n",
        "        #text_inter = text_inter + ' emotions:'\n",
        "        #for i in range(len(emotions) - 1):\n",
        "        #    if speakers[i] == current_speaker:\n",
        "        #      text_intra += f\" {emotions[i]},\"\n",
        "        #    else:\n",
        "        #      text_inter += f\" {emotions[i]},\"\n",
        "\n",
        "        #text_intra = text_intra + ' triggers:'\n",
        "        #text_inter = text_inter + ' triggers:'\n",
        "        #for i in range(len(triggers) - 1):\n",
        "        #    if speakers[i] == current_speaker:\n",
        "        #      text_intra += f\" {triggers[i]},\"\n",
        "        #    else:\n",
        "        #      text_inter += f\" {triggers[i]},\"\n",
        "\n",
        "        #text_intra = text_intra.strip()\n",
        "        #text_inter = text_inter.strip()\n",
        "\n",
        "        encoding_intra = self.tokenizer.encode_plus(\n",
        "            text_intra,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        encoding_inter = self.tokenizer.encode_plus(\n",
        "            text_inter,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=True,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids_intra': encoding_intra['input_ids'].flatten(),\n",
        "            'attention_mask_intra': encoding_intra['attention_mask'].flatten(),\n",
        "            'token_type_ids_intra': encoding_intra['token_type_ids'].flatten(),\n",
        "            'input_ids_inter': encoding_inter['input_ids'].flatten(),\n",
        "            'attention_mask_inter': encoding_inter['attention_mask'].flatten(),\n",
        "            'token_type_ids_inter': encoding_inter['token_type_ids'].flatten(),\n",
        "            'targets': torch.tensor(targets, dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "z3WilKrT-mx1"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.speakers = data.speakers\n",
        "        self.text = data.utterances\n",
        "        self.emotions = data.emotions\n",
        "        self.triggers = data.triggers\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        speakers = row[\"speakers\"]\n",
        "        text = row[\"utterances\"]\n",
        "        emotions = row[\"emotions\"]\n",
        "        triggers = row[\"triggers\"]\n",
        "        targets = emotions_one_hot_dict[emotions[-1]] + [triggers[-1]] # trigger is float while emotions are one hot encoded but as integers IN CASE OF ERROR CHECK THIS\n",
        "\n",
        "        text = \" \"\n",
        "        for i, d in enumerate(speakers):\n",
        "            text += f\" {d}: {row['utterances'][i]}\"\n",
        "\n",
        "        text = text + ' emotions:'\n",
        "        for i in range(len(emotions) - 1):\n",
        "            text += f\" {emotions[i]},\"\n",
        "\n",
        "        text = text + ' triggers:'\n",
        "        for i in range(len(triggers) - 1):\n",
        "            text += f\" {triggers[i]},\"\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'length': torch.tensor(len(speakers), dtype=torch.long),\n",
        "            'targets': torch.tensor(targets, dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "V3Ydj72c-mx1"
      },
      "outputs": [],
      "source": [
        "class BERTClassST(torch.nn.Module):\n",
        "    def __init__(self,model):\n",
        "        super(BERTClassST, self).__init__()\n",
        "        self.l1 = transformers.AutoModel.from_pretrained(model, return_dict=False)\n",
        "        self.l2 = torch.nn.Dropout(p=0.3)\n",
        "        self.l3 = torch.nn.Linear(OUT_CHANNELS*2, 8)\n",
        "\n",
        "    def forward(self, intra_ids, intra_attention_mask, intra_token_type_ids, inter_ids, inter_attention_mask, inter_token_type_ids):\n",
        "        _, output_1= self.l1(intra_ids, attention_mask = intra_attention_mask, token_type_ids = intra_token_type_ids)\n",
        "        _, output_2 = self.l1(inter_ids, attention_mask = inter_attention_mask, token_type_ids = inter_token_type_ids)\n",
        "        output = torch.cat((output_1, output_2), dim=1) # concatenate the outputs of the two BERTs\n",
        "        output = self.l2(output)\n",
        "        output = self.l3(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkIIGJct-mx1"
      },
      "outputs": [],
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self,model):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.AutoModel.from_pretrained(model, return_dict=False)\n",
        "        self.l2 = torch.nn.Dropout(p=0.3)\n",
        "        self.l3 = torch.nn.Linear(OUT_CHANNELS, 8)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF9uiidi-mx1"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(bert_train, tokenizer, MAX_LEN)\n",
        "tokenizer.decode(train_dataset[0]['ids'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ag7UN2zO-mx2"
      },
      "outputs": [],
      "source": [
        "val_dataset = CustomDataset(bert_val, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(bert_test, tokenizer, MAX_LEN)\n",
        "\n",
        "# Definiition of the Dataloader that will feed the data in batches to the neural network for suitable training and processing.\n",
        "training_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y3Ah9mZ-mx2"
      },
      "outputs": [],
      "source": [
        "model = BERTClass(MODEL_NAME)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "29zFLn_V-mx2",
        "outputId": "4d996da5-f16f-4481-b5e3-194e2778bccc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] monica : so how was joan? monica : how many perfectly fine women are you gonna reject over the most superficial insignificant things? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "train_dataset_st = CustomSTDataset(bert_train, tokenizer, MAX_LEN)\n",
        "tokenizer.decode(train_dataset_st[0]['input_ids_intra'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(train_dataset_st[0]['input_ids_inter'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "duaEjia31J7H",
        "outputId": "6b784e2c-cb65-4758-c2fd-443bdd8b2cfc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[CLS] chandler : hey. all : hey! chandler : i broke up with her. ross : don't tell me, because of the big nostril thing? chandler : they were huge. when she sneezed, bats flew out of them. rachel : come on, they were not that huge. chandler : i'm tellin'you, she leaned back ; i could see her brain. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwgxbeUo-mx2",
        "outputId": "f619fa51-df68-4db3-dff4-4b9e497641aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClassST(\n",
              "  (l1): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-1): 2 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
              "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (l2): Dropout(p=0.3, inplace=False)\n",
              "  (l3): Linear(in_features=256, out_features=8, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "val_dataset_st = CustomSTDataset(bert_val, tokenizer, MAX_LEN)\n",
        "test_dataset_st = CustomSTDataset(bert_test, tokenizer, MAX_LEN)\n",
        "\n",
        "# Definition of the Dataloader that will feed the data in batches to the neural network for suitable training and processing.\n",
        "training_loader_st = DataLoader(train_dataset_st, batch_size=BATCH_SIZE)\n",
        "val_loader_st = DataLoader(val_dataset_st, batch_size=BATCH_SIZE)\n",
        "test_loader_st = DataLoader(test_dataset_st, batch_size=BATCH_SIZE)\n",
        "\n",
        "model_st = BERTClassST(MODEL_NAME)\n",
        "model_st.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8kfW8ag_-mx2"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.CrossEntropyLoss()(outputs, targets) #+ torch.nn.BCEWithLogitsLoss()(outputs[7], targets[7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SVr0xElG-mx2"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params =  model_st.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(training_loader_st)*EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXu5YNUZ-mx3"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(training_loader)*EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1FiAu2Dg-mx3"
      },
      "outputs": [],
      "source": [
        "def train_st():\n",
        "    size = len(training_loader_st)\n",
        "    model_st.train()\n",
        "    for batch, data in enumerate(training_loader_st, 0):\n",
        "        ids_intra = data['input_ids_intra'].to(device, dtype = torch.long)\n",
        "        mask_intra = data['attention_mask_intra'].to(device, dtype = torch.long)\n",
        "        token_type_ids_intra = data['token_type_ids_intra'].to(device, dtype = torch.long)\n",
        "        ids_inter = data['input_ids_inter'].to(device, dtype = torch.long)\n",
        "        mask_inter = data['attention_mask_inter'].to(device, dtype = torch.long)\n",
        "        token_type_ids_inter = data['token_type_ids_inter'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model_st(ids_intra, mask_intra, token_type_ids_intra, ids_inter, mask_inter, token_type_ids_inter)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if batch % 100 == 0:\n",
        "            print(f\"Batch {batch} of {size}. Loss: {loss.item()}\")\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhcV1WiB-mx3"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    size = len(training_loader.dataset)\n",
        "    model.train()\n",
        "    for batch,data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        lenght = data['length'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        if batch%100==0:\n",
        "            current =  batch * len(data['ids'])\n",
        "            print(f\"Train loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jEVB1Ydd-mx3"
      },
      "outputs": [],
      "source": [
        "def validation_st(epoch, val_loss_min_input):\n",
        "    num_batches = len(test_loader_st)\n",
        "    model_st.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, data in enumerate(val_loader_st, 0):\n",
        "            ids_intra = data['input_ids_intra'].to(device, dtype = torch.long)\n",
        "            mask_intra = data['attention_mask_intra'].to(device, dtype = torch.long)\n",
        "            token_type_ids_intra = data['token_type_ids_intra'].to(device, dtype = torch.long)\n",
        "            ids_inter = data['input_ids_inter'].to(device, dtype = torch.long)\n",
        "            mask_inter = data['attention_mask_inter'].to(device, dtype = torch.long)\n",
        "            token_type_ids_inter = data['token_type_ids_inter'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "            outputs = model_st(ids_intra, mask_intra, token_type_ids_intra, ids_inter, mask_inter, token_type_ids_inter)\n",
        "            val_loss += loss_fn(outputs, targets).item()\n",
        "        val_loss /= num_batches\n",
        "        #outputs, targets = fin_outputs, fin_targets\n",
        "        print(f\"\\nValidation loss: {val_loss:>8f}.\")\n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if val_loss <= val_loss_min_input:\n",
        "            #create checkpoint variable and add important data\n",
        "            if epoch > 0:\n",
        "                print('Validation loss decreased ({:.8f} --> {:.8f}).  Saving model ...'.format(val_loss_min_input, val_loss))\n",
        "            else: print('Saving model ...')\n",
        "            # save best moel\n",
        "            torch.save(model_st.state_dict(), \"model_st_bert_standard_project.pth\")\n",
        "            print(\"Saved PyTorch Model State to model.pth\\n\")\n",
        "            val_loss_min_input = val_loss\n",
        "\n",
        "    return val_loss_min_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63RmbCYc-mx3"
      },
      "outputs": [],
      "source": [
        "def validation(epoch, val_loss_min_input):\n",
        "    num_batches = len(test_loader)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    #fin_targets=[]\n",
        "    #fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            # lenght = data['length'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            val_loss += loss_fn(outputs, targets).item()\n",
        "\n",
        "        val_loss /= num_batches\n",
        "        #outputs, targets = fin_outputs, fin_targets\n",
        "        print(f\"\\nValidation loss: {val_loss:>8f}.\")\n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if val_loss <= val_loss_min_input:\n",
        "            #create checkpoint variable and add important data\n",
        "            if epoch > 0:\n",
        "                print('Validation loss decreased ({:.8f} --> {:.8f}).  Saving model ...'.format(val_loss_min_input, val_loss))\n",
        "            else: print('Saving model ...')\n",
        "            # save best moel\n",
        "            torch.save(model.state_dict(), \"model_bert_standard_project.pth\")\n",
        "            print(\"Saved PyTorch Model State to model.pth\\n\")\n",
        "            val_loss_min_input = val_loss\n",
        "\n",
        "    return val_loss_min_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q2cwXKY-mx3"
      },
      "outputs": [],
      "source": [
        "val_loss_min = np.inf\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train()\n",
        "    val_loss_min = validation(epoch, val_loss_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWovRSaG-mx4",
        "outputId": "db1e7574-1369-44b0-e1dc-577572fbd8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Batch 0 of 3471. Loss: 2.4452013969421387\n",
            "Batch 100 of 3471. Loss: 1.3591538667678833\n",
            "Batch 200 of 3471. Loss: 2.0095245838165283\n",
            "Batch 300 of 3471. Loss: 2.2295665740966797\n",
            "Batch 400 of 3471. Loss: 2.624849557876587\n",
            "Batch 500 of 3471. Loss: 3.1785202026367188\n",
            "Batch 600 of 3471. Loss: 1.8338513374328613\n",
            "Batch 700 of 3471. Loss: 1.8875243663787842\n",
            "Batch 800 of 3471. Loss: 2.2537331581115723\n",
            "Batch 900 of 3471. Loss: 1.5238959789276123\n",
            "Batch 1000 of 3471. Loss: 2.0192935466766357\n",
            "Batch 1100 of 3471. Loss: 2.7999229431152344\n",
            "Batch 1200 of 3471. Loss: 1.7216740846633911\n",
            "Batch 1300 of 3471. Loss: 1.9662268161773682\n",
            "Batch 1400 of 3471. Loss: 1.626046895980835\n",
            "Batch 1500 of 3471. Loss: 1.734048843383789\n",
            "Batch 1600 of 3471. Loss: 2.3043603897094727\n",
            "Batch 1700 of 3471. Loss: 1.3990352153778076\n",
            "Batch 1800 of 3471. Loss: 2.1078357696533203\n",
            "Batch 1900 of 3471. Loss: 1.9711142778396606\n",
            "Batch 2000 of 3471. Loss: 1.6618149280548096\n",
            "Batch 2100 of 3471. Loss: 2.913597583770752\n",
            "Batch 2200 of 3471. Loss: 1.7417690753936768\n",
            "Batch 2300 of 3471. Loss: 2.1596474647521973\n",
            "Batch 2400 of 3471. Loss: 2.041337728500366\n",
            "Batch 2500 of 3471. Loss: 2.0851528644561768\n",
            "Batch 2600 of 3471. Loss: 2.285317897796631\n",
            "Batch 2700 of 3471. Loss: 2.096580982208252\n",
            "Batch 2800 of 3471. Loss: 1.625258445739746\n",
            "Batch 2900 of 3471. Loss: 2.0547311305999756\n",
            "Batch 3000 of 3471. Loss: 1.8252363204956055\n",
            "Batch 3100 of 3471. Loss: 1.716698408126831\n",
            "Batch 3200 of 3471. Loss: 1.5409518480300903\n",
            "Batch 3300 of 3471. Loss: 2.0181660652160645\n",
            "Batch 3400 of 3471. Loss: 1.6759685277938843\n",
            "\n",
            "Validation loss: 2.017212.\n",
            "Saving model ...\n",
            "Saved PyTorch Model State to model.pth\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Batch 0 of 3471. Loss: 2.1117496490478516\n",
            "Batch 100 of 3471. Loss: 1.2051068544387817\n",
            "Batch 200 of 3471. Loss: 2.0274505615234375\n",
            "Batch 300 of 3471. Loss: 2.140618324279785\n",
            "Batch 400 of 3471. Loss: 2.5165021419525146\n",
            "Batch 500 of 3471. Loss: 3.3072617053985596\n",
            "Batch 600 of 3471. Loss: 1.9567484855651855\n",
            "Batch 700 of 3471. Loss: 1.7641844749450684\n",
            "Batch 800 of 3471. Loss: 2.514805316925049\n",
            "Batch 900 of 3471. Loss: 1.4494892358779907\n",
            "Batch 1000 of 3471. Loss: 1.8291531801223755\n",
            "Batch 1100 of 3471. Loss: 2.682075023651123\n",
            "Batch 1200 of 3471. Loss: 1.6533395051956177\n",
            "Batch 1300 of 3471. Loss: 2.0133278369903564\n",
            "Batch 1400 of 3471. Loss: 1.4144258499145508\n",
            "Batch 1500 of 3471. Loss: 1.8411815166473389\n",
            "Batch 1600 of 3471. Loss: 2.29048490524292\n",
            "Batch 1700 of 3471. Loss: 1.3446812629699707\n",
            "Batch 1800 of 3471. Loss: 2.037135124206543\n",
            "Batch 1900 of 3471. Loss: 2.0484466552734375\n",
            "Batch 2000 of 3471. Loss: 1.527933120727539\n",
            "Batch 2100 of 3471. Loss: 2.98298716545105\n",
            "Batch 2200 of 3471. Loss: 1.9079002141952515\n",
            "Batch 2300 of 3471. Loss: 2.087832450866699\n",
            "Batch 2400 of 3471. Loss: 2.0824570655822754\n",
            "Batch 2500 of 3471. Loss: 1.835400938987732\n",
            "Batch 2600 of 3471. Loss: 2.2537314891815186\n",
            "Batch 2700 of 3471. Loss: 2.179741621017456\n",
            "Batch 2800 of 3471. Loss: 1.6009677648544312\n",
            "Batch 2900 of 3471. Loss: 1.9363951683044434\n",
            "Batch 3000 of 3471. Loss: 1.688906192779541\n",
            "Batch 3100 of 3471. Loss: 1.5369675159454346\n",
            "Batch 3200 of 3471. Loss: 1.6517003774642944\n",
            "Batch 3300 of 3471. Loss: 2.2764525413513184\n",
            "Batch 3400 of 3471. Loss: 1.6789146661758423\n",
            "\n",
            "Validation loss: 1.946706.\n",
            "Validation loss decreased (2.01721190 --> 1.94670615).  Saving model ...\n",
            "Saved PyTorch Model State to model.pth\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Batch 0 of 3471. Loss: 1.9714696407318115\n",
            "Batch 100 of 3471. Loss: 1.0640968084335327\n",
            "Batch 200 of 3471. Loss: 2.1914803981781006\n",
            "Batch 300 of 3471. Loss: 2.114206314086914\n",
            "Batch 400 of 3471. Loss: 2.079765796661377\n",
            "Batch 500 of 3471. Loss: 2.995337724685669\n",
            "Batch 600 of 3471. Loss: 1.7553179264068604\n",
            "Batch 700 of 3471. Loss: 1.590391755104065\n",
            "Batch 800 of 3471. Loss: 2.4363880157470703\n",
            "Batch 900 of 3471. Loss: 1.2270019054412842\n",
            "Batch 1000 of 3471. Loss: 1.82794189453125\n",
            "Batch 1100 of 3471. Loss: 2.7478058338165283\n",
            "Batch 1200 of 3471. Loss: 1.4892041683197021\n",
            "Batch 1300 of 3471. Loss: 1.9174857139587402\n",
            "Batch 1400 of 3471. Loss: 1.6044687032699585\n",
            "Batch 1500 of 3471. Loss: 1.7341128587722778\n",
            "Batch 1600 of 3471. Loss: 2.330949544906616\n",
            "Batch 1700 of 3471. Loss: 1.4738694429397583\n",
            "Batch 1800 of 3471. Loss: 1.9386802911758423\n",
            "Batch 1900 of 3471. Loss: 1.9060742855072021\n",
            "Batch 2000 of 3471. Loss: 1.2799739837646484\n",
            "Batch 2100 of 3471. Loss: 2.7306597232818604\n",
            "Batch 2200 of 3471. Loss: 1.869379997253418\n",
            "Batch 2300 of 3471. Loss: 1.734574794769287\n",
            "Batch 2400 of 3471. Loss: 2.102663516998291\n",
            "Batch 2500 of 3471. Loss: 1.8948420286178589\n",
            "Batch 2600 of 3471. Loss: 2.1235079765319824\n",
            "Batch 2700 of 3471. Loss: 2.0069262981414795\n",
            "Batch 2800 of 3471. Loss: 1.6206835508346558\n",
            "Batch 2900 of 3471. Loss: 1.8409032821655273\n",
            "Batch 3000 of 3471. Loss: 1.6021003723144531\n",
            "Batch 3100 of 3471. Loss: 1.7779079675674438\n",
            "Batch 3200 of 3471. Loss: 1.4709099531173706\n",
            "Batch 3300 of 3471. Loss: 2.0917792320251465\n",
            "Batch 3400 of 3471. Loss: 1.6154415607452393\n",
            "\n",
            "Validation loss: 1.903448.\n",
            "Validation loss decreased (1.94670615 --> 1.90344803).  Saving model ...\n",
            "Saved PyTorch Model State to model.pth\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Batch 0 of 3471. Loss: 1.85189950466156\n",
            "Batch 100 of 3471. Loss: 1.1210010051727295\n",
            "Batch 200 of 3471. Loss: 2.045098304748535\n",
            "Batch 300 of 3471. Loss: 2.049736976623535\n",
            "Batch 400 of 3471. Loss: 2.4174036979675293\n",
            "Batch 500 of 3471. Loss: 2.881983757019043\n",
            "Batch 600 of 3471. Loss: 1.6776262521743774\n",
            "Batch 700 of 3471. Loss: 1.588437795639038\n",
            "Batch 800 of 3471. Loss: 2.2467989921569824\n",
            "Batch 900 of 3471. Loss: 1.1022014617919922\n",
            "Batch 1000 of 3471. Loss: 1.7185925245285034\n",
            "Batch 1100 of 3471. Loss: 2.552687168121338\n",
            "Batch 1200 of 3471. Loss: 1.3712584972381592\n",
            "Batch 1300 of 3471. Loss: 1.843430519104004\n",
            "Batch 1400 of 3471. Loss: 1.3511548042297363\n",
            "Batch 1500 of 3471. Loss: 1.763735055923462\n",
            "Batch 1600 of 3471. Loss: 2.211191177368164\n",
            "Batch 1700 of 3471. Loss: 1.281855821609497\n",
            "Batch 1800 of 3471. Loss: 1.8709158897399902\n",
            "Batch 1900 of 3471. Loss: 1.9184560775756836\n",
            "Batch 2000 of 3471. Loss: 1.3427320718765259\n",
            "Batch 2100 of 3471. Loss: 2.4826531410217285\n",
            "Batch 2200 of 3471. Loss: 1.7879304885864258\n",
            "Batch 2300 of 3471. Loss: 1.740060806274414\n",
            "Batch 2400 of 3471. Loss: 2.017868995666504\n",
            "Batch 2500 of 3471. Loss: 1.7137622833251953\n",
            "Batch 2600 of 3471. Loss: 1.9788050651550293\n",
            "Batch 2700 of 3471. Loss: 2.005103588104248\n",
            "Batch 2800 of 3471. Loss: 1.5869189500808716\n",
            "Batch 2900 of 3471. Loss: 1.5989876985549927\n",
            "Batch 3000 of 3471. Loss: 1.6812283992767334\n",
            "Batch 3100 of 3471. Loss: 1.5602227449417114\n",
            "Batch 3200 of 3471. Loss: 1.5966472625732422\n",
            "Batch 3300 of 3471. Loss: 2.0535988807678223\n",
            "Batch 3400 of 3471. Loss: 1.6301065683364868\n",
            "\n",
            "Validation loss: 1.864719.\n",
            "Validation loss decreased (1.90344803 --> 1.86471934).  Saving model ...\n",
            "Saved PyTorch Model State to model.pth\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Batch 0 of 3471. Loss: 1.8809620141983032\n",
            "Batch 100 of 3471. Loss: 1.0932693481445312\n",
            "Batch 200 of 3471. Loss: 1.9791369438171387\n",
            "Batch 300 of 3471. Loss: 1.9970781803131104\n",
            "Batch 400 of 3471. Loss: 2.2565150260925293\n",
            "Batch 500 of 3471. Loss: 2.708523750305176\n",
            "Batch 600 of 3471. Loss: 1.353102445602417\n",
            "Batch 700 of 3471. Loss: 1.6131463050842285\n",
            "Batch 800 of 3471. Loss: 2.4013757705688477\n",
            "Batch 900 of 3471. Loss: 1.1303269863128662\n",
            "Batch 1000 of 3471. Loss: 1.6476025581359863\n",
            "Batch 1100 of 3471. Loss: 2.7173101902008057\n",
            "Batch 1200 of 3471. Loss: 1.3293216228485107\n",
            "Batch 1300 of 3471. Loss: 2.131722927093506\n",
            "Batch 1400 of 3471. Loss: 1.4331592321395874\n",
            "Batch 1500 of 3471. Loss: 1.5930057764053345\n",
            "Batch 1600 of 3471. Loss: 2.2102296352386475\n",
            "Batch 1700 of 3471. Loss: 1.449275016784668\n",
            "Batch 1800 of 3471. Loss: 1.742526888847351\n",
            "Batch 1900 of 3471. Loss: 1.7026666402816772\n",
            "Batch 2000 of 3471. Loss: 1.3418092727661133\n",
            "Batch 2100 of 3471. Loss: 1.9977641105651855\n",
            "Batch 2200 of 3471. Loss: 1.8697524070739746\n",
            "Batch 2300 of 3471. Loss: 1.6997780799865723\n",
            "Batch 2400 of 3471. Loss: 1.8486711978912354\n",
            "Batch 2500 of 3471. Loss: 1.777292251586914\n",
            "Batch 2600 of 3471. Loss: 1.7081639766693115\n",
            "Batch 2700 of 3471. Loss: 1.9130771160125732\n",
            "Batch 2800 of 3471. Loss: 1.6163439750671387\n",
            "Batch 2900 of 3471. Loss: 1.5978446006774902\n",
            "Batch 3000 of 3471. Loss: 1.5806331634521484\n",
            "Batch 3100 of 3471. Loss: 1.5718601942062378\n",
            "Batch 3200 of 3471. Loss: 1.3669461011886597\n",
            "Batch 3300 of 3471. Loss: 1.7702240943908691\n",
            "Batch 3400 of 3471. Loss: 1.5791743993759155\n",
            "\n",
            "Validation loss: 1.832589.\n",
            "Validation loss decreased (1.86471934 --> 1.83258914).  Saving model ...\n",
            "Saved PyTorch Model State to model.pth\n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Batch 0 of 3471. Loss: 2.0073137283325195\n",
            "Batch 100 of 3471. Loss: 1.056292176246643\n",
            "Batch 200 of 3471. Loss: 1.8168563842773438\n",
            "Batch 300 of 3471. Loss: 2.0416364669799805\n",
            "Batch 400 of 3471. Loss: 1.9816874265670776\n",
            "Batch 500 of 3471. Loss: 2.7824196815490723\n",
            "Batch 600 of 3471. Loss: 1.524405837059021\n",
            "Batch 700 of 3471. Loss: 1.7910089492797852\n",
            "Batch 800 of 3471. Loss: 2.294640064239502\n",
            "Batch 900 of 3471. Loss: 1.3292964696884155\n",
            "Batch 1000 of 3471. Loss: 1.632332682609558\n",
            "Batch 1100 of 3471. Loss: 2.5262434482574463\n",
            "Batch 1200 of 3471. Loss: 1.229001760482788\n",
            "Batch 1300 of 3471. Loss: 1.7486789226531982\n",
            "Batch 1400 of 3471. Loss: 1.2010200023651123\n",
            "Batch 1500 of 3471. Loss: 1.5776028633117676\n",
            "Batch 1600 of 3471. Loss: 2.2555720806121826\n",
            "Batch 1700 of 3471. Loss: 1.4225735664367676\n",
            "Batch 1800 of 3471. Loss: 1.7160937786102295\n",
            "Batch 1900 of 3471. Loss: 1.715726375579834\n",
            "Batch 2000 of 3471. Loss: 1.2912253141403198\n",
            "Batch 2100 of 3471. Loss: 1.8219243288040161\n",
            "Batch 2200 of 3471. Loss: 1.9060922861099243\n",
            "Batch 2300 of 3471. Loss: 1.8529398441314697\n",
            "Batch 2400 of 3471. Loss: 2.0438685417175293\n",
            "Batch 2500 of 3471. Loss: 1.9885731935501099\n",
            "Batch 2600 of 3471. Loss: 1.8490463495254517\n",
            "Batch 2700 of 3471. Loss: 2.1507272720336914\n",
            "Batch 2800 of 3471. Loss: 1.5026520490646362\n",
            "Batch 2900 of 3471. Loss: 1.6964266300201416\n",
            "Batch 3000 of 3471. Loss: 1.6948091983795166\n",
            "Batch 3100 of 3471. Loss: 1.6084940433502197\n",
            "Batch 3200 of 3471. Loss: 1.3930690288543701\n",
            "Batch 3300 of 3471. Loss: 1.7031397819519043\n",
            "Batch 3400 of 3471. Loss: 1.4355871677398682\n",
            "\n",
            "Validation loss: 1.805667.\n",
            "Validation loss decreased (1.83258914 --> 1.80566744).  Saving model ...\n",
            "Saved PyTorch Model State to model.pth\n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Batch 0 of 3471. Loss: 1.8112281560897827\n",
            "Batch 100 of 3471. Loss: 1.2134445905685425\n",
            "Batch 200 of 3471. Loss: 1.889768362045288\n",
            "Batch 300 of 3471. Loss: 1.9646090269088745\n",
            "Batch 400 of 3471. Loss: 2.0672898292541504\n",
            "Batch 500 of 3471. Loss: 2.8495171070098877\n",
            "Batch 600 of 3471. Loss: 1.5954563617706299\n",
            "Batch 700 of 3471. Loss: 1.6846661567687988\n",
            "Batch 800 of 3471. Loss: 2.4680919647216797\n",
            "Batch 900 of 3471. Loss: 1.0617196559906006\n",
            "Batch 1000 of 3471. Loss: 1.6400099992752075\n",
            "Batch 1100 of 3471. Loss: 2.4349493980407715\n",
            "Batch 1200 of 3471. Loss: 1.366180181503296\n",
            "Batch 1300 of 3471. Loss: 1.6333153247833252\n",
            "Batch 1400 of 3471. Loss: 1.210758924484253\n",
            "Batch 1500 of 3471. Loss: 1.352301001548767\n",
            "Batch 1600 of 3471. Loss: 2.1215929985046387\n",
            "Batch 1700 of 3471. Loss: 1.217693567276001\n",
            "Batch 1800 of 3471. Loss: 1.7877013683319092\n",
            "Batch 1900 of 3471. Loss: 1.6706395149230957\n",
            "Batch 2000 of 3471. Loss: 1.038933515548706\n",
            "Batch 2100 of 3471. Loss: 1.8078196048736572\n",
            "Batch 2200 of 3471. Loss: 1.6779388189315796\n",
            "Batch 2300 of 3471. Loss: 1.6544923782348633\n",
            "Batch 2400 of 3471. Loss: 1.7944248914718628\n",
            "Batch 2500 of 3471. Loss: 1.7136908769607544\n",
            "Batch 2600 of 3471. Loss: 1.9334559440612793\n",
            "Batch 2700 of 3471. Loss: 1.7171916961669922\n",
            "Batch 2800 of 3471. Loss: 1.354442834854126\n",
            "Batch 2900 of 3471. Loss: 1.4676153659820557\n",
            "Batch 3000 of 3471. Loss: 1.4363951683044434\n",
            "Batch 3100 of 3471. Loss: 1.4625627994537354\n",
            "Batch 3200 of 3471. Loss: 1.431318998336792\n",
            "Batch 3300 of 3471. Loss: 1.7386488914489746\n",
            "Batch 3400 of 3471. Loss: 1.5037323236465454\n",
            "\n",
            "Validation loss: 1.773568.\n",
            "Validation loss decreased (1.80566744 --> 1.77356811).  Saving model ...\n",
            "Saved PyTorch Model State to model.pth\n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Batch 0 of 3471. Loss: 1.8332924842834473\n",
            "Batch 100 of 3471. Loss: 0.9692368507385254\n",
            "Batch 200 of 3471. Loss: 1.701904058456421\n",
            "Batch 300 of 3471. Loss: 1.9280216693878174\n",
            "Batch 400 of 3471. Loss: 2.1529202461242676\n",
            "Batch 500 of 3471. Loss: 2.4910216331481934\n",
            "Batch 600 of 3471. Loss: 1.5256685018539429\n",
            "Batch 700 of 3471. Loss: 1.4264689683914185\n",
            "Batch 800 of 3471. Loss: 2.283994197845459\n",
            "Batch 900 of 3471. Loss: 1.1706089973449707\n",
            "Batch 1000 of 3471. Loss: 1.5985379219055176\n"
          ]
        }
      ],
      "source": [
        "val_loss_min_st = np.inf\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
        "    train_st()\n",
        "    val_loss_min_st = validation_st(epoch, val_loss_min_st)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3eT7GtHz1y6R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}