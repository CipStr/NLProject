{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Standard Project:\n",
    "\n",
    "- Students: **Matteo Belletti**, **Alessandro Pasi**, **Stricescu Razvan Ciprian**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NLProject\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import pandas as pd\n",
    "import json\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 4000\n",
      "Example of a sample: {'episode': 'utterance_0', 'speakers': ['Chandler', 'The Interviewer', 'Chandler', 'The Interviewer', 'Chandler'], 'emotions': ['neutral', 'neutral', 'neutral', 'neutral', 'surprise'], 'utterances': [\"also I was the point person on my company's transition from the KL-5 to GR-6 system.\", \"You must've had your hands full.\", 'That I did. That I did.', \"So let's talk a little bit about your duties.\", 'My duties?  All right.'], 'triggers': [0.0, 0.0, 0.0, 1.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "# open json in project_data_MELD folder\n",
    "with open('project_data_MELD/MELD_train_efr.json') as f:\n",
    "    data = json.load(f)\n",
    "print(f\"Number of samples: {len(data)}\")\n",
    "print(f\"Example of a sample: {data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (4000, 4)\n",
      "Dataframe columns: Index(['speakers', 'emotions', 'utterances', 'triggers'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Convert data to pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "# drop episode and speakers columns\n",
    "df = df.drop(columns=['episode'])\n",
    "print(f\"Dataframe shape: {df.shape}\")\n",
    "print(f\"Dataframe columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing nan values to zeros in order to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"triggers\"] = df[\"triggers\"].apply(lambda x: [0 if elem != 1 and elem != 0 else elem for elem in x])\n",
    "df[\"triggers\"][3359]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into train, validation and test sets with a 80-10-10 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_val, df_test = train_test_split(temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3200, 4)\n",
      "Val shape: (400, 4)\n",
      "Test shape: (400, 4)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Val shape: {df_val.shape}\")\n",
    "print(f\"Test shape: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As baselines models we need to implement a random model and a majority class model for emotions and triggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neutral': 12066, 'joy': 4986, 'surprise': 3664, 'anger': 3203, 'sadness': 2108, 'fear': 889, 'disgust': 848}\n"
     ]
    }
   ],
   "source": [
    "# first we create a dictionary of all emotions with their corresponding occurences\n",
    "emotions_dict = {}\n",
    "for emotions in df_train[\"emotions\"]:\n",
    "    for emotion in emotions:\n",
    "        if emotion in emotions_dict:\n",
    "            emotions_dict[emotion] += 1\n",
    "        else:\n",
    "            emotions_dict[emotion] = 1\n",
    "\n",
    "# then we sort the dictionary by occurences\n",
    "emotions_dict = {k: v for k, v in sorted(emotions_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(emotions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distribution of emotions could also be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the emotions distribution as histogram\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(emotions_dict.keys(), emotions_dict.values(), color='blue')\n",
    "plt.title(\"Emotions distribution in the training set\")\n",
    "plt.xlabel(\"Emotions\")\n",
    "plt.ylabel(\"Number of occurences\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also implement some text processing techniques such as stemming, lemmatization, stop words removal, etc. \n",
    "\n",
    "In this case we will use nltk library for lemmatization and stop words removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ciprian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ciprian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ciprian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ciprian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "bad_symbols = re.compile('[^a-z ]')\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to use lower case because we are going to use bert uncased models further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = bad_symbols.sub('', text)\n",
    "    text = word_tokenize(text)\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train, val and test sets after preprocessing: \n",
      "Train shape: (3200, 4)\n",
      "Val shape: (400, 4)\n",
      "Test shape: (400, 4)\n"
     ]
    }
   ],
   "source": [
    "baseline_train = df_train.copy()\n",
    "baseline_train[\"utterances\"] = baseline_train[\"utterances\"].apply(lambda x: [preprocess_text(elem) for elem in x])\n",
    "baseline_test = df_test.copy()\n",
    "baseline_test[\"utterances\"] = baseline_test[\"utterances\"].apply(lambda x: [preprocess_text(elem) for elem in x])\n",
    "baseline_val = df_val.copy()\n",
    "baseline_val[\"utterances\"] = baseline_val[\"utterances\"].apply(lambda x: [preprocess_text(elem) for elem in x])\n",
    "print(\"Shape of train, val and test sets after preprocessing: \")\n",
    "print(f\"Train shape: {baseline_train.shape}\")\n",
    "print(f\"Val shape: {baseline_val.shape}\")\n",
    "print(f\"Test shape: {baseline_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the TF-IDF vectorizer we need to split the utterances into single sentences, likewise the emotions and triggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(df, y_label):\n",
    "    X = []\n",
    "    y = []\n",
    "    for index, row in df.iterrows():\n",
    "        for i in range(len(row[\"utterances\"])):\n",
    "            X.append(row[\"utterances\"][i])\n",
    "            y.append(row[y_label][i])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after splitting: 27764\n",
      "Val shape after splitting: 3678\n",
      "Test shape after splitting: 3558\n"
     ]
    }
   ],
   "source": [
    "# Emotions baseline\n",
    "x_train_base, y_train_emotions = splitter(baseline_train, \"emotions\")\n",
    "x_val_base, y_val_emotions = splitter(baseline_val, \"emotions\")\n",
    "x_test_base, y_test_emotions = splitter(baseline_test, \"emotions\")\n",
    "\n",
    "# Triggers baseline\n",
    "_ , y_train_triggers = splitter(baseline_train, \"triggers\")\n",
    "_ , y_val_triggers = splitter(baseline_val, \"triggers\")\n",
    "_ , y_test_triggers = splitter(baseline_test, \"triggers\")\n",
    "\n",
    "print(f\"Train shape after splitting: {len(x_train_base)}\")\n",
    "print(f\"Val shape after splitting: {len(x_val_base)}\")\n",
    "print(f\"Test shape after splitting: {len(x_test_base)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a tokenizer we use TfidfVectorizer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape after vectorization: (27764, 5142)\n",
      "Val shape after vectorization: (3678, 5142)\n",
      "Test shape after vectorization: (3558, 5142)\n",
      "Size of the vocabulary: 5142\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "x_train_base = vectorizer.fit_transform(x_train_base)\n",
    "x_val_base = vectorizer.transform(x_val_base)\n",
    "x_test_base = vectorizer.transform(x_test_base)\n",
    "\n",
    "print(f\"Train shape after vectorization: {x_train_base.shape}\")\n",
    "print(f\"Val shape after vectorization: {x_val_base.shape}\")\n",
    "print(f\"Test shape after vectorization: {x_test_base.shape}\")\n",
    "\n",
    "print(f\"Size of the vocabulary: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation function that returns the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(Y_test, Y_pred):\n",
    "    report = classification_report(Y_test, Y_pred, zero_division=0)\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the dummy classifier for emotions and triggers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf_majority_emotions = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf_random_emotions = DummyClassifier(strategy=\"uniform\")\n",
    "\n",
    "dummy_clf_majority_triggers = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf_random_triggers = DummyClassifier(strategy=\"uniform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluation of the classifiers fitted on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority classifier for emotions: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.00      0.00      0.00       369\n",
      "     disgust       0.00      0.00      0.00       101\n",
      "        fear       0.00      0.00      0.00       109\n",
      "         joy       0.00      0.00      0.00       663\n",
      "     neutral       0.44      1.00      0.61      1572\n",
      "     sadness       0.00      0.00      0.00       258\n",
      "    surprise       0.00      0.00      0.00       486\n",
      "\n",
      "    accuracy                           0.44      3558\n",
      "   macro avg       0.06      0.14      0.09      3558\n",
      "weighted avg       0.20      0.44      0.27      3558\n",
      "\n",
      "-------------------------------------------------------\n",
      "Random classifier for emotions: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.09      0.13      0.11       369\n",
      "     disgust       0.03      0.17      0.05       101\n",
      "        fear       0.04      0.18      0.06       109\n",
      "         joy       0.20      0.15      0.17       663\n",
      "     neutral       0.47      0.14      0.21      1572\n",
      "     sadness       0.07      0.14      0.10       258\n",
      "    surprise       0.14      0.15      0.15       486\n",
      "\n",
      "    accuracy                           0.14      3558\n",
      "   macro avg       0.15      0.15      0.12      3558\n",
      "weighted avg       0.28      0.14      0.17      3558\n",
      "\n",
      "-------------------------------------------------------\n",
      "Majority classifier for triggers: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      1.00      0.92      3027\n",
      "         1.0       0.00      0.00      0.00       531\n",
      "\n",
      "    accuracy                           0.85      3558\n",
      "   macro avg       0.43      0.50      0.46      3558\n",
      "weighted avg       0.72      0.85      0.78      3558\n",
      "\n",
      "-------------------------------------------------------\n",
      "Random classifier for triggers: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.50      0.63      3027\n",
      "         1.0       0.15      0.51      0.23       531\n",
      "\n",
      "    accuracy                           0.50      3558\n",
      "   macro avg       0.50      0.51      0.43      3558\n",
      "weighted avg       0.75      0.50      0.57      3558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Emotions baseline\n",
    "dummy_clf_majority_emotions.fit(x_train_base, y_train_emotions)\n",
    "dummy_clf_random_emotions.fit(x_train_base, y_train_emotions)\n",
    "\n",
    "y_pred_majority_emotions = dummy_clf_majority_emotions.predict(x_test_base)\n",
    "y_pred_random_emotions = dummy_clf_random_emotions.predict(x_test_base)\n",
    "print(\"Majority classifier for emotions: \\n\")\n",
    "print(evaluate(y_test_emotions, y_pred_majority_emotions))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Random classifier for emotions: \\n\")\n",
    "print(evaluate(y_test_emotions, y_pred_random_emotions))\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "# Triggers baseline\n",
    "dummy_clf_majority_triggers.fit(x_train_base, y_train_triggers)\n",
    "dummy_clf_random_triggers.fit(x_train_base, y_train_triggers)\n",
    "\n",
    "y_pred_majority_triggers = dummy_clf_majority_triggers.predict(x_test_base)\n",
    "y_pred_random_triggers = dummy_clf_random_triggers.predict(x_test_base)\n",
    "print(\"Majority classifier for triggers: \\n\")\n",
    "print(evaluate(y_test_triggers, y_pred_majority_triggers))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Random classifier for triggers: \\n\")\n",
    "print(evaluate(y_test_triggers, y_pred_random_triggers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\NLProject\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "transformers.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences distribution: [9, 2, 3, 14, 4, 10, 8, 10, 19, 17, 4, 4, 6, 8, 10, 3, 9, 8, 22, 7, 9, 9, 12, 11, 7, 8, 4, 4, 8, 8, 5, 7, 14, 12, 7, 3, 8, 4, 13, 10, 13, 9, 3, 4, 14, 16, 5, 3, 11, 11, 3, 6, 5, 3, 15, 4, 6, 13, 7, 10, 7, 3, 4, 5, 22, 19, 8, 9, 8, 8, 3, 15, 6, 9, 6, 12, 3, 6, 19, 5, 7, 11, 6, 5, 4, 17, 6, 3, 3, 6, 12, 14, 4, 14, 5, 14, 14, 6, 4, 8, 4, 12, 18, 3, 5, 4, 4, 14, 12, 4, 5, 3, 11, 3, 10, 5, 13, 8, 5, 10, 10, 9, 6, 3, 9, 14, 9, 7, 5, 15, 16, 3, 9, 21, 4, 17, 6, 12, 9, 6, 10, 12, 2, 6, 9, 17, 17, 8, 15, 12, 24, 4, 16, 14, 19, 4, 4, 11, 4, 4, 8, 9, 19, 16, 6, 8, 4, 2, 8, 4, 8, 9, 15, 18, 19, 5, 18, 5, 14, 11, 5, 6, 5, 11, 3, 16, 5, 16, 10, 8, 3, 16, 8, 11, 20, 8, 10, 5, 4, 11, 16, 4, 9, 14, 8, 14, 8, 13, 4, 2, 10, 7, 14, 5, 6, 5, 6, 11, 15, 11, 5, 3, 6, 9, 3, 7, 6, 8, 8, 19, 6, 8, 3, 9, 6, 4, 4, 3, 9, 5, 8, 5, 8, 8, 7, 12, 16, 9, 9, 3, 4, 5, 9, 7, 18, 3, 3, 14, 10, 5, 4, 2, 4, 6, 3, 10, 8, 7, 6, 3, 5, 5, 8, 3, 8, 7, 5, 12, 7, 21, 6, 16, 10, 15, 4, 13, 12, 8, 8, 13, 16, 8, 9, 10, 4, 13, 11, 14, 6, 11, 7, 14, 3, 3, 3, 5, 8, 3, 5, 3, 7, 8, 7, 6, 13, 6, 8, 5, 14, 5, 6, 19, 9, 5, 3, 8, 9, 8, 18, 13, 3, 14, 8, 20, 12, 12, 16, 9, 4, 15, 12, 7, 11, 8, 12, 12, 7, 7, 20, 7, 7, 15, 13, 5, 6, 11, 3, 3, 4, 16, 8, 5, 11, 11, 8, 15, 9, 9, 10, 4, 3, 4, 14, 5, 19, 13, 11, 2, 5, 9, 3, 9, 9, 7, 16, 11, 3, 5, 19, 11, 6, 3, 12, 6, 9, 4, 5, 10, 14, 15, 7, 4, 8, 5, 10, 5, 10, 10, 3, 6, 5, 7, 7, 10, 2, 4, 6, 10, 4, 12, 7, 13, 5, 14, 12, 14, 11, 21, 8, 7, 10, 8, 17, 19, 2, 6, 3, 6, 10, 3, 10, 8, 16, 8, 2, 5, 12, 15, 11, 13, 7, 5, 6, 2, 9, 5, 19, 10, 12, 9, 12, 3, 7, 6, 5, 5, 13, 15, 12, 4, 9, 6, 15, 9, 7, 10, 6, 16, 12, 3, 6, 6, 9, 9, 6, 14, 12, 16, 10, 4, 6, 12, 18, 19, 4, 8, 4, 6, 7, 13, 5, 4, 8, 8, 14, 5, 18, 12, 21, 6, 5, 7, 10, 4, 5, 13, 6, 6, 3, 6, 5, 6, 18, 2, 3, 14, 9, 4, 2, 14, 15, 7, 13, 7, 8, 3, 4, 5, 5, 5, 12, 7, 10, 5, 5, 5, 3, 11, 12, 9, 4, 10, 6, 11, 11, 5, 13, 9, 17, 7, 4, 7, 3, 10, 6, 3, 7, 16, 5, 5, 3, 8, 7, 8, 5, 3, 13, 18, 6, 13, 15, 10, 13, 11, 12, 6, 7, 7, 10, 3, 6, 6, 6, 12, 22, 6, 16, 10, 8, 10, 9, 3, 8, 8, 3, 11, 5, 11, 8, 8, 3, 12, 15, 4, 4, 3, 12, 9, 8, 9, 2, 7, 6, 6, 9, 7, 5, 9, 7, 6, 11, 3, 14, 5, 17, 9, 7, 14, 4, 6, 8, 6, 3, 4, 11, 8, 6, 7, 21, 6, 15, 3, 8, 9, 11, 11, 5, 16, 5, 2, 12, 4, 10, 12, 8, 6, 13, 17, 4, 6, 9, 6, 19, 6, 9, 7, 10, 11, 9, 3, 15, 7, 12, 8, 11, 22, 14, 11, 6, 3, 11, 5, 4, 7, 12, 3, 10, 11, 14, 13, 2, 10, 4, 6, 5, 11, 15, 3, 10, 7, 14, 8, 10, 6, 4, 7, 15, 9, 3, 6, 4, 8, 11, 6, 12, 7, 9, 11, 7, 4, 12, 13, 14, 6, 4, 12, 6, 3, 7, 11, 11, 3, 4, 10, 2, 7, 8, 13, 10, 4, 3, 8, 12, 7, 11, 9, 14, 11, 8, 3, 10, 4, 4, 9, 3, 5, 10, 5, 20, 8, 13, 15, 10, 12, 5, 4, 7, 8, 3, 7, 10, 7, 16, 7, 5, 9, 10, 4, 3, 19, 19, 8, 6, 16, 3, 6, 6, 9, 6, 8, 6, 8, 4, 3, 9, 4, 14, 10, 10, 7, 6, 5, 16, 3, 8, 12, 10, 6, 7, 15, 4, 14, 3, 5, 11, 11, 8, 4, 4, 11, 9, 7, 17, 10, 10, 9, 6, 7, 4, 11, 5, 12, 5, 18, 8, 5, 7, 6, 8, 6, 7, 18, 8, 6, 15, 8, 8, 4, 10, 12, 7, 4, 4, 4, 12, 10, 8, 10, 13, 22, 16, 11, 3, 2, 3, 6, 9, 3, 12, 12, 3, 14, 3, 11, 4, 6, 3, 16, 18, 8, 5, 16, 7, 4, 7, 6, 13, 18, 7, 17, 9, 18, 9, 11, 13, 14, 5, 9, 9, 5, 21, 3, 3, 9, 8, 11, 13, 4, 15, 14, 20, 4, 8, 12, 8, 3, 9, 9, 4, 19, 8, 3, 6, 10, 11, 13, 6, 4, 6, 9, 3, 6, 15, 6, 3, 3, 4, 4, 4, 17, 7, 16, 3, 9, 15, 21, 15, 18, 10, 14, 6, 10, 18, 4, 10, 6, 6, 3, 8, 6, 14, 7, 6, 2, 6, 10, 3, 5, 6, 5, 8, 10, 11, 5, 7, 13, 13, 4, 16, 11, 12, 6, 5, 6, 6, 13, 6, 11, 11, 15, 6, 6, 7, 8, 9, 5, 9, 4, 7, 5, 4, 17, 9, 10, 12, 3, 8, 4, 3, 15, 6, 9, 6, 8, 4, 12, 3, 8, 16, 2, 11, 9, 4, 5, 2, 6, 15, 9, 5, 3, 7, 8, 9, 8, 5, 6, 6, 8, 12, 4, 5, 13, 10, 11, 16, 18, 10, 4, 11, 8, 5, 9, 9, 4, 13, 4, 3, 7, 14, 19, 8, 10, 18, 4, 8, 3, 14, 3, 8, 20, 6, 3, 7, 7, 16, 14, 8, 7, 8, 9, 6, 5, 5, 13, 7, 10, 6, 4, 4, 8, 10, 4, 4, 6, 10, 16, 6, 8, 4, 19, 3, 2, 2, 4, 12, 5, 7, 4, 20, 16, 10, 4, 6, 8, 16, 9, 7, 10, 9, 9, 11, 17, 17, 11, 4, 3, 17, 7, 3, 10, 2, 4, 7, 10, 14, 6, 6, 4, 5, 9, 5, 12, 8, 12, 11, 7, 6, 6, 4, 8, 3, 6, 3, 3, 9, 3, 5, 8, 6, 12, 5, 10, 6, 15, 12, 14, 17, 14, 17, 17, 22, 12, 8, 14, 10, 17, 19, 4, 12, 7, 8, 2, 10, 17, 4, 6, 4, 8, 6, 23, 9, 6, 9, 3, 4, 11, 5, 9, 9, 10, 11, 18, 9, 6, 5, 17, 12, 9, 20, 20, 3, 6, 19, 14, 11, 15, 4, 9, 6, 4, 20, 9, 15, 5, 6, 8, 8, 8, 23, 10, 8, 7, 11, 10, 12, 9, 12, 13, 4, 8, 6, 11, 11, 14, 16, 9, 10, 4, 4, 7, 12, 5, 9, 8, 7, 9, 6, 3, 3, 17, 14, 7, 20, 7, 6, 11, 4, 6, 14, 13, 7, 9, 3, 4, 4, 15, 4, 4, 3, 10, 5, 10, 3, 6, 5, 9, 7, 3, 4, 6, 4, 4, 4, 3, 7, 13, 6, 4, 4, 5, 7, 3, 10, 10, 9, 9, 7, 6, 5, 11, 10, 11, 8, 6, 12, 10, 4, 4, 8, 9, 14, 21, 11, 4, 7, 3, 7, 15, 3, 6, 14, 5, 19, 7, 5, 4, 8, 13, 6, 14, 22, 5, 3, 11, 4, 3, 14, 5, 17, 6, 8, 5, 9, 4, 6, 16, 3, 3, 10, 21, 10, 17, 17, 6, 13, 4, 4, 10, 4, 12, 12, 7, 13, 15, 12, 7, 6, 14, 7, 2, 7, 5, 14, 6, 3, 4, 5, 6, 7, 6, 16, 9, 4, 9, 7, 12, 5, 5, 4, 8, 13, 4, 15, 7, 11, 6, 17, 3, 12, 9, 14, 11, 15, 11, 10, 7, 13, 11, 4, 3, 7, 7, 4, 21, 5, 10, 4, 16, 5, 9, 13, 4, 14, 7, 5, 5, 10, 10, 2, 6, 11, 4, 6, 15, 14, 9, 13, 8, 7, 3, 4, 13, 7, 5, 3, 8, 3, 11, 16, 10, 18, 13, 8, 12, 8, 5, 3, 11, 2, 11, 9, 5, 15, 11, 2, 13, 4, 12, 11, 12, 9, 9, 15, 7, 7, 16, 8, 5, 9, 10, 3, 4, 5, 6, 16, 3, 6, 14, 9, 6, 5, 9, 9, 3, 11, 5, 6, 8, 5, 7, 7, 4, 15, 5, 9, 21, 9, 8, 9, 6, 3, 3, 10, 3, 11, 5, 6, 8, 5, 18, 3, 9, 16, 10, 7, 11, 19, 4, 7, 9, 13, 15, 14, 6, 15, 10, 2, 12, 12, 7, 5, 11, 5, 5, 12, 4, 18, 13, 5, 3, 4, 17, 3, 5, 13, 7, 6, 5, 6, 9, 17, 3, 7, 3, 12, 14, 12, 4, 3, 8, 4, 4, 4, 15, 6, 11, 8, 5, 9, 18, 9, 10, 9, 14, 15, 3, 12, 6, 15, 10, 7, 4, 15, 11, 4, 9, 20, 6, 13, 6, 8, 8, 17, 4, 21, 5, 7, 2, 9, 10, 2, 7, 9, 6, 4, 5, 5, 6, 6, 4, 13, 14, 17, 6, 14, 17, 7, 8, 18, 8, 16, 4, 9, 9, 10, 21, 11, 15, 3, 11, 18, 10, 6, 2, 6, 6, 7, 10, 3, 6, 11, 5, 5, 9, 3, 6, 6, 7, 6, 7, 10, 3, 5, 9, 9, 11, 4, 3, 4, 9, 7, 11, 13, 4, 11, 5, 5, 4, 10, 3, 13, 7, 14, 13, 6, 5, 8, 7, 15, 4, 3, 7, 6, 10, 6, 7, 6, 12, 12, 3, 6, 4, 17, 12, 13, 9, 7, 4, 6, 10, 14, 8, 11, 10, 6, 6, 3, 15, 7, 4, 10, 4, 16, 3, 18, 5, 10, 9, 3, 16, 11, 9, 9, 12, 18, 11, 10, 11, 7, 10, 9, 10, 6, 14, 4, 5, 12, 3, 15, 12, 3, 8, 3, 16, 14, 18, 12, 7, 8, 14, 5, 5, 16, 4, 4, 9, 4, 7, 6, 4, 12, 6, 7, 5, 8, 10, 5, 12, 11, 2, 8, 5, 11, 5, 3, 9, 3, 12, 5, 5, 20, 3, 15, 4, 18, 2, 14, 12, 7, 17, 8, 16, 11, 5, 5, 4, 7, 8, 7, 2, 2, 9, 10, 13, 8, 7, 5, 11, 3, 11, 13, 3, 4, 10, 13, 11, 7, 3, 14, 13, 7, 10, 2, 4, 4, 17, 13, 3, 5, 14, 13, 11, 8, 8, 14, 5, 10, 3, 9, 16, 9, 9, 16, 12, 8, 14, 13, 3, 3, 10, 8, 10, 6, 5, 5, 16, 5, 9, 7, 12, 7, 5, 12, 18, 5, 12, 10, 3, 14, 10, 10, 11, 17, 4, 13, 9, 6, 10, 19, 15, 17, 6, 18, 3, 9, 3, 6, 12, 6, 9, 3, 9, 9, 10, 11, 4, 15, 5, 12, 13, 8, 7, 9, 14, 3, 9, 21, 3, 5, 21, 4, 4, 12, 4, 3, 5, 18, 10, 8, 13, 6, 12, 6, 14, 6, 8, 5, 3, 19, 5, 9, 5, 12, 7, 16, 8, 13, 5, 13, 7, 8, 8, 7, 11, 21, 7, 3, 5, 10, 6, 3, 11, 16, 6, 3, 15, 7, 6, 9, 7, 7, 9, 17, 9, 8, 13, 4, 6, 5, 4, 10, 8, 4, 14, 11, 5, 4, 14, 4, 7, 10, 3, 3, 3, 10, 10, 3, 3, 9, 5, 13, 6, 9, 11, 3, 4, 10, 5, 9, 6, 8, 11, 9, 8, 4, 8, 10, 5, 4, 5, 9, 8, 12, 17, 15, 20, 15, 7, 9, 3, 7, 11, 5, 18, 3, 10, 5, 8, 13, 10, 3, 17, 5, 3, 7, 10, 2, 5, 11, 11, 10, 4, 22, 9, 8, 5, 4, 7, 6, 8, 4, 5, 16, 9, 15, 11, 4, 14, 12, 11, 6, 7, 9, 7, 8, 7, 8, 15, 5, 9, 9, 3, 7, 5, 10, 21, 15, 12, 11, 3, 6, 16, 12, 13, 7, 15, 2, 4, 16, 14, 10, 18, 7, 16, 4, 13, 13, 7, 3, 5, 3, 13, 2, 6, 6, 4, 22, 8, 8, 14, 5, 5, 3, 8, 3, 2, 12, 6, 3, 8, 6, 5, 20, 19, 4, 6, 8, 4, 6, 8, 3, 3, 15, 5, 14, 5, 8, 11, 11, 16, 5, 6, 11, 12, 20, 6, 5, 16, 4, 13, 15, 3, 11, 5, 12, 5, 5, 19, 9, 13, 11, 5, 19, 4, 16, 13, 4, 24, 16, 10, 14, 3, 10, 13, 8, 3, 6, 11, 16, 5, 4, 10, 10, 13, 6, 8, 8, 5, 17, 12, 5, 4, 12, 3, 6, 2, 4, 12, 13, 4, 11, 8, 2, 15, 15, 3, 18, 12, 5, 6, 6, 6, 3, 3, 8, 11, 11, 9, 16, 7, 4, 4, 13, 7, 2, 10, 8, 9, 7, 10, 8, 2, 10, 7, 6, 11, 11, 2, 11, 5, 7, 14, 7, 13, 3, 6, 6, 2, 4, 9, 7, 18, 7, 3, 13, 11, 7, 8, 5, 7, 21, 8, 4, 4, 3, 12, 14, 5, 8, 9, 13, 6, 3, 6, 9, 12, 5, 3, 6, 17, 10, 10, 11, 5, 9, 7, 14, 14, 8, 10, 5, 5, 9, 6, 2, 3, 13, 13, 7, 12, 17, 8, 12, 7, 4, 10, 5, 4, 2, 9, 11, 3, 4, 15, 4, 6, 14, 10, 12, 12, 9, 12, 22, 6, 8, 4, 11, 16, 3, 7, 16, 18, 19, 11, 5, 12, 10, 10, 18, 9, 16, 10, 10, 18, 5, 16, 8, 3, 12, 6, 8, 18, 13, 18, 22, 6, 14, 17, 4, 4, 15, 24, 9, 5, 4, 6, 7, 7, 18, 6, 3, 5, 11, 6, 4, 4, 6, 5, 15, 8, 9, 14, 2, 6, 9, 11, 6, 18, 9, 3, 16, 5, 8, 4, 8, 10, 6, 18, 4, 5, 3, 4, 5, 9, 6, 8, 13, 4, 8, 8, 10, 10, 6, 8, 8, 6, 8, 3, 8, 11, 3, 4, 3, 7, 3, 14, 7, 16, 4, 13, 5, 12, 4, 17, 8, 15, 15, 3, 3, 9, 3, 12, 11, 11, 7, 4, 5, 15, 11, 2, 17, 17, 5, 4, 9, 3, 3, 7, 17, 20, 6, 18, 18, 6, 15, 19, 17, 4, 19, 12, 7, 18, 11, 3, 8, 3, 9, 5, 4, 6, 7, 5, 13, 6, 6, 4, 8, 7, 7, 3, 5, 12, 9, 6, 4, 8, 8, 8, 3, 17, 3, 11, 5, 8, 12, 5, 14, 8, 7, 5, 7, 10, 9, 6, 3, 6, 5, 3, 13, 9, 8, 6, 12, 12, 9, 7, 10, 8, 3, 7, 7, 21, 6, 14, 9, 16, 11, 17, 7, 10, 2, 8, 6, 12, 4, 6, 9, 11, 20, 15, 5, 9, 7, 7, 10, 16, 12, 7, 5, 11, 11, 5, 11, 7, 18, 4, 17, 15, 6, 8, 6, 3, 18, 7, 9, 6, 7, 8, 7, 12, 4, 18, 16, 8, 18, 3, 10, 8, 4, 16, 11, 9, 6, 5, 6, 4, 8, 3, 19, 9, 6, 2, 13, 6, 7, 9, 14, 11, 11, 4, 11, 3, 7, 4, 6, 4, 8, 6, 3, 17, 11, 4, 12, 15, 8, 14, 4, 9, 17, 15, 7, 10, 4, 9, 6, 5, 12, 9, 13, 16, 7, 14, 16, 3, 6, 5, 16, 8, 6, 4, 7, 5, 3, 14, 11, 4, 7, 4, 6, 4, 9, 16, 11, 4, 6, 18, 14, 6, 7, 21, 6, 8, 4, 16, 4, 12, 6, 5, 4, 13, 7, 12, 6, 4, 7, 8, 9, 12, 11, 6, 11, 6, 15, 4, 11, 8, 13, 9, 12, 5, 19, 5, 4, 3, 7, 20, 5, 2, 4, 3, 3, 4, 13, 8, 3, 11, 5, 15, 12, 13, 7, 9, 3, 7, 14, 9, 7, 5, 7, 4, 6, 9, 9, 15, 7, 4, 17, 5, 7, 5, 4, 9, 13, 17, 15, 20, 6, 4, 6, 7, 7, 7, 12, 9, 4, 14, 3, 11, 7, 17, 3, 8, 15, 6, 5, 8, 10, 9, 6, 5, 12, 15, 13, 4, 5, 4, 21, 7, 8, 13, 3, 3, 5, 7, 6, 7, 11, 15, 6, 5, 12, 4, 5, 12, 7, 11, 3, 3, 12, 6, 5, 9, 3, 10, 5, 9, 6, 4, 14, 5, 3, 7, 4, 11, 5, 8, 3, 3, 4, 4, 13, 10, 8, 7, 6, 20, 3, 4, 3, 11, 9, 12, 3, 9, 4, 12, 3, 21, 3, 9, 10, 11, 9, 10, 8, 3, 4, 17, 10, 4, 11, 20, 13, 16, 17, 8, 17, 5, 6, 11, 10, 11, 10, 3, 10, 15, 12, 4, 19, 6, 20, 10, 9, 8, 6, 13, 9, 4, 4, 8, 14, 19, 9, 5, 5, 14, 11, 4, 8, 10, 10, 8, 7, 13, 7, 13, 3, 12, 6, 10, 10, 17, 4, 6, 8, 11, 15, 6, 9, 3, 9, 10, 13, 3, 10, 13, 6, 9, 4, 3, 4, 4, 4, 11, 11, 9, 15, 10, 5, 16, 7, 13, 3, 13, 6, 4, 3, 22, 5, 8, 15, 3, 11, 4, 23, 12, 3, 7, 11, 6, 7, 6, 12, 19, 12, 9, 12, 16, 13, 6, 5, 6, 8, 9, 3, 6, 16, 8, 17, 2, 10, 8, 6, 8, 6, 5, 3, 10, 8, 7, 6, 6, 18, 16, 6, 5, 7, 9, 5, 9, 4, 14, 7, 5, 16, 15, 7, 6, 16, 3, 15, 6, 16, 11, 7, 11, 13, 5, 2, 19, 5, 9, 6, 9, 10, 4, 7, 8, 10, 3, 12, 3, 15, 11, 9, 5, 11, 8, 4, 8, 7, 6, 7, 8, 13, 4, 5, 3, 17, 6, 22, 7, 3, 13, 19, 17, 11, 3, 4, 14, 9, 7, 8, 12, 11, 4, 10, 7, 8, 4, 7, 15, 11, 16, 10, 2, 5, 6, 5, 7, 14, 3, 10, 7, 5, 3, 4, 6, 7, 17, 5, 4, 21, 5, 17, 15, 4, 10, 6, 8, 5, 7, 5, 17, 6, 3, 13, 3, 3, 10, 4, 5, 13, 7, 19, 3, 3, 11, 8, 7, 14, 14, 10, 7, 12, 7, 10, 5, 13, 16, 7, 3, 17, 3, 2, 13, 6, 12, 14, 3, 6, 7, 12, 7, 3, 9, 10, 14, 7, 14, 12, 23, 3, 6, 8, 6, 11, 8, 15, 10, 10, 13, 9, 11, 10, 6, 4, 2, 6, 16, 18, 13, 10, 3, 8, 5, 16, 3, 10, 5]\n",
      "Max number of sentences: 24\n"
     ]
    }
   ],
   "source": [
    "# get number of sentencese distribution\n",
    "num_sentences = []\n",
    "for index, row in df_train.iterrows():\n",
    "    num_sentences.append(len(row[\"utterances\"]))\n",
    "print(f\"Number of sentences distribution: {num_sentences}\")\n",
    "print(f\"Max number of sentences: {max(num_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the emotions \n",
    "emotions_one_hot_dict = {\n",
    "    \"neutral\": [1, 0, 0, 0, 0, 0, 0],\n",
    "    \"joy\": [0, 1, 0, 0, 0, 0, 0],\n",
    "    \"surprise\": [0, 0, 1, 0, 0, 0, 0],\n",
    "    \"sadness\": [0, 0, 0, 1, 0, 0, 0],\n",
    "    \"anger\": [0, 0, 0, 0, 1, 0, 0],\n",
    "    \"disgust\": [0, 0, 0, 0, 0, 1, 0],\n",
    "    \"fear\": [0, 0, 0, 0, 0, 0, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train = df_train.copy()\n",
    "bert_val = df_val.copy()\n",
    "bert_test = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"speakers\", \"emotions\", \"utterances\", \"triggers\"]\n",
    "\n",
    "def splitter_bert(df):\n",
    "    temp = pd.DataFrame(columns=columns)\n",
    "    for i in range(df.shape[0]):\n",
    "        for j, _ in enumerate(df.iloc[i]['speakers']):\n",
    "            new_row = pd.DataFrame({'speakers': [df.iloc[i]['speakers']],\n",
    "                        'emotions': [df.iloc[i]['emotions'][:j+1]],\n",
    "                        'utterances': [df.iloc[i]['utterances']],\n",
    "                        'triggers': [df.iloc[i]['triggers'][:j+1]]})\n",
    "            temp = pd.concat([temp, new_row], ignore_index=True)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train, val and test sets after splitting: \n",
      "Train shape: (27764, 4)\n",
      "Val shape: (3678, 4)\n",
      "Test shape: (3558, 4)\n",
      "-------------------------------------------------------\n",
      "Example of a sample: \n",
      "speakers      [Chandler, All, Monica, Chandler, Ross, Chandl...\n",
      "emotions                                              [neutral]\n",
      "utterances    [Hey., Hey!, So how was Joan?, I broke up with...\n",
      "triggers                                                  [0.0]\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# split train, val and test sets for BERT\n",
    "bert_train = splitter_bert(bert_train)\n",
    "bert_val = splitter_bert(bert_val)\n",
    "bert_test = splitter_bert(bert_test)\n",
    "print(\"Shape of train, val and test sets after splitting: \")\n",
    "print(f\"Train shape: {bert_train.shape}\")\n",
    "print(f\"Val shape: {bert_val.shape}\")\n",
    "print(f\"Test shape: {bert_test.shape}\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"Example of a sample: \")\n",
    "print(bert_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 2\n",
    "LEARNING_RATE = 2e-05\n",
    "OUT_CHANNELS = 768 if \"base\" in  MODEL_NAME else 1024\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.speakers = data.speakers\n",
    "        self.text = data.utterances\n",
    "        self.emotions = data.emotions\n",
    "        self.triggers = data.triggers\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        speakers = row[\"speakers\"]\n",
    "        text = row[\"utterances\"]\n",
    "        emotions = row[\"emotions\"]\n",
    "        triggers = row[\"triggers\"]\n",
    "        targets = emotions_one_hot_dict[emotions[-1]] + [triggers[-1]] # trigger is float while emotions are one hot encoded but as integers IN CASE OF ERROR CHECK THIS\n",
    "\n",
    "        text = \" \"\n",
    "        for i, d in enumerate(speakers):\n",
    "            text += f\" {d}: {row['utterances'][i]}\"\n",
    "        \n",
    "        text = text + ' emotions:'\n",
    "        for i in range(len(emotions) - 1):\n",
    "            text += f\" {emotions[i]},\"\n",
    "        \n",
    "        text = text + ' triggers:'\n",
    "        for i in range(len(triggers) - 1):\n",
    "            text += f\" {triggers[i]},\"\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'length': torch.tensor(len(speakers), dtype=torch.long),\n",
    "            'targets': torch.tensor(targets, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.AutoModel.from_pretrained(model, return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(p=0.3)\n",
    "        self.l3 = torch.nn.Linear(OUT_CHANNELS, 8)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] chandler : hey. all : hey! monica : so how was joan? chandler : i broke up with her. ross : don't tell me, because of the big nostril thing? chandler : they were huge. when she sneezed, bats flew out of them. rachel : come on, they were not that huge. chandler : i'm tellin'you, she leaned back ; i could see her brain. monica : how many perfectly fine women are you gonna reject over the most superficial insignificant things? emotions : triggers : [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = CustomDataset(bert_train, tokenizer, MAX_LEN)\n",
    "tokenizer.decode(train_dataset[0]['ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataset(bert_val, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(bert_test, tokenizer, MAX_LEN)\n",
    "\n",
    "# Definiition of the Dataloader that will feed the data in batches to the neural network for suitable training and processing.\n",
    "training_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BERTClass(MODEL_NAME)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_f(outputs, targets):\n",
    "    return torch.nn.CrossEntropyLoss()(outputs[:7], targets[:7]) + torch.nn.BCEWithLogitsLoss()(outputs[7], targets[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(training_loader)*EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    size = len(training_loader.dataset)\n",
    "    model.train()\n",
    "    for batch,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        lenght = data['length'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if batch%100==0:\n",
    "            current =  batch * len(data['ids'])\n",
    "            print(f\"Train loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, val_loss_min_input):\n",
    "    num_batches = len(test_loader)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    #fin_targets=[]\n",
    "    #fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            # lenght = data['length'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            val_loss += loss_fn(outputs, targets).item()\n",
    "        \n",
    "        val_loss /= num_batches\n",
    "        #outputs, targets = fin_outputs, fin_targets\n",
    "        print(f\"\\nValidation loss: {val_loss:>8f}.\")\n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if val_loss <= val_loss_min_input:\n",
    "            #create checkpoint variable and add important data\n",
    "            if epoch > 0: \n",
    "                print('Validation loss decreased ({:.8f} --> {:.8f}).  Saving model ...'.format(val_loss_min_input, val_loss))\n",
    "            else: print('Saving model ...')   \n",
    "            # save best moel\n",
    "            torch.save(model.state_dict(), \"model_bert_standard_project.pth\")\n",
    "            print(\"Saved PyTorch Model State to model.pth\\n\")\n",
    "            val_loss_min_input = val_loss\n",
    "    \n",
    "    return val_loss_min_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train loss: 0.734936  [    0/27764]\n",
      "Train loss: 0.361490  [ 1600/27764]\n",
      "Train loss: 0.359620  [ 3200/27764]\n",
      "Train loss: 0.319719  [ 4800/27764]\n",
      "Train loss: 0.363569  [ 6400/27764]\n",
      "Train loss: 0.281951  [ 8000/27764]\n",
      "Train loss: 0.348813  [ 9600/27764]\n",
      "Train loss: 0.340077  [11200/27764]\n",
      "Train loss: 0.310860  [12800/27764]\n",
      "Train loss: 0.368450  [14400/27764]\n",
      "Train loss: 0.316165  [16000/27764]\n",
      "Train loss: 0.375435  [17600/27764]\n",
      "Train loss: 0.252554  [19200/27764]\n",
      "Train loss: 0.350332  [20800/27764]\n",
      "Train loss: 0.250187  [22400/27764]\n",
      "Train loss: 0.249024  [24000/27764]\n",
      "Train loss: 0.295376  [25600/27764]\n",
      "Train loss: 0.195440  [27200/27764]\n",
      "\n",
      "Validation loss: 0.289172.\n",
      "Saving model ...\n",
      "Saved PyTorch Model State to model.pth\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train loss: 0.397700  [    0/27764]\n",
      "Train loss: 0.286744  [ 1600/27764]\n",
      "Train loss: 0.290482  [ 3200/27764]\n",
      "Train loss: 0.235449  [ 4800/27764]\n",
      "Train loss: 0.288546  [ 6400/27764]\n",
      "Train loss: 0.167383  [ 8000/27764]\n",
      "Train loss: 0.227619  [ 9600/27764]\n",
      "Train loss: 0.191024  [11200/27764]\n",
      "Train loss: 0.280679  [12800/27764]\n",
      "Train loss: 0.306096  [14400/27764]\n",
      "Train loss: 0.303827  [16000/27764]\n",
      "Train loss: 0.277825  [17600/27764]\n",
      "Train loss: 0.197763  [19200/27764]\n",
      "Train loss: 0.291208  [20800/27764]\n",
      "Train loss: 0.215877  [22400/27764]\n",
      "Train loss: 0.177918  [24000/27764]\n",
      "Train loss: 0.213232  [25600/27764]\n",
      "Train loss: 0.142622  [27200/27764]\n",
      "\n",
      "Validation loss: 0.218143.\n",
      "Validation loss decreased (0.28917156 --> 0.21814285).  Saving model ...\n",
      "Saved PyTorch Model State to model.pth\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train loss: 0.295868  [    0/27764]\n",
      "Train loss: 0.264507  [ 1600/27764]\n",
      "Train loss: 0.213561  [ 3200/27764]\n",
      "Train loss: 0.122233  [ 4800/27764]\n",
      "Train loss: 0.257385  [ 6400/27764]\n",
      "Train loss: 0.106005  [ 8000/27764]\n",
      "Train loss: 0.160043  [ 9600/27764]\n",
      "Train loss: 0.125501  [11200/27764]\n",
      "Train loss: 0.169864  [12800/27764]\n",
      "Train loss: 0.170115  [14400/27764]\n",
      "Train loss: 0.237466  [16000/27764]\n",
      "Train loss: 0.214326  [17600/27764]\n",
      "Train loss: 0.125741  [19200/27764]\n",
      "Train loss: 0.210842  [20800/27764]\n",
      "Train loss: 0.197147  [22400/27764]\n",
      "Train loss: 0.136925  [24000/27764]\n",
      "Train loss: 0.142397  [25600/27764]\n",
      "Train loss: 0.071582  [27200/27764]\n",
      "\n",
      "Validation loss: 0.157981.\n",
      "Validation loss decreased (0.21814285 --> 0.15798141).  Saving model ...\n",
      "Saved PyTorch Model State to model.pth\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train loss: 0.186025  [    0/27764]\n",
      "Train loss: 0.130411  [ 1600/27764]\n",
      "Train loss: 0.152845  [ 3200/27764]\n",
      "Train loss: 0.101780  [ 4800/27764]\n",
      "Train loss: 0.141514  [ 6400/27764]\n",
      "Train loss: 0.081512  [ 8000/27764]\n",
      "Train loss: 0.137865  [ 9600/27764]\n",
      "Train loss: 0.116510  [11200/27764]\n",
      "Train loss: 0.091348  [12800/27764]\n",
      "Train loss: 0.137958  [14400/27764]\n",
      "Train loss: 0.167459  [16000/27764]\n",
      "Train loss: 0.181346  [17600/27764]\n",
      "Train loss: 0.088319  [19200/27764]\n",
      "Train loss: 0.259904  [20800/27764]\n",
      "Train loss: 0.168110  [22400/27764]\n",
      "Train loss: 0.082266  [24000/27764]\n",
      "Train loss: 0.071548  [25600/27764]\n",
      "Train loss: 0.057396  [27200/27764]\n",
      "\n",
      "Validation loss: 0.129443.\n",
      "Validation loss decreased (0.15798141 --> 0.12944319).  Saving model ...\n",
      "Saved PyTorch Model State to model.pth\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train loss: 0.094481  [    0/27764]\n",
      "Train loss: 0.076334  [ 1600/27764]\n",
      "Train loss: 0.123056  [ 3200/27764]\n",
      "Train loss: 0.079153  [ 4800/27764]\n",
      "Train loss: 0.104595  [ 6400/27764]\n",
      "Train loss: 0.129212  [ 8000/27764]\n",
      "Train loss: 0.044287  [ 9600/27764]\n",
      "Train loss: 0.090002  [11200/27764]\n",
      "Train loss: 0.077706  [12800/27764]\n",
      "Train loss: 0.122952  [14400/27764]\n",
      "Train loss: 0.120677  [16000/27764]\n",
      "Train loss: 0.077865  [17600/27764]\n",
      "Train loss: 0.173438  [19200/27764]\n",
      "Train loss: 0.115728  [20800/27764]\n",
      "Train loss: 0.115153  [22400/27764]\n",
      "Train loss: 0.087194  [24000/27764]\n",
      "Train loss: 0.068420  [25600/27764]\n",
      "Train loss: 0.037727  [27200/27764]\n",
      "\n",
      "Validation loss: 0.116147.\n",
      "Validation loss decreased (0.12944319 --> 0.11614699).  Saving model ...\n",
      "Saved PyTorch Model State to model.pth\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train loss: 0.056824  [    0/27764]\n",
      "Train loss: 0.127529  [ 1600/27764]\n",
      "Train loss: 0.083421  [ 3200/27764]\n",
      "Train loss: 0.081598  [ 4800/27764]\n",
      "Train loss: 0.090975  [ 6400/27764]\n",
      "Train loss: 0.100120  [ 8000/27764]\n",
      "Train loss: 0.063864  [ 9600/27764]\n",
      "Train loss: 0.048165  [11200/27764]\n",
      "Train loss: 0.079855  [12800/27764]\n",
      "Train loss: 0.126542  [14400/27764]\n",
      "Train loss: 0.141706  [16000/27764]\n",
      "Train loss: 0.038807  [17600/27764]\n",
      "Train loss: 0.158040  [19200/27764]\n",
      "Train loss: 0.146912  [20800/27764]\n",
      "Train loss: 0.097340  [22400/27764]\n",
      "Train loss: 0.035157  [24000/27764]\n",
      "Train loss: 0.055626  [25600/27764]\n",
      "Train loss: 0.058095  [27200/27764]\n",
      "\n",
      "Validation loss: 0.100273.\n",
      "Validation loss decreased (0.11614699 --> 0.10027273).  Saving model ...\n",
      "Saved PyTorch Model State to model.pth\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train loss: 0.051738  [    0/27764]\n",
      "Train loss: 0.030193  [ 1600/27764]\n",
      "Train loss: 0.031413  [ 3200/27764]\n",
      "Train loss: 0.071678  [ 4800/27764]\n",
      "Train loss: 0.055692  [ 6400/27764]\n",
      "Train loss: 0.032146  [ 8000/27764]\n",
      "Train loss: 0.075276  [ 9600/27764]\n",
      "Train loss: 0.017878  [11200/27764]\n",
      "Train loss: 0.053803  [12800/27764]\n",
      "Train loss: 0.087125  [14400/27764]\n",
      "Train loss: 0.240917  [16000/27764]\n",
      "Train loss: 0.042637  [17600/27764]\n",
      "Train loss: 0.080911  [19200/27764]\n",
      "Train loss: 0.102663  [20800/27764]\n",
      "Train loss: 0.030547  [22400/27764]\n",
      "Train loss: 0.030952  [24000/27764]\n",
      "Train loss: 0.028359  [25600/27764]\n",
      "Train loss: 0.073188  [27200/27764]\n",
      "\n",
      "Validation loss: 0.104544.\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train loss: 0.043855  [    0/27764]\n",
      "Train loss: 0.042371  [ 1600/27764]\n",
      "Train loss: 0.029921  [ 3200/27764]\n",
      "Train loss: 0.024976  [ 4800/27764]\n",
      "Train loss: 0.050360  [ 6400/27764]\n",
      "Train loss: 0.026022  [ 8000/27764]\n",
      "Train loss: 0.039824  [ 9600/27764]\n",
      "Train loss: 0.025339  [11200/27764]\n",
      "Train loss: 0.072125  [12800/27764]\n",
      "Train loss: 0.062922  [14400/27764]\n",
      "Train loss: 0.107248  [16000/27764]\n",
      "Train loss: 0.059852  [17600/27764]\n",
      "Train loss: 0.045057  [19200/27764]\n",
      "Train loss: 0.051883  [20800/27764]\n",
      "Train loss: 0.050401  [22400/27764]\n",
      "Train loss: 0.052840  [24000/27764]\n",
      "Train loss: 0.049560  [25600/27764]\n",
      "Train loss: 0.035008  [27200/27764]\n",
      "\n",
      "Validation loss: 0.092042.\n",
      "Validation loss decreased (0.10027273 --> 0.09204242).  Saving model ...\n",
      "Saved PyTorch Model State to model.pth\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train loss: 0.050873  [    0/27764]\n",
      "Train loss: 0.032531  [ 1600/27764]\n",
      "Train loss: 0.036698  [ 3200/27764]\n",
      "Train loss: 0.020977  [ 4800/27764]\n",
      "Train loss: 0.048189  [ 6400/27764]\n",
      "Train loss: 0.066932  [ 8000/27764]\n",
      "Train loss: 0.034320  [ 9600/27764]\n",
      "Train loss: 0.013097  [11200/27764]\n",
      "Train loss: 0.029398  [12800/27764]\n",
      "Train loss: 0.049948  [14400/27764]\n",
      "Train loss: 0.122412  [16000/27764]\n",
      "Train loss: 0.029099  [17600/27764]\n",
      "Train loss: 0.059095  [19200/27764]\n",
      "Train loss: 0.043273  [20800/27764]\n",
      "Train loss: 0.041047  [22400/27764]\n",
      "Train loss: 0.010515  [24000/27764]\n",
      "Train loss: 0.021676  [25600/27764]\n",
      "Train loss: 0.080577  [27200/27764]\n",
      "\n",
      "Validation loss: 0.083764.\n",
      "Validation loss decreased (0.09204242 --> 0.08376381).  Saving model ...\n",
      "Saved PyTorch Model State to model.pth\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train loss: 0.037488  [    0/27764]\n",
      "Train loss: 0.046613  [ 1600/27764]\n",
      "Train loss: 0.023484  [ 3200/27764]\n",
      "Train loss: 0.010685  [ 4800/27764]\n",
      "Train loss: 0.045422  [ 6400/27764]\n",
      "Train loss: 0.023691  [ 8000/27764]\n",
      "Train loss: 0.020753  [ 9600/27764]\n",
      "Train loss: 0.016131  [11200/27764]\n",
      "Train loss: 0.020597  [12800/27764]\n",
      "Train loss: 0.056516  [14400/27764]\n",
      "Train loss: 0.110415  [16000/27764]\n",
      "Train loss: 0.036692  [17600/27764]\n",
      "Train loss: 0.023073  [19200/27764]\n",
      "Train loss: 0.029794  [20800/27764]\n",
      "Train loss: 0.024632  [22400/27764]\n",
      "Train loss: 0.014307  [24000/27764]\n",
      "Train loss: 0.030199  [25600/27764]\n",
      "Train loss: 0.025605  [27200/27764]\n",
      "\n",
      "Validation loss: 0.083297.\n",
      "Validation loss decreased (0.08376381 --> 0.08329735).  Saving model ...\n",
      "Saved PyTorch Model State to model.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_loss_min = np.inf\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    train()\n",
    "    val_loss_min = validation(epoch, val_loss_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bert_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('bert_model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference \n",
    "\n",
    "def inference(model, test_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            # lenght = data['length'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets\n",
    "\n",
    "outputs, targets = inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(outputs, targets):\n",
    "    results = {}\n",
    "    for tr in np.arange(0.1, 0.9, 0.1):\n",
    "        tr = round(tr, 2)\n",
    "        predictions= np.array(outputs) >= tr\n",
    "        f1 = f1_score(targets, predictions, average='macro', zero_division=1)\n",
    "        results[tr] = f1\n",
    "\n",
    "    #for k, v in results.items():\n",
    "        # print(f\"Threshold: {k}, F1-score: {v}\")\n",
    "    return max(results, key=results.get), results, outputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neutral': 12066,\n",
       " 'joy': 4986,\n",
       " 'surprise': 3664,\n",
       " 'anger': 3203,\n",
       " 'sadness': 2108,\n",
       " 'fear': 889,\n",
       " 'disgust': 848,\n",
       " 'trigger': 0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_dict.keys()\n",
    "# add trigger to the emotions\n",
    "emotions_dict[\"trigger\"] = 0\n",
    "emotions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.3, F1-score: 0.46247229026428344\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.71      0.86      0.78      1572\n",
      "         joy       0.63      0.73      0.68       663\n",
      "    surprise       0.55      0.63      0.58       486\n",
      "       anger       0.47      0.54      0.50       258\n",
      "     sadness       0.54      0.60      0.57       369\n",
      "        fear       1.00      0.00      0.00       101\n",
      "     disgust       1.00      0.00      0.00       109\n",
      "     trigger       0.51      0.72      0.59       531\n",
      "\n",
      "   micro avg       0.61      0.71      0.66      4089\n",
      "   macro avg       0.67      0.51      0.46      4089\n",
      "weighted avg       0.64      0.71      0.64      4089\n",
      " samples avg       0.67      0.71      0.90      4089\n",
      "\n"
     ]
    }
   ],
   "source": [
    "th, results, outputs, targets= get_threshold(outputs=outputs, targets=targets)\n",
    "print(f\"Best threshold: {th}, F1-score: {results[th]}\")\n",
    "predictions = np.array(outputs) >= th\n",
    "report_bert = classification_report(targets, predictions, target_names=emotions_dict.keys() ,zero_division=1)\n",
    "print(report_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
