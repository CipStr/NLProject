{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\39328\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\39328\\anaconda3\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\39328\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\39328\\anaconda3\\lib\\site-packages (4.50.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\39328\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from seaborn) (3.3.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from seaborn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from seaborn) (1.19.2)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from seaborn) (1.1.3)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\39328\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-win_amd64.whl (1.9 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-intel==2.13.1; platform_system == \"Windows\" (from tensorflow) (from versions: 0.0.1, 2.10.0.dev20220728, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0)\n",
      "ERROR: No matching distribution found for tensorflow-intel==2.13.1; platform_system == \"Windows\" (from tensorflow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.15.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas \n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install seaborn\n",
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 4/199 [00:00<00:05, 38.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:04<00:00, 44.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Dataframes\n",
      "0              word   pos\n",
      "0         Pierre   NNP\n",
      "...\n",
      "1              word  pos\n",
      "0        Rudolph  NNP\n",
      "1 ...\n",
      "2           word   pos\n",
      "0           A    DT\n",
      "1     ...\n",
      "3               word  pos\n",
      "0          Yields  NNS\n",
      "...\n",
      "4                 word   pos\n",
      "0              J.P. ...\n",
      "     word  pos\n",
      "0  Pierre  NNP\n",
      "1  Vinken  NNP\n",
      "2       ,    ,\n",
      "3      61   CD\n",
      "4   years  NNS\n",
      "(199, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create documents dataframe\n",
    "dp_docs = [file for file in os.listdir('dependency_treebank/') if file.endswith('.dp')]\n",
    "dataframes = []\n",
    "\n",
    "for file in tqdm(dp_docs):\n",
    "    with open('dependency_treebank/' + file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        data = [line.split('\\t') for line in lines]\n",
    "        df = pd.DataFrame(data, columns=['word', 'pos', 'head'])\n",
    "        # drop the last column\n",
    "        df = df.iloc[:, :-1]\n",
    "        dataframes.append(df)\n",
    "\n",
    "df = pd.DataFrame({'Dataframes': dataframes})\n",
    "print(df.head(5))\n",
    "print(df['Dataframes'][0][0:5])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(50,)\n",
      "(49,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataframes into train, validation and test sets\n",
    "train = df['Dataframes'][0:100]\n",
    "val = df['Dataframes'][100:150]\n",
    "test = df['Dataframes'][150:200]\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           word  pos\n",
      "0             a   DT\n",
      "1     licensing   NN\n",
      "2       company   NN\n",
      "3  representing  VBG\n",
      "4           the   DT\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "def to_lower_case(df, docs):\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        df[i]['word'] = df[i]['word'].str.lower()\n",
    "\n",
    "to_lower_case(train, 0)\n",
    "to_lower_case(val, 100)\n",
    "to_lower_case(test, 150)\n",
    "print(train[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word  pos\n",
      "0         pierre  NNP\n",
      "1         vinken  NNP\n",
      "2              ,    ,\n",
      "3             61   CD\n",
      "4          years  NNS\n",
      "5            old   JJ\n",
      "6              ,    ,\n",
      "7           will   MD\n",
      "8           join   VB\n",
      "9            the   DT\n",
      "10         board   NN\n",
      "11            as   IN\n",
      "12             a   DT\n",
      "13  nonexecutive   JJ\n",
      "14      director   NN\n",
      "15          nov.  NNP\n",
      "16            29   CD\n",
      "17             .    .\n",
      "19           mr.  NNP\n",
      "20        vinken  NNP\n",
      "21            is  VBZ\n",
      "22      chairman   NN\n",
      "23            of   IN\n",
      "24      elsevier  NNP\n",
      "25          n.v.  NNP\n",
      "26             ,    ,\n",
      "27           the   DT\n",
      "28         dutch  NNP\n",
      "29    publishing  VBG\n",
      "30         group   NN\n",
      "31             .    .\n",
      "31\n",
      "827\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing: from each doc remove newlines and empty lines\n",
    "def remove_newlines(df, docs):\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        df[i] = df[i][df[i]['word'] != '\\n']\n",
    "        df[i] = df[i][df[i]['word'] != '']\n",
    "\n",
    "remove_newlines(train, 0)\n",
    "remove_newlines(val, 100)\n",
    "remove_newlines(test, 150)\n",
    "print(train[0])\n",
    "print(len(train[0]))\n",
    "print(len(val[100]))\n",
    "print(len(test[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1277\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "# Create new dataframe that contains the single sentences\n",
    "def create_sentences(df, docs):\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        for element in df[i]['word']:\n",
    "            if element != '.' and element != '!' and element != '?':\n",
    "                chunks.append(element)\n",
    "            else:\n",
    "                chunks.append(element)\n",
    "                sentences.append(chunks)\n",
    "                chunks = []\n",
    "    return sentences\n",
    "\n",
    "# Create sentences for train, val and test\n",
    "train_sentences = create_sentences(train, 0)\n",
    "val_sentences = create_sentences(val, 100)\n",
    "test_sentences = create_sentences(test, 150)\n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1277\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "def create_tag_sentences(df, docs):\n",
    "    tag_sentences = []\n",
    "    chunks = []\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        for element in df[i]['pos']:\n",
    "            if element != '.' and element != '!' and element != '?':\n",
    "                chunks.append(element)\n",
    "            else:\n",
    "                chunks.append(element)\n",
    "                tag_sentences.append(chunks)\n",
    "                chunks = []\n",
    "    return tag_sentences\n",
    "\n",
    "# Create tag sentences for train, val and test\n",
    "train_tag_sentences = create_tag_sentences(train, 0)\n",
    "val_tag_sentences = create_tag_sentences(val, 100)\n",
    "test_tag_sentences = create_tag_sentences(test, 150)\n",
    "print(len(train_tag_sentences))\n",
    "print(len(val_tag_sentences))\n",
    "print(len(test_tag_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences[0]))\n",
    "print(len(train_tag_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874\n"
     ]
    }
   ],
   "source": [
    "# Append all sentences to one list for encoding\n",
    "all_sentences = []\n",
    "for sentence in train_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "for sentence in val_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "for sentence in test_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874\n"
     ]
    }
   ],
   "source": [
    "# Same for tag sentences\n",
    "all_tag_sentences = []\n",
    "for sentence in train_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "for sentence in val_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "for sentence in test_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "\n",
    "print(len(all_tag_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5419, 3714, 1, 2005, 78, 316, 1, 39, 2383, 2, 122, 22, 6, 2006, 317, 444, 2007, 3]\n"
     ]
    }
   ],
   "source": [
    "# Encode train sentences and tags\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Encode train sentences\n",
    "sentence_tokenizer = Tokenizer()\n",
    "sentence_tokenizer.fit_on_texts(all_sentences)\n",
    "encoded_sentences = sentence_tokenizer.texts_to_sequences(all_sentences)\n",
    "\n",
    "print(encoded_sentences[0])\n",
    "# sentence_tokenizer = Tokenizer()\n",
    "# sentence_tokenizer.fit_on_texts(train_sentences)\n",
    "# encoded_train_sentences = sentence_tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "# print(encoded_train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 7, 9, 5, 6, 7, 20, 12, 4, 1, 2, 4, 6, 1, 3, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(all_tag_sentences)\n",
    "encoded_tags = tag_tokenizer.texts_to_sequences(all_tag_sentences)\n",
    "\n",
    "print(encoded_tags[0])\n",
    "\n",
    "# tag_tokenizer = Tokenizer()\n",
    "# tag_tokenizer.fit_on_texts(train_tag_sentences)\n",
    "# encoded_train_tags = tag_tokenizer.texts_to_sequences(train_tag_sentences)\n",
    "\n",
    "#print(encoded_train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check length of longest sentence \n",
    "lengths = [len(sentence) for sentence in encoded_sentences]\n",
    "print(max(lengths))\n",
    "sns.boxplot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 5419 3714\n",
      "    1 2005   78  316    1   39 2383    2  122   22    6 2006  317  444\n",
      " 2007    3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  3  3  7  9  5  6  7 20 12  4  1  2  4  6\n",
      "  1  3  9  8]\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Pad each sequence to MAX_SEQ_LENGTH using KERAS' pad_sequences() function. \n",
    "# Sentences longer than MAX_SEQ_LENGTH are truncated.\n",
    "# Sentences shorter than MAX_SEQ_LENGTH are padded with zeroes.\n",
    "\n",
    "# Truncation and padding can either be 'pre' or 'post'. \n",
    "# For padding we are using 'pre' padding type, that is, add zeroes on the left side.\n",
    "# For truncation, we are using 'post', that is, truncate a sentence from right side.\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "X = pad_sequences(encoded_sentences, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "print(X[0])\n",
    "Y = pad_sequences(encoded_tags, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "print(Y[0])\n",
    "print(len(X[0]))\n",
    "print(len(Y[0]))\n",
    "\n",
    "# X_train = pad_sequences(encoded_train_sentences, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "# print(X_train[0])\n",
    "# Y_train = pad_sequences(encoded_train_tags, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "# print(Y_train[0])\n",
    "# print(len(X_train[0]))\n",
    "# print(len(Y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"\"\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove -> 50, 100, 200, 300\n",
    "embedding_model = load_embedding_model(embedding_dimension=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10947/10947 [00:00<00:00, 353043.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10948, 50)\n",
      "[ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      " -1.15139998e-01 -7.85809994e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(sentence_tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "\n",
    "word2id = sentence_tokenizer.word_index\n",
    "\n",
    "for word, i in tqdm(word2id.items()):\n",
    "    try:\n",
    "        embedding_matrix[i, :] = embedding_model[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[word2id['the']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3874, 100, 46)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# For tags use one-hot encoding\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(Y)\n",
    "print(Y.shape)\n",
    "print(Y[0])\n",
    "\n",
    "# Y_train = to_categorical(Y_train)\n",
    "# print(Y_train.shape)\n",
    "# print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10947\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "# num words in all sentences\n",
    "num_words = len(set([word for sentence in all_sentences for word in sentence]))\n",
    "print(num_words)\n",
    "num_tags = len(set([tag for sentence in all_tag_sentences for tag in sentence]))\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3874, 100)\n",
      "(3874, 100, 46)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10948, 50)\n",
      "Embedding for 'the': [ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      " -1.15139998e-01 -7.85809994e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings shape: {}\".format(embedding_matrix.shape))\n",
    "print(\"Embedding for 'the': {}\".format(embedding_matrix[word2id['the']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10948\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "print(\"Vocabulary size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1959, 100)\n",
      "(1959, 100, 46)\n",
      "(1277, 100)\n",
      "(1277, 100, 46)\n",
      "(638, 100)\n",
      "(638, 100, 46)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train, val and test sets\n",
    "X_train = X[0:len(train_sentences)]\n",
    "Y_train = Y[0:len(train_sentences)]\n",
    "X_val = X[len(train_sentences):len(train_sentences) + len(val_sentences)]\n",
    "Y_val = Y[len(train_sentences):len(train_sentences) + len(val_sentences)]\n",
    "X_test = X[len(train_sentences) + len(val_sentences):]\n",
    "Y_test = Y[len(train_sentences) + len(val_sentences):]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: LSTM + FC\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, TimeDistributed, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           547400    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 100, 256)          183296    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 100, 46)           11822     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 742518 (2.83 MB)\n",
      "Trainable params: 195118 (762.18 KB)\n",
      "Non-trainable params: 547400 (2.09 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 19s 807ms/step - loss: 2.7577 - accuracy: 0.7273 - val_loss: 1.0010 - val_accuracy: 0.7751\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 12s 767ms/step - loss: 0.7948 - accuracy: 0.8030 - val_loss: 0.7089 - val_accuracy: 0.8062\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 12s 757ms/step - loss: 0.6666 - accuracy: 0.8192 - val_loss: 0.6432 - val_accuracy: 0.8333\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 12s 782ms/step - loss: 0.6045 - accuracy: 0.8425 - val_loss: 0.5818 - val_accuracy: 0.8509\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 12s 781ms/step - loss: 0.5444 - accuracy: 0.8612 - val_loss: 0.5223 - val_accuracy: 0.8676\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 13s 802ms/step - loss: 0.4863 - accuracy: 0.8758 - val_loss: 0.4674 - val_accuracy: 0.8783\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 13s 799ms/step - loss: 0.4335 - accuracy: 0.8864 - val_loss: 0.4199 - val_accuracy: 0.8908\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 12s 775ms/step - loss: 0.3893 - accuracy: 0.8984 - val_loss: 0.3806 - val_accuracy: 0.9014\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 13s 792ms/step - loss: 0.3526 - accuracy: 0.9093 - val_loss: 0.3487 - val_accuracy: 0.9109\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 13s 796ms/step - loss: 0.3233 - accuracy: 0.9181 - val_loss: 0.3229 - val_accuracy: 0.9164\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
