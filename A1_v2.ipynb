{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas \n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install seaborn\n",
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/199 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:02<00:00, 93.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Dataframes\n",
      "0              word   pos\n",
      "0         Pierre   NNP\n",
      "...\n",
      "1              word  pos\n",
      "0        Rudolph  NNP\n",
      "1 ...\n",
      "2           word   pos\n",
      "0           A    DT\n",
      "1     ...\n",
      "3               word  pos\n",
      "0          Yields  NNS\n",
      "...\n",
      "4                 word   pos\n",
      "0              J.P. ...\n",
      "     word  pos\n",
      "0  Pierre  NNP\n",
      "1  Vinken  NNP\n",
      "2       ,    ,\n",
      "3      61   CD\n",
      "4   years  NNS\n",
      "(199, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create documents dataframe\n",
    "dp_docs = [file for file in os.listdir('dependency_treebank/') if file.endswith('.dp')]\n",
    "dataframes = []\n",
    "\n",
    "for file in tqdm(dp_docs):\n",
    "    with open('dependency_treebank/' + file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        data = [line.split('\\t') for line in lines]\n",
    "        df = pd.DataFrame(data, columns=['word', 'pos', 'head'])\n",
    "        # drop the last column\n",
    "        df = df.iloc[:, :-1]\n",
    "        dataframes.append(df)\n",
    "\n",
    "df = pd.DataFrame({'Dataframes': dataframes})\n",
    "print(df.head(5))\n",
    "print(df['Dataframes'][0][0:5])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(50,)\n",
      "(49,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataframes into train, validation and test sets\n",
    "train = df['Dataframes'][0:100]\n",
    "val = df['Dataframes'][100:150]\n",
    "test = df['Dataframes'][150:200]\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  pos\n",
      "0  pierre  NNP\n",
      "1  vinken  NNP\n",
      "2       ,    ,\n",
      "3      61   CD\n",
      "4   years  NNS\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "def to_lower_case(df, docs):\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        df[i]['word'] = df[i]['word'].str.lower()\n",
    "\n",
    "to_lower_case(train, 0)\n",
    "to_lower_case(val, 100)\n",
    "to_lower_case(test, 150)\n",
    "print(train[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word  pos\n",
      "0         pierre  NNP\n",
      "1         vinken  NNP\n",
      "2              ,    ,\n",
      "3             61   CD\n",
      "4          years  NNS\n",
      "5            old   JJ\n",
      "6              ,    ,\n",
      "7           will   MD\n",
      "8           join   VB\n",
      "9            the   DT\n",
      "10         board   NN\n",
      "11            as   IN\n",
      "12             a   DT\n",
      "13  nonexecutive   JJ\n",
      "14      director   NN\n",
      "15          nov.  NNP\n",
      "16            29   CD\n",
      "17             .    .\n",
      "19           mr.  NNP\n",
      "20        vinken  NNP\n",
      "21            is  VBZ\n",
      "22      chairman   NN\n",
      "23            of   IN\n",
      "24      elsevier  NNP\n",
      "25          n.v.  NNP\n",
      "26             ,    ,\n",
      "27           the   DT\n",
      "28         dutch  NNP\n",
      "29    publishing  VBG\n",
      "30         group   NN\n",
      "31             .    .\n",
      "31\n",
      "827\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing: from each doc remove newlines and empty lines\n",
    "def remove_newlines(df, docs):\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        df[i] = df[i][df[i]['word'] != '\\n']\n",
    "        df[i] = df[i][df[i]['word'] != '']\n",
    "\n",
    "print(len(train[0]))\n",
    "remove_newlines(train, 0)\n",
    "remove_newlines(val, 100)\n",
    "remove_newlines(test, 150)\n",
    "print(train[0])\n",
    "print(len(train[0]))\n",
    "print(len(val[100]))\n",
    "print(len(test[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1277\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "# Create new dataframe that contains the single sentences\n",
    "def create_sentences(df, docs):\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        for element in df[i]['word']:\n",
    "            if element != '.' and element != '!' and element != '?':\n",
    "                chunks.append(element)\n",
    "            else:\n",
    "                chunks.append(element)\n",
    "                sentences.append(chunks)\n",
    "                chunks = []\n",
    "    return sentences\n",
    "\n",
    "# Create sentences for train, val and test\n",
    "train_sentences = create_sentences(train, 0)\n",
    "val_sentences = create_sentences(val, 100)\n",
    "test_sentences = create_sentences(test, 150)\n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1277\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "def create_tag_sentences(df, docs):\n",
    "    tag_sentences = []\n",
    "    chunks = []\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        for element in df[i]['pos']:\n",
    "            if element != '.' and element != '!' and element != '?':\n",
    "                chunks.append(element)\n",
    "            else:\n",
    "                chunks.append(element)\n",
    "                tag_sentences.append(chunks)\n",
    "                chunks = []\n",
    "    return tag_sentences\n",
    "\n",
    "# Create tag sentences for train, val and test\n",
    "train_tag_sentences = create_tag_sentences(train, 0)\n",
    "val_tag_sentences = create_tag_sentences(val, 100)\n",
    "test_tag_sentences = create_tag_sentences(test, 150)\n",
    "print(len(train_tag_sentences))\n",
    "print(len(val_tag_sentences))\n",
    "print(len(test_tag_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences[0]))\n",
    "print(len(train_tag_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874\n"
     ]
    }
   ],
   "source": [
    "# Append all sentences to one list for encoding\n",
    "all_sentences = []\n",
    "for sentence in train_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "for sentence in val_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "for sentence in test_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874\n"
     ]
    }
   ],
   "source": [
    "# Same for tag sentences\n",
    "all_tag_sentences = []\n",
    "for sentence in train_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "for sentence in val_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "for sentence in test_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "\n",
    "print(len(all_tag_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5419, 3714, 1, 2005, 78, 316, 1, 39, 2383, 2, 122, 22, 6, 2006, 317, 444, 2007, 3]\n"
     ]
    }
   ],
   "source": [
    "# Encode train sentences and tags\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Encode train sentences\n",
    "sentence_tokenizer = Tokenizer()\n",
    "sentence_tokenizer.fit_on_texts(all_sentences)\n",
    "encoded_sentences = sentence_tokenizer.texts_to_sequences(all_sentences)\n",
    "\n",
    "print(encoded_sentences[0])\n",
    "# sentence_tokenizer = Tokenizer()\n",
    "# sentence_tokenizer.fit_on_texts(train_sentences)\n",
    "# encoded_train_sentences = sentence_tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "# print(encoded_train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2892, 5421, 1, 1118, 78, 316, 8, 586, 167, 4, 3715, 1029, 876, 877, 1, 28, 468, 6, 2006, 317, 4, 44, 1030, 420, 2893, 3]\n",
      "['rudolph agnew , 55 years old and former chairman of consolidated gold fields plc , was named a nonexecutive director of this british industrial conglomerate .']\n"
     ]
    }
   ],
   "source": [
    "# I print and decode sentence 0\n",
    "i = 2\n",
    "print(encoded_sentences[i])\n",
    "print(sentence_tokenizer.sequences_to_texts(encoded_sentences[i:i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 7, 9, 5, 6, 7, 20, 12, 4, 1, 2, 4, 6, 1, 3, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(all_tag_sentences)\n",
    "encoded_tags = tag_tokenizer.texts_to_sequences(all_tag_sentences)\n",
    "\n",
    "print(encoded_tags[0])\n",
    "\n",
    "# tag_tokenizer = Tokenizer()\n",
    "# tag_tokenizer.fit_on_texts(train_tag_sentences)\n",
    "# encoded_train_tags = tag_tokenizer.texts_to_sequences(train_tag_sentences)\n",
    "\n",
    "#print(encoded_train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\39328\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check length of longest sentence \n",
    "import seaborn as sns\n",
    "lengths = [len(sentence) for sentence in encoded_sentences]\n",
    "print(max(lengths))\n",
    "sns.boxplot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 5419 3714\n",
      "    1 2005   78  316    1   39 2383    2  122   22    6 2006  317  444\n",
      " 2007    3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  3  3  7  9  5  6  7 20 12  4  1  2  4  6\n",
      "  1  3  9  8]\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad each sequence to MAX_SEQ_LENGTH using KERAS' pad_sequences() function. \n",
    "# Sentences longer than MAX_SEQ_LENGTH are truncated.\n",
    "# Sentences shorter than MAX_SEQ_LENGTH are padded with zeroes.\n",
    "\n",
    "# Truncation and padding can either be 'pre' or 'post'. \n",
    "# For padding we are using 'pre' padding type, that is, add zeroes on the left side.\n",
    "# For truncation, we are using 'post', that is, truncate a sentence from right side.\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "X = pad_sequences(encoded_sentences, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "print(X[0])\n",
    "Y = pad_sequences(encoded_tags, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "print(Y[0])\n",
    "print(len(X[0]))\n",
    "print(len(Y[0]))\n",
    "\n",
    "# X_train = pad_sequences(encoded_train_sentences, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "# print(X_train[0])\n",
    "# Y_train = pad_sequences(encoded_train_tags, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "# print(Y_train[0])\n",
    "# print(len(X_train[0]))\n",
    "# print(len(Y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"\"\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove -> 50, 100, 200, 300\n",
    "embedding_model = load_embedding_model(embedding_dimension=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10947/10947 [00:00<00:00, 240121.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10948\n",
      "(10948, 50)\n",
      "[ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      " -1.15139998e-01 -7.85809994e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(sentence_tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "\n",
    "word2id = sentence_tokenizer.word_index\n",
    "\n",
    "for word, i in tqdm(word2id.items()):\n",
    "    try:\n",
    "        embedding_matrix[i, :] = embedding_model[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[word2id['the']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3874, 100, 46)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# For tags use one-hot encoding\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(Y)\n",
    "print(Y.shape)\n",
    "print(Y[0])\n",
    "\n",
    "# Y_train = to_categorical(Y_train)\n",
    "# print(Y_train.shape)\n",
    "# print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10947\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "# num words in all sentences\n",
    "num_words = len(set([word for sentence in all_sentences for word in sentence]))\n",
    "print(num_words)\n",
    "num_tags = len(set([tag for sentence in all_tag_sentences for tag in sentence]))\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3874, 100)\n",
      "(3874, 100, 46)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10948, 50)\n",
      "Embedding for 'the': [ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      " -1.15139998e-01 -7.85809994e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings shape: {}\".format(embedding_matrix.shape))\n",
    "print(\"Embedding for 'the': {}\".format(embedding_matrix[word2id['the']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10948\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "print(\"Vocabulary size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1959, 100)\n",
      "(1959, 100, 46)\n",
      "(1277, 100)\n",
      "(1277, 100, 46)\n",
      "(638, 100)\n",
      "(638, 100, 46)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train, val and test sets\n",
    "X_train = X[0:len(train_sentences)]\n",
    "Y_train = Y[0:len(train_sentences)]\n",
    "X_val = X[len(train_sentences):len(train_sentences) + len(val_sentences)]\n",
    "Y_val = Y[len(train_sentences):len(train_sentences) + len(val_sentences)]\n",
    "X_test = X[len(train_sentences) + len(val_sentences):]\n",
    "Y_test = Y[len(train_sentences) + len(val_sentences):]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: LSTM + FC\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, TimeDistributed, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "baseline = Sequential()\n",
    "baseline.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "baseline.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "baseline.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           547400    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 100, 256)          183296    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 100, 46)           11822     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 742518 (2.83 MB)\n",
      "Trainable params: 195118 (762.18 KB)\n",
      "Non-trainable params: 547400 (2.09 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 26s 1s/step - loss: 2.5687 - accuracy: 0.7297 - val_loss: 0.9022 - val_accuracy: 0.7819\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.7593 - accuracy: 0.8088 - val_loss: 0.6969 - val_accuracy: 0.8124\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.6534 - accuracy: 0.8253 - val_loss: 0.6283 - val_accuracy: 0.8330\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 19s 1s/step - loss: 0.5920 - accuracy: 0.8432 - val_loss: 0.5683 - val_accuracy: 0.8520\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.5323 - accuracy: 0.8627 - val_loss: 0.5097 - val_accuracy: 0.8716\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 20s 1s/step - loss: 0.4754 - accuracy: 0.8777 - val_loss: 0.4553 - val_accuracy: 0.8829\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 23s 1s/step - loss: 0.4233 - accuracy: 0.8908 - val_loss: 0.4087 - val_accuracy: 0.8946\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 27s 2s/step - loss: 0.3798 - accuracy: 0.9033 - val_loss: 0.3707 - val_accuracy: 0.9048\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.3446 - accuracy: 0.9141 - val_loss: 0.3407 - val_accuracy: 0.9126\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 29s 2s/step - loss: 0.3166 - accuracy: 0.9207 - val_loss: 0.3164 - val_accuracy: 0.9183\n"
     ]
    }
   ],
   "source": [
    "history = baseline.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "model1.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model1.add(Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model1.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 69s 3s/step - loss: 2.2579 - accuracy: 0.7171 - val_loss: 0.8262 - val_accuracy: 0.7804\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 46s 3s/step - loss: 0.7573 - accuracy: 0.7908 - val_loss: 0.7394 - val_accuracy: 0.8050\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.7122 - accuracy: 0.8045 - val_loss: 0.7147 - val_accuracy: 0.8004\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 53s 3s/step - loss: 0.6892 - accuracy: 0.8074 - val_loss: 0.6907 - val_accuracy: 0.8078\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 50s 3s/step - loss: 0.6603 - accuracy: 0.8177 - val_loss: 0.6563 - val_accuracy: 0.8196\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 55s 4s/step - loss: 0.6239 - accuracy: 0.8294 - val_loss: 0.6147 - val_accuracy: 0.8335\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-99ab07b8c4b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1740\u001b[0m                         ):\n\u001b[0;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1742\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1743\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    855\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m       (concrete_function,\n\u001b[0;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1348\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m       outputs = execute.execute(\n\u001b[0m\u001b[0;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "model2.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model2.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))\n",
    "model2.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 43s 2s/step - loss: 3.7857 - accuracy: 0.2276 - val_loss: 3.6524 - val_accuracy: 0.7656\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 39s 2s/step - loss: 3.5834 - accuracy: 0.7609 - val_loss: 3.5407 - val_accuracy: 0.7558\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 40s 3s/step - loss: 3.5132 - accuracy: 0.7594 - val_loss: 3.4830 - val_accuracy: 0.7566\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 57s 4s/step - loss: 3.4542 - accuracy: 0.7626 - val_loss: 3.4216 - val_accuracy: 0.7684\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 52s 3s/step - loss: 3.3908 - accuracy: 0.7789 - val_loss: 3.3582 - val_accuracy: 0.7830\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 56s 4s/step - loss: 3.3277 - accuracy: 0.7866 - val_loss: 3.2957 - val_accuracy: 0.7861\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 66s 4s/step - loss: 3.2664 - accuracy: 0.7883 - val_loss: 3.2359 - val_accuracy: 0.7862\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 61s 4s/step - loss: 3.2073 - accuracy: 0.7913 - val_loss: 3.1777 - val_accuracy: 0.7875\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 64s 4s/step - loss: 3.1496 - accuracy: 0.7943 - val_loss: 3.1208 - val_accuracy: 0.7877\n",
      "Epoch 10/10\n",
      "14/16 [=========================>....] - ETA: 7s - loss: 3.0945 - accuracy: 0.7956 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-cf830adf1046>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1740\u001b[0m                         ):\n\u001b[0;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1742\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1743\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    855\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m       (concrete_function,\n\u001b[0;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1348\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mcall_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m       outputs = execute.execute(\n\u001b[0m\u001b[0;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\39328\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model2.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
