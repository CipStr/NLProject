{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\39328\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\39328\\anaconda3\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\39328\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (8.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\39328\\anaconda3\\lib\\site-packages (4.50.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\39328\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from seaborn) (3.3.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from seaborn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from seaborn) (1.19.2)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from seaborn) (1.1.3)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2020.6.20)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (8.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\39328\\anaconda3\\lib\\site-packages (from pandas>=0.23->seaborn) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\39328\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-win_amd64.whl (1.9 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-intel==2.13.1; platform_system == \"Windows\" (from tensorflow) (from versions: 0.0.1, 2.10.0.dev20220728, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0)\n",
      "ERROR: No matching distribution found for tensorflow-intel==2.13.1; platform_system == \"Windows\" (from tensorflow)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.15.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas \n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install seaborn\n",
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:00<00:00, 1156.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Dataframes\n",
      "0              word   pos\n",
      "0         Pierre   NNP\n",
      "...\n",
      "1              word  pos\n",
      "0        Rudolph  NNP\n",
      "1 ...\n",
      "2           word   pos\n",
      "0           A    DT\n",
      "1     ...\n",
      "3               word  pos\n",
      "0          Yields  NNS\n",
      "...\n",
      "4                 word   pos\n",
      "0              J.P. ...\n",
      "     word  pos\n",
      "0  Pierre  NNP\n",
      "1  Vinken  NNP\n",
      "2       ,    ,\n",
      "3      61   CD\n",
      "4   years  NNS\n",
      "(199, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create documents dataframe\n",
    "dp_docs = [file for file in os.listdir('dependency_treebank/') if file.endswith('.dp')]\n",
    "dataframes = []\n",
    "\n",
    "for file in tqdm(dp_docs):\n",
    "    with open('dependency_treebank/' + file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        data = [line.split('\\t') for line in lines]\n",
    "        df = pd.DataFrame(data, columns=['word', 'pos', 'head'])\n",
    "        # drop the last column\n",
    "        df = df.iloc[:, :-1]\n",
    "        dataframes.append(df)\n",
    "\n",
    "df = pd.DataFrame({'Dataframes': dataframes})\n",
    "print(df.head(5))\n",
    "print(df['Dataframes'][0][0:5])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(50,)\n",
      "(49,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataframes into train, validation and test sets\n",
    "train = df['Dataframes'][0:100]\n",
    "val = df['Dataframes'][100:150]\n",
    "test = df['Dataframes'][150:200]\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  pos\n",
      "0  pierre  NNP\n",
      "1  vinken  NNP\n",
      "2       ,    ,\n",
      "3      61   CD\n",
      "4   years  NNS\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "def to_lower_case(df, docs):\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        df[i]['word'] = df[i]['word'].str.lower()\n",
    "\n",
    "to_lower_case(train, 0)\n",
    "to_lower_case(val, 100)\n",
    "to_lower_case(test, 150)\n",
    "print(train[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word  pos\n",
      "0         pierre  NNP\n",
      "1         vinken  NNP\n",
      "2              ,    ,\n",
      "3             61   CD\n",
      "4          years  NNS\n",
      "5            old   JJ\n",
      "6              ,    ,\n",
      "7           will   MD\n",
      "8           join   VB\n",
      "9            the   DT\n",
      "10         board   NN\n",
      "11            as   IN\n",
      "12             a   DT\n",
      "13  nonexecutive   JJ\n",
      "14      director   NN\n",
      "15          nov.  NNP\n",
      "16            29   CD\n",
      "17             .    .\n",
      "19           mr.  NNP\n",
      "20        vinken  NNP\n",
      "21            is  VBZ\n",
      "22      chairman   NN\n",
      "23            of   IN\n",
      "24      elsevier  NNP\n",
      "25          n.v.  NNP\n",
      "26             ,    ,\n",
      "27           the   DT\n",
      "28         dutch  NNP\n",
      "29    publishing  VBG\n",
      "30         group   NN\n",
      "31             .    .\n",
      "31\n",
      "827\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing: from each doc remove newlines and empty lines\n",
    "def remove_newlines(df, docs):\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        df[i] = df[i][df[i]['word'] != '\\n']\n",
    "        df[i] = df[i][df[i]['word'] != '']\n",
    "\n",
    "remove_newlines(train, 0)\n",
    "remove_newlines(val, 100)\n",
    "remove_newlines(test, 150)\n",
    "print(train[0])\n",
    "print(len(train[0]))\n",
    "print(len(val[100]))\n",
    "print(len(test[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1277\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "# Create new dataframe that contains the single sentences\n",
    "def create_sentences(df, docs):\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        for element in df[i]['word']:\n",
    "            if element != '.' and element != '!' and element != '?':\n",
    "                chunks.append(element)\n",
    "            else:\n",
    "                chunks.append(element)\n",
    "                sentences.append(chunks)\n",
    "                chunks = []\n",
    "    return sentences\n",
    "\n",
    "# Create sentences for train, val and test\n",
    "train_sentences = create_sentences(train, 0)\n",
    "val_sentences = create_sentences(val, 100)\n",
    "test_sentences = create_sentences(test, 150)\n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1277\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "def create_tag_sentences(df, docs):\n",
    "    tag_sentences = []\n",
    "    chunks = []\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        for element in df[i]['pos']:\n",
    "            if element != '.' and element != '!' and element != '?':\n",
    "                chunks.append(element)\n",
    "            else:\n",
    "                chunks.append(element)\n",
    "                tag_sentences.append(chunks)\n",
    "                chunks = []\n",
    "    return tag_sentences\n",
    "\n",
    "# Create tag sentences for train, val and test\n",
    "train_tag_sentences = create_tag_sentences(train, 0)\n",
    "val_tag_sentences = create_tag_sentences(val, 100)\n",
    "test_tag_sentences = create_tag_sentences(test, 150)\n",
    "print(len(train_tag_sentences))\n",
    "print(len(val_tag_sentences))\n",
    "print(len(test_tag_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences[0]))\n",
    "print(len(train_tag_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874\n"
     ]
    }
   ],
   "source": [
    "# Append all sentences to one list for encoding\n",
    "all_sentences = []\n",
    "for sentence in train_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "for sentence in val_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "for sentence in test_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874\n"
     ]
    }
   ],
   "source": [
    "# Same for tag sentences\n",
    "all_tag_sentences = []\n",
    "for sentence in train_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "for sentence in val_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "for sentence in test_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "\n",
    "print(len(all_tag_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5419, 3714, 1, 2005, 78, 316, 1, 39, 2383, 2, 122, 22, 6, 2006, 317, 444, 2007, 3]\n"
     ]
    }
   ],
   "source": [
    "# Encode train sentences and tags\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Encode train sentences\n",
    "sentence_tokenizer = Tokenizer()\n",
    "sentence_tokenizer.fit_on_texts(all_sentences)\n",
    "encoded_sentences = sentence_tokenizer.texts_to_sequences(all_sentences)\n",
    "\n",
    "print(encoded_sentences[0])\n",
    "# sentence_tokenizer = Tokenizer()\n",
    "# sentence_tokenizer.fit_on_texts(train_sentences)\n",
    "# encoded_train_sentences = sentence_tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "# print(encoded_train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 7, 9, 5, 6, 7, 20, 12, 4, 1, 2, 4, 6, 1, 3, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(all_tag_sentences)\n",
    "encoded_tags = tag_tokenizer.texts_to_sequences(all_tag_sentences)\n",
    "\n",
    "print(encoded_tags[0])\n",
    "\n",
    "# tag_tokenizer = Tokenizer()\n",
    "# tag_tokenizer.fit_on_texts(train_tag_sentences)\n",
    "# encoded_train_tags = tag_tokenizer.texts_to_sequences(train_tag_sentences)\n",
    "\n",
    "#print(encoded_train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Alessandro\\GitHub\\NLProject\\A1_v2.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Alessandro/GitHub/NLProject/A1_v2.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m lengths \u001b[39m=\u001b[39m [\u001b[39mlen\u001b[39m(sentence) \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m encoded_sentences]\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Alessandro/GitHub/NLProject/A1_v2.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mmax\u001b[39m(lengths))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Alessandro/GitHub/NLProject/A1_v2.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sns\u001b[39m.\u001b[39mboxplot(lengths)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Alessandro/GitHub/NLProject/A1_v2.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "# check length of longest sentence \n",
    "lengths = [len(sentence) for sentence in encoded_sentences]\n",
    "print(max(lengths))\n",
    "sns.boxplot(lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 5419 3714\n",
      "    1 2005   78  316    1   39 2383    2  122   22    6 2006  317  444\n",
      " 2007    3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  3  3  7  9  5  6  7 20 12  4  1  2  4  6\n",
      "  1  3  9  8]\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Pad each sequence to MAX_SEQ_LENGTH using KERAS' pad_sequences() function. \n",
    "# Sentences longer than MAX_SEQ_LENGTH are truncated.\n",
    "# Sentences shorter than MAX_SEQ_LENGTH are padded with zeroes.\n",
    "\n",
    "# Truncation and padding can either be 'pre' or 'post'. \n",
    "# For padding we are using 'pre' padding type, that is, add zeroes on the left side.\n",
    "# For truncation, we are using 'post', that is, truncate a sentence from right side.\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "X = pad_sequences(encoded_sentences, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "print(X[0])\n",
    "Y = pad_sequences(encoded_tags, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "print(Y[0])\n",
    "print(len(X[0]))\n",
    "print(len(Y[0]))\n",
    "\n",
    "# X_train = pad_sequences(encoded_train_sentences, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "# print(X_train[0])\n",
    "# Y_train = pad_sequences(encoded_train_tags, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "# print(Y_train[0])\n",
    "# print(len(X_train[0]))\n",
    "# print(len(Y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"\"\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove -> 50, 100, 200, 300\n",
    "embedding_model = load_embedding_model(embedding_dimension=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10947/10947 [00:00<00:00, 405470.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10948, 50)\n",
      "[ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      " -1.15139998e-01 -7.85809994e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(sentence_tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "\n",
    "word2id = sentence_tokenizer.word_index\n",
    "\n",
    "for word, i in tqdm(word2id.items()):\n",
    "    try:\n",
    "        embedding_matrix[i, :] = embedding_model[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[word2id['the']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3874, 100, 46)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# For tags use one-hot encoding\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(Y)\n",
    "print(Y.shape)\n",
    "print(Y[0])\n",
    "\n",
    "# Y_train = to_categorical(Y_train)\n",
    "# print(Y_train.shape)\n",
    "# print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10947\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "# num words in all sentences\n",
    "num_words = len(set([word for sentence in all_sentences for word in sentence]))\n",
    "print(num_words)\n",
    "num_tags = len(set([tag for sentence in all_tag_sentences for tag in sentence]))\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3874, 100)\n",
      "(3874, 100, 46)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10948, 50)\n",
      "Embedding for 'the': [ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      " -1.15139998e-01 -7.85809994e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings shape: {}\".format(embedding_matrix.shape))\n",
    "print(\"Embedding for 'the': {}\".format(embedding_matrix[word2id['the']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10948\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "print(\"Vocabulary size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1959, 100)\n",
      "(1959, 100, 46)\n",
      "(1277, 100)\n",
      "(1277, 100, 46)\n",
      "(638, 100)\n",
      "(638, 100, 46)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train, val and test sets\n",
    "X_train = X[0:len(train_sentences)]\n",
    "Y_train = Y[0:len(train_sentences)]\n",
    "X_val = X[len(train_sentences):len(train_sentences) + len(val_sentences)]\n",
    "Y_val = Y[len(train_sentences):len(train_sentences) + len(val_sentences)]\n",
    "X_test = X[len(train_sentences) + len(val_sentences):]\n",
    "Y_test = Y[len(train_sentences) + len(val_sentences):]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: LSTM + FC\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, TimeDistributed, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "baseline = Sequential()\n",
    "baseline.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "baseline.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "baseline.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           547400    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 100, 256)          183296    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 100, 46)           11822     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 742518 (2.83 MB)\n",
      "Trainable params: 195118 (762.18 KB)\n",
      "Non-trainable params: 547400 (2.09 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 19s 807ms/step - loss: 2.7577 - accuracy: 0.7273 - val_loss: 1.0010 - val_accuracy: 0.7751\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 12s 767ms/step - loss: 0.7948 - accuracy: 0.8030 - val_loss: 0.7089 - val_accuracy: 0.8062\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 12s 757ms/step - loss: 0.6666 - accuracy: 0.8192 - val_loss: 0.6432 - val_accuracy: 0.8333\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 12s 782ms/step - loss: 0.6045 - accuracy: 0.8425 - val_loss: 0.5818 - val_accuracy: 0.8509\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 12s 781ms/step - loss: 0.5444 - accuracy: 0.8612 - val_loss: 0.5223 - val_accuracy: 0.8676\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 13s 802ms/step - loss: 0.4863 - accuracy: 0.8758 - val_loss: 0.4674 - val_accuracy: 0.8783\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 13s 799ms/step - loss: 0.4335 - accuracy: 0.8864 - val_loss: 0.4199 - val_accuracy: 0.8908\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 12s 775ms/step - loss: 0.3893 - accuracy: 0.8984 - val_loss: 0.3806 - val_accuracy: 0.9014\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 13s 792ms/step - loss: 0.3526 - accuracy: 0.9093 - val_loss: 0.3487 - val_accuracy: 0.9109\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 13s 796ms/step - loss: 0.3233 - accuracy: 0.9181 - val_loss: 0.3229 - val_accuracy: 0.9164\n"
     ]
    }
   ],
   "source": [
    "history = baseline.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "model1.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model1.add(Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model1.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 21s 794ms/step - loss: 2.0219 - accuracy: 0.7389 - val_loss: 0.7971 - val_accuracy: 0.7879\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 11s 714ms/step - loss: 0.7425 - accuracy: 0.7974 - val_loss: 0.7247 - val_accuracy: 0.7978\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 11s 720ms/step - loss: 0.6953 - accuracy: 0.8054 - val_loss: 0.6926 - val_accuracy: 0.8061\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 12s 727ms/step - loss: 0.6632 - accuracy: 0.8128 - val_loss: 0.6578 - val_accuracy: 0.8177\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 12s 744ms/step - loss: 0.6257 - accuracy: 0.8243 - val_loss: 0.6139 - val_accuracy: 0.8283\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 12s 744ms/step - loss: 0.5790 - accuracy: 0.8403 - val_loss: 0.5610 - val_accuracy: 0.8494\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 12s 757ms/step - loss: 0.5242 - accuracy: 0.8640 - val_loss: 0.5032 - val_accuracy: 0.8716\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 12s 741ms/step - loss: 0.4657 - accuracy: 0.8801 - val_loss: 0.4491 - val_accuracy: 0.8821\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 12s 758ms/step - loss: 0.4144 - accuracy: 0.8906 - val_loss: 0.4054 - val_accuracy: 0.8915\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 12s 769ms/step - loss: 0.3749 - accuracy: 0.8996 - val_loss: 0.3712 - val_accuracy: 0.9013\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "model2.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model2.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))\n",
    "model2.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
