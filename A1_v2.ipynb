{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "This Jupyter Notebook document is our implementation of Assignment 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas \n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install tqdm\n",
    "# !pip install seaborn\n",
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Loading and Splitting\n",
    "* **Download** the corpus.\n",
    "* **Encode** the corpus into a pandas.DataFrame object.\n",
    "* **Split** it in training, validation, and test sets.\n",
    "\n",
    "[Penn TreeBank corpus](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 199/199 [00:00<00:00, 1156.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Dataframes\n",
      "0              word   pos\n",
      "0         Pierre   NNP\n",
      "...\n",
      "1              word  pos\n",
      "0        Rudolph  NNP\n",
      "1 ...\n",
      "2           word   pos\n",
      "0           A    DT\n",
      "1     ...\n",
      "3               word  pos\n",
      "0          Yields  NNS\n",
      "...\n",
      "4                 word   pos\n",
      "0              J.P. ...\n",
      "     word  pos\n",
      "0  Pierre  NNP\n",
      "1  Vinken  NNP\n",
      "2       ,    ,\n",
      "3      61   CD\n",
      "4   years  NNS\n",
      "(199, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create documents dataframe\n",
    "dp_docs = [file for file in os.listdir('dependency_treebank/') if file.endswith('.dp')]\n",
    "dataframes = []\n",
    "\n",
    "for file in tqdm(dp_docs):\n",
    "    with open('dependency_treebank/' + file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        data = [line.split('\\t') for line in lines]\n",
    "        df = pd.DataFrame(data, columns=['word', 'pos', 'head'])\n",
    "        # drop the last column\n",
    "        df = df.iloc[:, :-1]\n",
    "        dataframes.append(df)\n",
    "\n",
    "df = pd.DataFrame({'Dataframes': dataframes})\n",
    "print(df.head(5))\n",
    "print(df['Dataframes'][0][0:5])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(50,)\n",
      "(49,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataframes into train, validation and test sets\n",
    "train = df['Dataframes'][0:100]\n",
    "val = df['Dataframes'][100:150]\n",
    "test = df['Dataframes'][150:200]\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     word  pos\n",
      "0  pierre  NNP\n",
      "1  vinken  NNP\n",
      "2       ,    ,\n",
      "3      61   CD\n",
      "4   years  NNS\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing\n",
    "def to_lower_case(df, docs):\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        df[i]['word'] = df[i]['word'].str.lower()\n",
    "\n",
    "to_lower_case(train, 0)\n",
    "to_lower_case(val, 100)\n",
    "to_lower_case(test, 150)\n",
    "print(train[0][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word  pos\n",
      "0         pierre  NNP\n",
      "1         vinken  NNP\n",
      "2              ,    ,\n",
      "3             61   CD\n",
      "4          years  NNS\n",
      "5            old   JJ\n",
      "6              ,    ,\n",
      "7           will   MD\n",
      "8           join   VB\n",
      "9            the   DT\n",
      "10         board   NN\n",
      "11            as   IN\n",
      "12             a   DT\n",
      "13  nonexecutive   JJ\n",
      "14      director   NN\n",
      "15          nov.  NNP\n",
      "16            29   CD\n",
      "17             .    .\n",
      "19           mr.  NNP\n",
      "20        vinken  NNP\n",
      "21            is  VBZ\n",
      "22      chairman   NN\n",
      "23            of   IN\n",
      "24      elsevier  NNP\n",
      "25          n.v.  NNP\n",
      "26             ,    ,\n",
      "27           the   DT\n",
      "28         dutch  NNP\n",
      "29    publishing  VBG\n",
      "30         group   NN\n",
      "31             .    .\n",
      "31\n",
      "827\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing: from each doc remove newlines and empty lines\n",
    "def remove_newlines(df, docs):\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        df[i] = df[i][df[i]['word'] != '\\n']\n",
    "        df[i] = df[i][df[i]['word'] != '']\n",
    "\n",
    "remove_newlines(train, 0)\n",
    "remove_newlines(val, 100)\n",
    "remove_newlines(test, 150)\n",
    "print(train[0])\n",
    "print(len(train[0]))\n",
    "print(len(val[100]))\n",
    "print(len(test[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1277\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "# Create new dataframe that contains the single sentences\n",
    "def create_sentences(df, docs):\n",
    "    sentences = []\n",
    "    chunks = []\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        for element in df[i]['word']:\n",
    "            if element != '.' and element != '!' and element != '?':\n",
    "                chunks.append(element)\n",
    "            else:\n",
    "                chunks.append(element)\n",
    "                sentences.append(chunks)\n",
    "                chunks = []\n",
    "    return sentences\n",
    "\n",
    "# Create sentences for train, val and test\n",
    "train_sentences = create_sentences(train, 0)\n",
    "val_sentences = create_sentences(val, 100)\n",
    "test_sentences = create_sentences(test, 150)\n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959\n",
      "1277\n",
      "638\n"
     ]
    }
   ],
   "source": [
    "def create_tag_sentences(df, docs):\n",
    "    tag_sentences = []\n",
    "    chunks = []\n",
    "    for i in range(docs, docs + len(df)):\n",
    "        for element in df[i]['pos']:\n",
    "            if element != '.' and element != '!' and element != '?':\n",
    "                chunks.append(element)\n",
    "            else:\n",
    "                chunks.append(element)\n",
    "                tag_sentences.append(chunks)\n",
    "                chunks = []\n",
    "    return tag_sentences\n",
    "\n",
    "# Create tag sentences for train, val and test\n",
    "train_tag_sentences = create_tag_sentences(train, 0)\n",
    "val_tag_sentences = create_tag_sentences(val, 100)\n",
    "test_tag_sentences = create_tag_sentences(test, 150)\n",
    "print(len(train_tag_sentences))\n",
    "print(len(val_tag_sentences))\n",
    "print(len(test_tag_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentences[0]))\n",
    "print(len(train_tag_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874\n"
     ]
    }
   ],
   "source": [
    "# Append all sentences to one list for encoding\n",
    "all_sentences = []\n",
    "for sentence in train_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "for sentence in val_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "for sentence in test_sentences:\n",
    "    all_sentences.append(sentence)\n",
    "\n",
    "print(len(all_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3874\n"
     ]
    }
   ],
   "source": [
    "# Same for tag sentences\n",
    "all_tag_sentences = []\n",
    "for sentence in train_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "for sentence in val_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "for sentence in test_tag_sentences:\n",
    "    all_tag_sentences.append(sentence)\n",
    "\n",
    "print(len(all_tag_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Text encoding\n",
    "\n",
    "To train a neural POS tagger, you first need to encode text into numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5419, 3714, 1, 2005, 78, 316, 1, 39, 2383, 2, 122, 22, 6, 2006, 317, 444, 2007, 3]\n"
     ]
    }
   ],
   "source": [
    "# Encode train sentences and tags\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Encode train sentences\n",
    "sentence_tokenizer = Tokenizer()\n",
    "sentence_tokenizer.fit_on_texts(all_sentences)\n",
    "encoded_sentences = sentence_tokenizer.texts_to_sequences(all_sentences)\n",
    "\n",
    "print(encoded_sentences[0])\n",
    "# sentence_tokenizer = Tokenizer()\n",
    "# sentence_tokenizer.fit_on_texts(train_sentences)\n",
    "# encoded_train_sentences = sentence_tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "# print(encoded_train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 7, 9, 5, 6, 7, 20, 12, 4, 1, 2, 4, 6, 1, 3, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(all_tag_sentences)\n",
    "encoded_tags = tag_tokenizer.texts_to_sequences(all_tag_sentences)\n",
    "\n",
    "print(encoded_tags[0])\n",
    "\n",
    "# tag_tokenizer = Tokenizer()\n",
    "# tag_tokenizer.fit_on_texts(train_tag_sentences)\n",
    "# encoded_train_tags = tag_tokenizer.texts_to_sequences(train_tag_sentences)\n",
    "\n",
    "#print(encoded_train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 5419 3714\n",
      "    1 2005   78  316    1   39 2383    2  122   22    6 2006  317  444\n",
      " 2007    3]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  3  3  7  9  5  6  7 20 12  4  1  2  4  6\n",
      "  1  3  9  8]\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Pad each sequence to MAX_SEQ_LENGTH using KERAS' pad_sequences() function. \n",
    "# Sentences longer than MAX_SEQ_LENGTH are truncated.\n",
    "# Sentences shorter than MAX_SEQ_LENGTH are padded with zeroes.\n",
    "\n",
    "# Truncation and padding can either be 'pre' or 'post'. \n",
    "# For padding we are using 'pre' padding type, that is, add zeroes on the left side.\n",
    "# For truncation, we are using 'post', that is, truncate a sentence from right side.\n",
    "\n",
    "MAX_SEQ_LENGTH = 100\n",
    "X = pad_sequences(encoded_sentences, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "print(X[0])\n",
    "Y = pad_sequences(encoded_tags, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "print(Y[0])\n",
    "print(len(X[0]))\n",
    "print(len(Y[0]))\n",
    "\n",
    "# X_train = pad_sequences(encoded_train_sentences, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "# print(X_train[0])\n",
    "# Y_train = pad_sequences(encoded_train_tags, maxlen=MAX_SEQ_LENGTH, padding='pre', truncating='post')\n",
    "# print(Y_train[0])\n",
    "# print(len(X_train[0]))\n",
    "# print(len(Y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50) -> gensim.models.keyedvectors.KeyedVectors:\n",
    "    download_path = \"\"\n",
    "    download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "        \n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove -> 50, 100, 200, 300\n",
    "embedding_model = load_embedding_model(embedding_dimension=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def check_OOV_terms(embedding_model: gensim.models.keyedvectors.KeyedVectors, word_listing: List[str]):\n",
    "    \n",
    "    embedding_vocabulary = set(embedding_model.key_to_index.keys())\n",
    "    oov = set(word_listing).difference(embedding_vocabulary)\n",
    "    return list(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_terms = check_OOV_terms(embedding_model, train_sentences)\n",
    "oov_percentage = float(len(oov_terms)) * 100 / len(train_sentences)\n",
    "\n",
    "print(oov_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(embedding_model: gensim.models.keyedvectors.KeyedVectors,\n",
    "                           embedding_dimension: int,\n",
    "                           word_to_idx: Dict[str, int],\n",
    "                           vocab_size: int,\n",
    "                           oov_terms: List[str]) -> np.ndarray:\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dimension), dtype=np.float32)\n",
    "    for word, idx in tqdm(word_to_idx.items()):\n",
    "        try:\n",
    "            embedding_vector = embedding_model[word]\n",
    "        except (KeyError, TypeError):\n",
    "            embedding_vector = np.random.uniform(low=-0.05, high=0.05, size=embedding_dimension)\n",
    "\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10947/10947 [00:00<00:00, 405470.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10948, 50)\n",
      "[ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      " -1.15139998e-01 -7.85809994e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(sentence_tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "\n",
    "word2id = sentence_tokenizer.word_index\n",
    "\n",
    "embedding_dimension = 50\n",
    "embedding_matrix = build_embedding_matrix(embedding_model, embedding_dimension, word2id, len(word2id), oov_terms)\n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[word2id['the']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3874, 100, 46)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# For tags use one-hot encoding\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(Y)\n",
    "print(Y.shape)\n",
    "print(Y[0])\n",
    "\n",
    "# Y_train = to_categorical(Y_train)\n",
    "# print(Y_train.shape)\n",
    "# print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10947\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "# num words in all sentences\n",
    "num_words = len(set([word for sentence in all_sentences for word in sentence]))\n",
    "print(num_words)\n",
    "num_tags = len(set([tag for sentence in all_tag_sentences for tag in sentence]))\n",
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3874, 100)\n",
      "(3874, 100, 46)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (10948, 50)\n",
      "Embedding for 'the': [ 4.18000013e-01  2.49679998e-01 -4.12420005e-01  1.21699996e-01\n",
      "  3.45270008e-01 -4.44569997e-02 -4.96879995e-01 -1.78619996e-01\n",
      " -6.60229998e-04 -6.56599998e-01  2.78430015e-01 -1.47670001e-01\n",
      " -5.56770027e-01  1.46579996e-01 -9.50950012e-03  1.16579998e-02\n",
      "  1.02040000e-01 -1.27920002e-01 -8.44299972e-01 -1.21809997e-01\n",
      " -1.68009996e-02 -3.32789987e-01 -1.55200005e-01 -2.31309995e-01\n",
      " -1.91809997e-01 -1.88230002e+00 -7.67459989e-01  9.90509987e-02\n",
      " -4.21249986e-01 -1.95260003e-01  4.00710011e+00 -1.85939997e-01\n",
      " -5.22870004e-01 -3.16810012e-01  5.92130003e-04  7.44489999e-03\n",
      "  1.77780002e-01 -1.58969998e-01  1.20409997e-02 -5.42230010e-02\n",
      " -2.98709989e-01 -1.57490000e-01 -3.47579986e-01 -4.56370004e-02\n",
      " -4.42510009e-01  1.87849998e-01  2.78489990e-03 -1.84110001e-01\n",
      " -1.15139998e-01 -7.85809994e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings shape: {}\".format(embedding_matrix.shape))\n",
    "print(\"Embedding for 'the': {}\".format(embedding_matrix[word2id['the']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10948\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "print(\"Vocabulary size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1959, 100)\n",
      "(1959, 100, 46)\n",
      "(1277, 100)\n",
      "(1277, 100, 46)\n",
      "(638, 100)\n",
      "(638, 100, 46)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train, val and test sets\n",
    "X_train = X[0:len(train_sentences)]\n",
    "Y_train = Y[0:len(train_sentences)]\n",
    "X_val = X[len(train_sentences):len(train_sentences) + len(val_sentences)]\n",
    "Y_val = Y[len(train_sentences):len(train_sentences) + len(val_sentences)]\n",
    "X_test = X[len(train_sentences) + len(val_sentences):]\n",
    "Y_test = Y[len(train_sentences) + len(val_sentences):]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Model definition\n",
    "\n",
    "You are now tasked to define your neural POS tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "* You are **free** to experiment with hyper-parameters to define the baseline model.\n",
    "\n",
    "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
    "* **Model 2**: add an additional Dense layer to the Baseline model.\n",
    "\n",
    "* **Do not mix Model 1 and Model 2**. Each model has its own instructions.\n",
    "\n",
    "**Note**: if a document contains many tokens, you are **free** to split them into chunks or sentences to define your mini-batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: LSTM + FC\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Bidirectional, TimeDistributed, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "baseline = Sequential()\n",
    "baseline.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "baseline.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "baseline.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           547400    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 100, 256)          183296    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 100, 46)           11822     \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 742518 (2.83 MB)\n",
      "Trainable params: 195118 (762.18 KB)\n",
      "Non-trainable params: 547400 (2.09 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])\n",
    "\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "model1.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model1.add(Bidirectional(LSTM(units=64, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model1.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = Y_train.shape[2]\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=vocab_size, output_dim=50, input_length=MAX_SEQ_LENGTH, weights=[embedding_matrix], trainable=False))\n",
    "model2.add(Bidirectional(LSTM(units=128, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model2.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))\n",
    "model2.add(TimeDistributed(Dense(num_tags, activation=\"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Metrics\n",
    "\n",
    "Before training the models, you are tasked to define the evaluation metrics for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Evaluate your models using macro F1-score, compute over **all** tokens.\n",
    "* **Concatenate** all tokens in a data split to compute the F1-score. (**Hint**: accumulate FP, TP, FN, TN iteratively) \n",
    "* **Do not consider punctuation and symbol classes** $\\rightarrow$ [What is punctuation?](https://en.wikipedia.org/wiki/English_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: What about OOV tokens?\n",
    "   * All the tokens in the **training** set that are not in GloVe are **not** considered as OOV\n",
    "   * For the remaining tokens (i.e., OOV in the validation and test sets), you have to assign them a **static** embedding.\n",
    "   * You are **free** to define the static embedding using any strategy (e.g., random, neighbourhood, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Training and Evaluation\n",
    "\n",
    "You are now tasked to train and evaluate the Baseline, Model 1, and Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Compute metrics on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Pick the **best** performing model according to the observed validation set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = baseline.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=128, epochs=10, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
